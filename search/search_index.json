{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to St. Jude Cloud Documentation!\n\n\nOverview\n\n\nHere, you'll find\nauthoritative guides for accessing St. Jude Cloud data, running analysis workflows (our workflows or your own) on the cloud,\nand exploring curated data from numerous published studies by St. Jude and our collaborating institutions. For a brief overview of\neverything St. Jude Cloud provides, we recommend that you watch the video\non \nour home page\n.\n\n\nSign up \nhere\n to receive email notifications when we add new datasets, analysis pipelines, or other exciting features.\n\n\nFeatures\n\n\nYou can apply many different capabilities of St. Jude Cloud to your research, such as:\n\n\n\n\nExplore the \nraw genomics data\n we currently offer. You can browse by diagnosis, publication, or curated dataset while applying a number of different filters. For more information, see our \ndata request guide\n.\n\n\nRun your tools on \nour\n data by \nrequesting data\n and packaging your tools in a secure cloud environment. See this \nguide\n for an example.\n\n\nRun our in house analysis workflows on \nyour\n data by \nmoving your data\n to the cloud and selecting a \nworkflow\n to run. See this \nguide\n for an example. \n\n\nExplore St. Jude datasets through interactive visualizations that we have packaged for the community. For example, visit \nPeCan\n to visually investigate pediatric cancer mutation data.\n\n\nCreate manuscript quality figures with \nyour\n data to use in publications or to host on your website with \nProteinPaint\n or \nGenomePaint\n. See the \nProteinPaint\n and \nGenomePaint\n doc pages for help.\n\n\n\n\n\n\nNote\n\n\nPlease note that while it is free to receive and store our data in St. Jude Cloud, there are \ncompute and storage fees associated with working in the cloud, as well as egress fees for downloading our data\n. \n\n\n\n\nDatasets\n\n\nThe following projects currently distribute their data through St. Jude Cloud. Click \nhere\n for more information about the projects listed below.\n\n\n\n\nPediatric Cancer Genome Project (PCGP)\n\n\nSt. Jude Lifetime (SJLIFE)\n\n\nClinical Genomics (Clinical Pilot and G4K)\n\n\nSickle Cell Genome Project (SGP)\n\n\nChildhood Cancer Survivor Study (CCSS)\n\n\n\n\nAnalysis Workflows\n\n\nSt. Jude shares a number of end-to-end analysis workflows on the cloud. Click on the links below to learn more about the workflow.\n\n\n\n\nPeCan Pie\n\n\nNeoepitopePred\n\n\nChIP-Seq Peak Calling\n\n\nRapid RNA-Seq Fusion Detection\n\n\nWARDEN Differential Expression Analysis\n\n\nMutational Signatures\n\n\ncis-x (coming soon)\n\n\nXenoCP (coming soon)\n\n\n\n\nPortals\n\n\nSt. Jude Cloud provides a number of user portals which you can use to interactively explore the results we produce. Click on the links below to\nvisit each of the corresponding portals.\n\n\n\n\nPediatric Cancer Portal (PeCan)\n. Interactively explore mutational recurrence and pathogenicity assessment of variants in pediatric cancer using a wide variety of St. Jude + publicly available data.\n\n\nSickle Cell Diseases Portal\n. Explore the latest from the Sickle Cell Genomics Project (a collaboration between St. Jude and Baylor College of Medicine). \n\n\n\n\nContact Us\n\n\nAny questions, comments, or concerns can be directed to our \n\"Contact Us\"\n form or you can email us directly at support@stjude.cloud.",
            "title": "Welcome to St. Jude Cloud"
        },
        {
            "location": "/#welcome-to-st-jude-cloud-documentation",
            "text": "",
            "title": "Welcome to St. Jude Cloud Documentation!"
        },
        {
            "location": "/#overview",
            "text": "Here, you'll find\nauthoritative guides for accessing St. Jude Cloud data, running analysis workflows (our workflows or your own) on the cloud,\nand exploring curated data from numerous published studies by St. Jude and our collaborating institutions. For a brief overview of\neverything St. Jude Cloud provides, we recommend that you watch the video\non  our home page .  Sign up  here  to receive email notifications when we add new datasets, analysis pipelines, or other exciting features.",
            "title": "Overview"
        },
        {
            "location": "/#features",
            "text": "You can apply many different capabilities of St. Jude Cloud to your research, such as:   Explore the  raw genomics data  we currently offer. You can browse by diagnosis, publication, or curated dataset while applying a number of different filters. For more information, see our  data request guide .  Run your tools on  our  data by  requesting data  and packaging your tools in a secure cloud environment. See this  guide  for an example.  Run our in house analysis workflows on  your  data by  moving your data  to the cloud and selecting a  workflow  to run. See this  guide  for an example.   Explore St. Jude datasets through interactive visualizations that we have packaged for the community. For example, visit  PeCan  to visually investigate pediatric cancer mutation data.  Create manuscript quality figures with  your  data to use in publications or to host on your website with  ProteinPaint  or  GenomePaint . See the  ProteinPaint  and  GenomePaint  doc pages for help.    Note  Please note that while it is free to receive and store our data in St. Jude Cloud, there are  compute and storage fees associated with working in the cloud, as well as egress fees for downloading our data .",
            "title": "Features"
        },
        {
            "location": "/#datasets",
            "text": "The following projects currently distribute their data through St. Jude Cloud. Click  here  for more information about the projects listed below.   Pediatric Cancer Genome Project (PCGP)  St. Jude Lifetime (SJLIFE)  Clinical Genomics (Clinical Pilot and G4K)  Sickle Cell Genome Project (SGP)  Childhood Cancer Survivor Study (CCSS)",
            "title": "Datasets"
        },
        {
            "location": "/#analysis-workflows",
            "text": "St. Jude shares a number of end-to-end analysis workflows on the cloud. Click on the links below to learn more about the workflow.   PeCan Pie  NeoepitopePred  ChIP-Seq Peak Calling  Rapid RNA-Seq Fusion Detection  WARDEN Differential Expression Analysis  Mutational Signatures  cis-x (coming soon)  XenoCP (coming soon)",
            "title": "Analysis Workflows"
        },
        {
            "location": "/#portals",
            "text": "St. Jude Cloud provides a number of user portals which you can use to interactively explore the results we produce. Click on the links below to\nvisit each of the corresponding portals.   Pediatric Cancer Portal (PeCan) . Interactively explore mutational recurrence and pathogenicity assessment of variants in pediatric cancer using a wide variety of St. Jude + publicly available data.  Sickle Cell Diseases Portal . Explore the latest from the Sickle Cell Genomics Project (a collaboration between St. Jude and Baylor College of Medicine).",
            "title": "Portals"
        },
        {
            "location": "/#contact-us",
            "text": "Any questions, comments, or concerns can be directed to our  \"Contact Us\"  form or you can email us directly at support@stjude.cloud.",
            "title": "Contact Us"
        },
        {
            "location": "/create-an-account/",
            "text": "Create an account\n\n\nIn order to request St. Jude data and work with that data in the cloud, you will need to create a DNAnexus account and use it to log in to St. Jude Cloud. St. Jude Cloud is built on top of the \nDNAnexus\n genomics cloud ecosystem. Each new user will receive a $50 credit upon creation of their St. Jude Cloud account with DNAnexus (see the \nnote\n in the Billing Setup section).\n\n\n\n\nNote\n\n\nThe account creation and login process is slightly different if you are an internal user (you work at St. Jude). Internal users please go to the \nintranet home page\n and type 'Bioinformatics Self-Service' into the search bar. From there, click on the link that says 'Bioinformatics Self-Service on St. Jude Cloud' to access the internal guide to creating an account.\n\n\n\n\nAccount Creation\n\n\nIf you work at an institution that is not St. Jude (the standard case), you can use the following process to create an account:\n\n\n\n\nGo to the \nSt. Jude Cloud log in page\n on DNAnexus.\n\n\nClick \"Create an Account\".\n\n\nFill in your information.\n\n\nOn the Create New Account page, make sure to select \"Microsoft Azure (westus)\" as the Default Cloud Region.\n\n\nClick 'CREATE ACCOUNT'\n\n\n\n\n\n\nLogging in\n\n\nSimply go to the \nSt. Jude Cloud log in page\n on DNAnexus, enter the username and password that you registered with from the preceding section, and click 'LOG IN'.\n\n\nBilling Setup\n\n\n\n\nClick on the drop down next to your user name in the far right of the DNAnexus navigation bar, and select 'Profile'.\n\n\nClick on 'Billing Account' from the tabs listed just under the navigation bar.\n\n\nClick on the green 'ADD BILLING INFO' button to the right of your account name.\n\n\n\nA window labeled 'ACCOUNT UPGRADE' will pop up. In this window agree to DNAnexus's Terms of Service, agree to DNAnexus's pricing model, review your account information, and finally enter your billing information.\n\n\nClick 'Upgrade Account'. This will send an email to the individual listed as the billing contact requesting that they verify the change.\n\n\n\n\n\n\nNote\n\n\nIf you are unable to log in at this link, it probably means you have already set up a DNAnexus account through the \nDNANexus log in page\n using your St. Jude email address. In this case, a DNAnexus account is already linked to your St. Jude email. To continue using this account, you will need to log in through the \nDNAnexus log in page\n.\n\n\n\n\n\n\nNote\n\n\nOn step 4, you must enter the billing contact's name, physical address, email address and phone number. You do not need to enter any credit card information. Once the billing contact has verified the account upgrade request, your account will be credited $50.\n\n\n\n\nPlease \ncontact us\n for help if you encounter any problems creating an account.",
            "title": "Create An Account"
        },
        {
            "location": "/create-an-account/#create-an-account",
            "text": "In order to request St. Jude data and work with that data in the cloud, you will need to create a DNAnexus account and use it to log in to St. Jude Cloud. St. Jude Cloud is built on top of the  DNAnexus  genomics cloud ecosystem. Each new user will receive a $50 credit upon creation of their St. Jude Cloud account with DNAnexus (see the  note  in the Billing Setup section).   Note  The account creation and login process is slightly different if you are an internal user (you work at St. Jude). Internal users please go to the  intranet home page  and type 'Bioinformatics Self-Service' into the search bar. From there, click on the link that says 'Bioinformatics Self-Service on St. Jude Cloud' to access the internal guide to creating an account.",
            "title": "Create an account"
        },
        {
            "location": "/create-an-account/#account-creation",
            "text": "If you work at an institution that is not St. Jude (the standard case), you can use the following process to create an account:   Go to the  St. Jude Cloud log in page  on DNAnexus.  Click \"Create an Account\".  Fill in your information.  On the Create New Account page, make sure to select \"Microsoft Azure (westus)\" as the Default Cloud Region.  Click 'CREATE ACCOUNT'",
            "title": "Account Creation"
        },
        {
            "location": "/create-an-account/#logging-in",
            "text": "Simply go to the  St. Jude Cloud log in page  on DNAnexus, enter the username and password that you registered with from the preceding section, and click 'LOG IN'.",
            "title": "Logging in"
        },
        {
            "location": "/create-an-account/#billing-setup",
            "text": "Click on the drop down next to your user name in the far right of the DNAnexus navigation bar, and select 'Profile'.  Click on 'Billing Account' from the tabs listed just under the navigation bar.  Click on the green 'ADD BILLING INFO' button to the right of your account name.  A window labeled 'ACCOUNT UPGRADE' will pop up. In this window agree to DNAnexus's Terms of Service, agree to DNAnexus's pricing model, review your account information, and finally enter your billing information.  Click 'Upgrade Account'. This will send an email to the individual listed as the billing contact requesting that they verify the change.    Note  If you are unable to log in at this link, it probably means you have already set up a DNAnexus account through the  DNANexus log in page  using your St. Jude email address. In this case, a DNAnexus account is already linked to your St. Jude email. To continue using this account, you will need to log in through the  DNAnexus log in page .    Note  On step 4, you must enter the billing contact's name, physical address, email address and phone number. You do not need to enter any credit card information. Once the billing contact has verified the account upgrade request, your account will be credited $50.   Please  contact us  for help if you encounter any problems creating an account.",
            "title": "Billing Setup"
        },
        {
            "location": "/guides/data/about-our-data/",
            "text": "About Our Data\n\n\nFile Formats\n\n\nSt. Jude Cloud hosts both raw genomic data files and processed results files:\n\n\n\n\n\n\n\n\nFile Type\n\n\nShort Description\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nBAM\n\n\nHG38 aligned BAM files produced by \nMicrosoft Genomics Service\n (DNA-Seq) or STAR 2-pass mapping (RNA-Seq).\n\n\nClick here\n\n\n\n\n\n\ngVCF\n\n\nGenomic VCF\n files produced by \nMicrosoft Genomics Service\n.\n\n\nClick here\n\n\n\n\n\n\nSomatic VCF\n\n\nCurated list of somatic variants produced by the St. Jude somatic variant analysis pipeline.\n\n\nClick here\n\n\n\n\n\n\nCNV\n\n\nList of somatic copy number alterations produced by St. Jude CONSERTING pipeline.\n\n\nClick here\n\n\n\n\n\n\n\n\nBAM files\n\n\nIn St. Jude Cloud, we store aligned sequence reads in BAM file format for whole genome sequencing, whole exome sequencing, and RNA-seq. For more information on SAM/BAM files, please refer to the \nSAM/BAM specification\n. For research samples, we require the standard 30X coverage for whole genome and 100X for whole exome sequencing. For clinical samples, we require higher coverage, 45X, for whole genome sequencing due to tumor purity issues found in clinical tumor specimens. For RNA-Seq, since only a subset of genes are expressed in a specific tissue, we require 30% of the exons to have 20X coverage in order to ensure that at least 30% of the expressed genes have sufficient coverage.\n\n\ngVCF files\n\n\nWe provide gVCF files produced by the \nMicrosoft Genomics Service\n. gVCF files are derived from the BAM files produced above as called by \nGATK's haplotype caller\n. Today, we defer to \nthe official specification document\n from the Broad Institute, as well as \nthis discussion\n on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to \nthe official Microsoft Genomics whitepaper\n.\n\n\nSomatic VCF files\n\n\nSomatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking:\n\n\n\n\nReads were aligned to HG19 using \nbwa backtrack\n (\nbwa aln\n + \nbwa sampe\n) using default parameters.\n\n\nPost processing of aligned reads was performed using \nPicard\n \nCleanSam\n and \nMarkDuplicates\n.\n\n\nVariants were called using the \nBambino\n variant caller (you can download Bambino \nhere\n or by navigating to the \nZhang Lab page\n where the  \"Bambino package\" is listed as a dependency under the CONSERTING section).\n\n\nVariants were post-processed using an in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available.\n\n\nVariants were manually reviewed by analysts and published with \nthe relevant Pediatric Cancer Genome Project (PCGP) paper\n.\n\n\nPost-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the \nHG19\n INFO field.).\n\n\n\n\n\n\nNote\n\n\nOur Somatic VCF files were designed specifically for St. Jude Cloud visualization purposes. Variants in these files were manually curated from analyses across multiple sequencing types including WGS and WES.\n\nFor more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed \nhere\n.\n\n\n\n\nCNV files\n\n\nCNV files contain copy number alteration (CNA) analysis results for paired tumor-normal WGS samples. Files are produced by running paired tumor-normal BAM files through the \nCONSERTING\n pipeline which identifies CNA through iterative analysis of (i) local segmentation by read depth within boundaries identified by structural variation (SV) breakpoints followed by (ii) segment merging and local SV analysis. \nCREST\n was used to identify local SV breakpoints. CNV files contain the following information:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nchrom\n\n\nchromosome\n\n\n\n\n\n\nloc.start\n\n\nstart of segment\n\n\n\n\n\n\nloc.end\n\n\nend of segment\n\n\n\n\n\n\nnum.mark\n\n\nnumber of windows retained in the segment (gaps and windows with low mappability are excluded)\n\n\n\n\n\n\nlength.ratio\n\n\nThe ratio between the length of the used windows to the genomic length\n\n\n\n\n\n\nseg.mean\n\n\nThe estimated GC corrected difference signal (2 copy gain will have a seg.mean of 1)\n\n\n\n\n\n\nGMean\n\n\nThe mean coverage in the germline sample (a value of 1 represents diploid)\n\n\n\n\n\n\nDMean\n\n\nThe mean coverage in the tumor sample\n\n\n\n\n\n\nLogRatio\n\n\nLog2 ratio between tumor and normal coverage\n\n\n\n\n\n\nQuality score\n\n\nA empirical score used in merging\n\n\n\n\n\n\nSV_Matching\n\n\nWhether the boundary of the segments were supported by SVs (3: both ends supported, 2: right end supported, 1: left end supported, 0: neither end supported)\n\n\n\n\n\n\n\n\nSequencing Information\n\n\nWhole Genome and Whole Exome\n\n\nWhole Genome Sequence (WGS) and Whole Exome Sequence (WES) BAM files were produced by the \nMicrosoft Genomics Service\n aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to \nthe official Microsoft Genomics whitepaper\n.\n\n\nRNA-Seq\n\n\nRNA-Seq BAM files are mapped to HG38 + \nERCC Spike In Sequences\n (commonly used for normalization of expression analyses). For alignment, \nSTAR\n v2.5.3a 2-pass mapping followed by \nPicard MarkDuplicates\n. Below is the \nSTAR\n command used during alignment. For more information about any of the parameters used, please refer to the \nSTAR manual\n for v2.5.3a.\n\n\n    STAR \n\\\n\n        --runThreadN \n$NUM_THREADS\n \n\\ \n                          \n# $NUM_THREADS is the number of threads to parallelize the alignment across (generally we use 4).\n\n        --genomeDir \n$GENOME_DIR\n \n\\ \n                            \n# $GENOME_DIR is a STAR reference directory containing HG38 and ERCC Spike In sequences.\n\n        --readFilesIn \n$READ_FILES\n \n\\ \n                          \n# $READ_FILES are the input FastQ files to align.\n\n        --limitBAMsortRAM \n$MEMORY_LIMIT\n \n\\ \n                    \n# $MEMORY_LIMIT is a upper limit on the amount of RAM to use in the alignment.\n\n        --outFileNamePrefix \n$OUT_FILE_PREFIX\n \n\\\n\n        --outSAMtype BAM SortedByCoordinate \n\\\n\n        --outSAMstrandField intronMotif \n\\\n\n        --outSAMattributes NH   HI   AS   nM   NM   MD   XS \n\\\n\n        --outSAMunmapped Within \n\\\n\n        --outSAMattrRGline \n$RGs\n \n\\ \n                            \n# $RGs is the read group information for each FastQ passed in $READ_FILES.\n\n        --outFilterMultimapNmax \n20\n \n\\\n\n        --outFilterMultimapScoreRange \n1\n \n\\\n\n        --outFilterScoreMinOverLread \n0\n.66 \n\\\n\n        --outFilterMatchNminOverLread \n0\n.66 \n\\\n\n        --outFilterMismatchNmax \n10\n \n\\\n\n        --alignIntronMax \n500000\n \n\\\n\n        --alignMatesGapMax \n1000000\n \n\\\n\n        --alignSJDBoverhangMin \n1\n \n\\\n\n        --sjdbScore \n2\n \n\\\n\n        --twopassMode Basic\n\n\n\n\n\nData Access Units\n\n\nA Data Access Unit is a grouping of data that typically corresponds to a dataset generated at the same time at the same institution, and can also correspond to a specific study. Each DAU has its own Data Access Committee, which contains the researchers who reside over the data. Each Data Access Committee has its own protocols for approving access to their DAU. Please \ncontact us\n if you have questions about committee approval protocols. \nBasic clinical data\n is available for relevant subjects in each DAU. \n\n\nPediatric Cancer Genome Project (PCGP)\n\n\nPCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer.\n\nThe Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients. \n\n\nSt. Jude Lifetime (SJLIFE)\n\n\nSJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy.\n\nSt. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples. \n\n\nClinical Genomics (Clinical Pilot, G4K, and RTCG)\n\n\nClinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors.\n\nClinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of three studies: Clinical Pilot, Genomes4Kids, and Real-time Clinical Genomics. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. The RTCG study aims to release Clinical Genomics data in real time to the research community. The goal of these studies is to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors. \n\n\nSickle Cell Genome Project (SGP)\n\n\nSGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood.\n\nThe Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.\n\n\nChildhood Cancer Survivor Study (CCSS)\n\n\nCCSS is a germline-only dataset consisting of whole genome sequencing of childhood cancer survivors.\n\nCCSS is a multi-institutional, multi-disciplinary, NCI-funded collaborative resource established to evaluate long-term outcomes among survivors of childhood cancer. It is a retrospective cohort consisting of >24,000 five-year survivors of childhood cancer who were diagnosed between 1970-1999 at one of 31 participating centers in the U.S. and Canada. The primary purpose of this sequencing of CCSS participants is to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. \n\n\n\n\nCCSS: Potential Bacterial Contamination\n\n\nSamples for the Childhood Cancer Survivorship Study were collected by sending out Buccal swab kits to enrolled participants and having them complete the kits at home. This mechanism of collecting saliva and buccal cells for sequencing is highly desirable because of its non-invasive nature and ease of execution. However, collection of samples in this manner also has higher probability of contamination from external sources (as compared to, say, samples collected using blood). We have observed some samples in this cohort which suffer from bacterial contamination. To address this issue, we have taken the following steps:\n\n\n\n\nWe have estimated the bacterial contamination rate and annotated each of the samples in the CCSS cohort. For each sample, you will find the estimated contamination rate in the \nDescription\n field of the \nSAMPLE_INFO.txt\n file that is vended with your data (and as a property on the DNAnexus file). For information on this field, see the \nMetadata specification\n.\n\n\nUsing this estimated contamination rate, we have removed 82 samples which exhibited large rates of bacterial contamination.\n\n\nFor the remaining samples, we have provided the \nBAM\n file as aligned with \nbwa mem\n with default parameters. We have observed that there are instances of reads originating from bacterial contamination that are erroneously mapped to the human genome and display a \nvery\n low mapping quality. Please be advised that we have kept these reads as they were aligned and have not yet made any attempt to unmap these reads. Any analysis you perform on these samples will need to take this into account!\n\n\nLast, we will be working over the coming months to unmap the reads originating from bacterial contamination and release updated \nBAM\n files along with the associated \ngVCF\n files from Microsoft Genomics Service.\n\n\n\n\nWith any questions on the nature or implications of this warning, please contact us at \nsupport@stjude.cloud\n.\n\n\n\n\nMetadata\n\n\nEach data request includes a text file called \nSAMPLE_INFO.txt\n that provides a number of file level properties (sample identifiers, clinical attributes, etc).\n\n\nStandard Metadata\n\n\nBelow are the set of tags which \nmay\n exist for any given file in St. Jude Cloud. All optional metadata will have \nsj_\n prepended to their tag name.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfile_path\n\n\nThe path to the file in your St. Jude Cloud project.\n\n\n\n\n\n\nsubject_name\n\n\nA unique subject identifier assigned internally at St. Jude.\n\n\n\n\n\n\nsample_name\n\n\nA unique sample identifier assigned internally at St. Jude.\n\n\n\n\n\n\nsample_type\n\n\nOne of Autopsy, Cell line, Diagnosis, Germline, Metastasis, Relapse, or Xenograft.\n\n\n\n\n\n\nsequencing_type\n\n\nWhether the file was generated from Whole Genome (WGS), Whole Exome (WES), or RNA-Seq.\n\n\n\n\n\n\nfile_type\n\n\nOne of the \nfile types\n available in St. Jude Cloud.\n\n\n\n\n\n\ndescription\n\n\nOptional field that may contain additional file information.\n\n\n\n\n\n\nsj_diseases\n\n\nShort disease identifier assigned at the time of genomic sequencing. Note that this diagnosis may be refined after undergoing genomic testing. When including diagnosis in your analysis, we recommend you use \nattr_diagnosis\n, which is the most up to date value for diagnosis.\n\n\n\n\n\n\nsj_datasets\n\n\nIf present, the datasets in the data browser which this file is associated with.\n\n\n\n\n\n\nsj_pmid_accessions\n\n\nIf the file was associated with a paper, the related \nPubmed\n accession number.\n\n\n\n\n\n\nsj_ega_accessions\n\n\nIf the file was associated with a paper, the related \nEGA\n accession number.\n\n\n\n\n\n\nsj_dataset_accession\n\n\nIf present, the permanent accession number assigned in St. Jude Cloud.\n\n\n\n\n\n\nsj_embargo_date\n\n\nThe \nembargo date\n, which specifies the first date which the files can be used in a publication.\n\n\n\n\n\n\n\n\nClinical and Phenotypic Information\n\n\nAlso included is a set of phenotypic information queried from the physician or research team's records at the time of sample submission to St. Jude Cloud. These are all considered to be \noptional\n, as the level of information gathered for each sample varies. If empty, the physician or research team did not indicate a value for the field. All basic clinical or phenotypic information will have \nattr_\n prepended to their tag name.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nattr_age_at_diagnosis\n\n\nAge at first diagnosis. This field is normalized as a decimal value. If empty, the physician or research team did not indicate a value for this field.\n\n\n\n\n\n\nattr_diagnosis\n\n\nPrimary diagnosis reported by the clinic.\n\n\n\n\n\n\nattr_ethnicity\n\n\nSelf-reported ethnicity. Values are normalized according to the \nUS Census Bureau classifications\n.\n\n\n\n\n\n\nattr_race\n\n\nSelf-reported race. Values are normalized according to the \nUS Census Bureau classifications\n.\n\n\n\n\n\n\nattr_sex\n\n\nSelf-reported sex.\n\n\n\n\n\n\nattr_oncotree_disease_code\n\n\nThe disease code (assigned at the time of genomic sequencing) as specified by \nOncotree Version 2019-03-01\n.\n\n\n\n\n\n\n\n\nShort Disease Code Mapping\n\n\nEmbedded in both the filename and the \nSAMPLE_INFO.txt\n file that comes with your data request will be a list of short diagnosis codes (\nsj_diseases\n). These short codes were assigned at the time that the sample was sent for sequencing, and they are not necessarily the final diagnosis (\nattr_diagnosis\n). For instance, the diagnosis is often refined as the sample undergoes genomic testing. Below, we include a table of short disease code to long disease name mappings so you can interpret what these abbreviations mean.\n\n\n\n\n\n\n\n\nShort Disease Code\n\n\nLong Diagnosis Description\n\n\n\n\n\n\n\n\n\n\nACT\n\n\nAdrenocortical Carcinoma\n\n\n\n\n\n\nAEL\n\n\nAcute erythroid leukemia (AML M6)\n\n\n\n\n\n\nALCL\n\n\nAnaplastic Large Cell Lymphoma\n\n\n\n\n\n\nALL\n\n\nAcute Lymphoblastic Leukemia\n\n\n\n\n\n\nALS\n\n\nAmyotrophic Lateral Sclerosis (\"Lou Gehrig's Disease\"\")\"\n\n\n\n\n\n\nALZ\n\n\nAlzheimer's Disease\n\n\n\n\n\n\nAML\n\n\nAcute Myeloid Leukemia\n\n\n\n\n\n\nAMLM\n\n\nAcute Megakaryoblastic Leukemia\n\n\n\n\n\n\nANEM\n\n\nAnemia\n\n\n\n\n\n\nASPS\n\n\nAlveolar Soft Part Sarcoma\n\n\n\n\n\n\nAUL\n\n\nAcute Undifferentiated Leukemia\n\n\n\n\n\n\nBALL\n\n\nB-cell Acute Lymphoblastic Leukemia\n\n\n\n\n\n\nBCC\n\n\nBasal Cell Carcinoma\n\n\n\n\n\n\nBLACA\n\n\nBladder Cancer\n\n\n\n\n\n\nBT\n\n\nBrain Tumor\n\n\n\n\n\n\nCA\n\n\nCarcinoma\n\n\n\n\n\n\nCBF\n\n\nAcute Myeloid Leukemia - Core Binding Factor subtype\n\n\n\n\n\n\nCLL\n\n\nChronic Lymphocytic Leukemia\n\n\n\n\n\n\nCML\n\n\nChronic Myelogenous Leukemia\n\n\n\n\n\n\nCMML\n\n\nChronic Myelomonocytic Leukemia\n\n\n\n\n\n\nCMV\n\n\nCytomegalovirus\n\n\n\n\n\n\nCNS\n\n\nCentral Nervous System\n\n\n\n\n\n\nCPC\n\n\nChoroid Plexus Carcinoma\n\n\n\n\n\n\nCRC\n\n\nColorectal Cancer\n\n\n\n\n\n\nCS\n\n\nChondrosarcoma\n\n\n\n\n\n\nCTP\n\n\nCongenital Thrombocytopenia\n\n\n\n\n\n\nDIPG\n\n\nDiffuse Intrinsic Pontine Glioma\n\n\n\n\n\n\nDLBCL\n\n\nDiffuse Large B-cell Lymphoma\n\n\n\n\n\n\nDOWN\n\n\nDown Syndrome\n\n\n\n\n\n\nDSRCT\n\n\nDesmoplastic Small Round Cell Tumor\n\n\n\n\n\n\nE2A\n\n\nB-Lineage Acute Lymphoblastic Leukemia - E2A-PBX1 subtype\n\n\n\n\n\n\nECD\n\n\nErdheim-Chester Disease\n\n\n\n\n\n\nEPD\n\n\nEpendymoma\n\n\n\n\n\n\nERG\n\n\nAcute Lymphoblastic Leukemia - ERG alteration subtype\n\n\n\n\n\n\nETV\n\n\nAcute Lymphoblastic Leukemia - ETV6-RUNX1 fusion subtype\n\n\n\n\n\n\nEWS\n\n\nEwing's Sarcoma\n\n\n\n\n\n\nGCT\n\n\nGerm Cell Tumor\n\n\n\n\n\n\nGENBN\n\n\nGeneral Bone\n\n\n\n\n\n\nGENKY\n\n\nGeneral Kidney\n\n\n\n\n\n\nGENLK\n\n\nGeneral Leukemia\n\n\n\n\n\n\nGICT\n\n\nGiant Cell Tumor\n\n\n\n\n\n\nGIST\n\n\nGastrointestinal Stromal Tumor\n\n\n\n\n\n\nHB\n\n\nHepatoblastoma\n\n\n\n\n\n\nHGG\n\n\nHigh Grade Glioma\n\n\n\n\n\n\nHGS\n\n\nHigh Grade Sarcoma\n\n\n\n\n\n\nHIST\n\n\nHistiocytosis\n\n\n\n\n\n\nHL\n\n\nHodgkin's Lymphoma\n\n\n\n\n\n\nHM\n\n\nHematopoietic Malignancies\n\n\n\n\n\n\nHS\n\n\nHidradenitis Suppurativa\n\n\n\n\n\n\nHYPER\n\n\nAcute Lymphoblastic Leukemia - Hyperdiploid subtype\n\n\n\n\n\n\nHYPO\n\n\nAcute Lymphoblastic Leukemia - Hypodiploid subtype\n\n\n\n\n\n\nIFS\n\n\nInfantile Fibromyosarcoma\n\n\n\n\n\n\nINF\n\n\nAcute Lymphoblastic Leukemia (Infant)\n\n\n\n\n\n\nITP\n\n\nIdiopathic Thrombocytopenia\n\n\n\n\n\n\nJMML\n\n\nJuvenile Myelomonocytic Leukemia\n\n\n\n\n\n\nLCH\n\n\nLangerhans Cell Histiocytocis\n\n\n\n\n\n\nLGG\n\n\nLow Grade Glioma\n\n\n\n\n\n\nLM\n\n\nLiver Malignancies\n\n\n\n\n\n\nMB\n\n\nMedulloblastoma\n\n\n\n\n\n\nMDS\n\n\nMyelodysplastic Syndrome\n\n\n\n\n\n\nMEL\n\n\nMelanoma\n\n\n\n\n\n\nMLL\n\n\nMixed Lineage Leukemia\n\n\n\n\n\n\nMM\n\n\nMultiple Myeloma\n\n\n\n\n\n\nMPAL\n\n\nAcute Lymphoblastic Leukemia - Multi-phenotypic\n\n\n\n\n\n\nMPNST\n\n\nMalignant Peripheral Nerve Sheath Tumor\n\n\n\n\n\n\nMRT\n\n\nMalignant Rhabdoid Tumour\n\n\n\n\n\n\nMYF\n\n\nMyelofibrosis\n\n\n\n\n\n\nNBL\n\n\nNeuroblastoma\n\n\n\n\n\n\nNEUTP\n\n\nNeutropenia\n\n\n\n\n\n\nNHL\n\n\nNon-Hodgkin's Lymphoma\n\n\n\n\n\n\nNM\n\n\nNon-malignancy\n\n\n\n\n\n\nNORM\n\n\nControl Sample\n\n\n\n\n\n\nNPC\n\n\nNasopharyngeal Carcinoma\n\n\n\n\n\n\nNPCA\n\n\nNasopharyngeal Carcinoma\n\n\n\n\n\n\nOS\n\n\nOsteosarcoma\n\n\n\n\n\n\nPF\n\n\nPosterior Fossa\n\n\n\n\n\n\nPGL\n\n\nParaganglioma\n\n\n\n\n\n\nPHALL\n\n\nAcute Lymphoblastic Leukemia - BCR-ABL1 fusion subtype\n\n\n\n\n\n\nPHCML\n\n\nPh+ Chronic Myeloid Leukemia\n\n\n\n\n\n\nPML\n\n\nPromyelocitic Leukemia\n\n\n\n\n\n\nPRAD\n\n\nProstate Adenocarcoma\n\n\n\n\n\n\nPSO\n\n\nPsoriasis\n\n\n\n\n\n\nRB\n\n\nRetinoblastoma\n\n\n\n\n\n\nRCC\n\n\nRenal cell carcinoma\n\n\n\n\n\n\nRECA\n\n\nRenal Cancer\n\n\n\n\n\n\nRHB\n\n\nRhabdomyosarcoma\n\n\n\n\n\n\nSBO\n\n\nSpina Bifida Occulta\n\n\n\n\n\n\nSCD\n\n\nSickle Cell Disease\n\n\n\n\n\n\nSCZ\n\n\nSchizophrenia\n\n\n\n\n\n\nSS\n\n\nSynovial Sarcoma\n\n\n\n\n\n\nST\n\n\nSolid Tumor\n\n\n\n\n\n\nSTS\n\n\nSoft Tissue Sarcoma\n\n\n\n\n\n\nTALL\n\n\nT-cell Acute Lymphoblastic Leukemia\n\n\n\n\n\n\nTCP\n\n\nThrombocytopenia\n\n\n\n\n\n\nTESCA\n\n\nTesticular Cancer\n\n\n\n\n\n\nTHCA\n\n\nThyroid Carcinoma\n\n\n\n\n\n\nWLM\n\n\nWilms' tumor",
            "title": "About Our Data"
        },
        {
            "location": "/guides/data/about-our-data/#about-our-data",
            "text": "",
            "title": "About Our Data"
        },
        {
            "location": "/guides/data/about-our-data/#file-formats",
            "text": "St. Jude Cloud hosts both raw genomic data files and processed results files:     File Type  Short Description  Details      BAM  HG38 aligned BAM files produced by  Microsoft Genomics Service  (DNA-Seq) or STAR 2-pass mapping (RNA-Seq).  Click here    gVCF  Genomic VCF  files produced by  Microsoft Genomics Service .  Click here    Somatic VCF  Curated list of somatic variants produced by the St. Jude somatic variant analysis pipeline.  Click here    CNV  List of somatic copy number alterations produced by St. Jude CONSERTING pipeline.  Click here",
            "title": "File Formats"
        },
        {
            "location": "/guides/data/about-our-data/#bam-files",
            "text": "In St. Jude Cloud, we store aligned sequence reads in BAM file format for whole genome sequencing, whole exome sequencing, and RNA-seq. For more information on SAM/BAM files, please refer to the  SAM/BAM specification . For research samples, we require the standard 30X coverage for whole genome and 100X for whole exome sequencing. For clinical samples, we require higher coverage, 45X, for whole genome sequencing due to tumor purity issues found in clinical tumor specimens. For RNA-Seq, since only a subset of genes are expressed in a specific tissue, we require 30% of the exons to have 20X coverage in order to ensure that at least 30% of the expressed genes have sufficient coverage.",
            "title": "BAM files"
        },
        {
            "location": "/guides/data/about-our-data/#gvcf-files",
            "text": "We provide gVCF files produced by the  Microsoft Genomics Service . gVCF files are derived from the BAM files produced above as called by  GATK's haplotype caller . Today, we defer to  the official specification document  from the Broad Institute, as well as  this discussion  on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to  the official Microsoft Genomics whitepaper .",
            "title": "gVCF files"
        },
        {
            "location": "/guides/data/about-our-data/#somatic-vcf-files",
            "text": "Somatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking:   Reads were aligned to HG19 using  bwa backtrack  ( bwa aln  +  bwa sampe ) using default parameters.  Post processing of aligned reads was performed using  Picard   CleanSam  and  MarkDuplicates .  Variants were called using the  Bambino  variant caller (you can download Bambino  here  or by navigating to the  Zhang Lab page  where the  \"Bambino package\" is listed as a dependency under the CONSERTING section).  Variants were post-processed using an in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available.  Variants were manually reviewed by analysts and published with  the relevant Pediatric Cancer Genome Project (PCGP) paper .  Post-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the  HG19  INFO field.).    Note  Our Somatic VCF files were designed specifically for St. Jude Cloud visualization purposes. Variants in these files were manually curated from analyses across multiple sequencing types including WGS and WES. \nFor more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed  here .",
            "title": "Somatic VCF files"
        },
        {
            "location": "/guides/data/about-our-data/#cnv-files",
            "text": "CNV files contain copy number alteration (CNA) analysis results for paired tumor-normal WGS samples. Files are produced by running paired tumor-normal BAM files through the  CONSERTING  pipeline which identifies CNA through iterative analysis of (i) local segmentation by read depth within boundaries identified by structural variation (SV) breakpoints followed by (ii) segment merging and local SV analysis.  CREST  was used to identify local SV breakpoints. CNV files contain the following information:     Field  Description      chrom  chromosome    loc.start  start of segment    loc.end  end of segment    num.mark  number of windows retained in the segment (gaps and windows with low mappability are excluded)    length.ratio  The ratio between the length of the used windows to the genomic length    seg.mean  The estimated GC corrected difference signal (2 copy gain will have a seg.mean of 1)    GMean  The mean coverage in the germline sample (a value of 1 represents diploid)    DMean  The mean coverage in the tumor sample    LogRatio  Log2 ratio between tumor and normal coverage    Quality score  A empirical score used in merging    SV_Matching  Whether the boundary of the segments were supported by SVs (3: both ends supported, 2: right end supported, 1: left end supported, 0: neither end supported)",
            "title": "CNV files"
        },
        {
            "location": "/guides/data/about-our-data/#sequencing-information",
            "text": "",
            "title": "Sequencing Information"
        },
        {
            "location": "/guides/data/about-our-data/#whole-genome-and-whole-exome",
            "text": "Whole Genome Sequence (WGS) and Whole Exome Sequence (WES) BAM files were produced by the  Microsoft Genomics Service  aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to  the official Microsoft Genomics whitepaper .",
            "title": "Whole Genome and Whole Exome"
        },
        {
            "location": "/guides/data/about-our-data/#rna-seq",
            "text": "RNA-Seq BAM files are mapped to HG38 +  ERCC Spike In Sequences  (commonly used for normalization of expression analyses). For alignment,  STAR  v2.5.3a 2-pass mapping followed by  Picard MarkDuplicates . Below is the  STAR  command used during alignment. For more information about any of the parameters used, please refer to the  STAR manual  for v2.5.3a.      STAR  \\ \n        --runThreadN  $NUM_THREADS   \\                             # $NUM_THREADS is the number of threads to parallelize the alignment across (generally we use 4). \n        --genomeDir  $GENOME_DIR   \\                               # $GENOME_DIR is a STAR reference directory containing HG38 and ERCC Spike In sequences. \n        --readFilesIn  $READ_FILES   \\                             # $READ_FILES are the input FastQ files to align. \n        --limitBAMsortRAM  $MEMORY_LIMIT   \\                       # $MEMORY_LIMIT is a upper limit on the amount of RAM to use in the alignment. \n        --outFileNamePrefix  $OUT_FILE_PREFIX   \\ \n        --outSAMtype BAM SortedByCoordinate  \\ \n        --outSAMstrandField intronMotif  \\ \n        --outSAMattributes NH   HI   AS   nM   NM   MD   XS  \\ \n        --outSAMunmapped Within  \\ \n        --outSAMattrRGline  $RGs   \\                               # $RGs is the read group information for each FastQ passed in $READ_FILES. \n        --outFilterMultimapNmax  20   \\ \n        --outFilterMultimapScoreRange  1   \\ \n        --outFilterScoreMinOverLread  0 .66  \\ \n        --outFilterMatchNminOverLread  0 .66  \\ \n        --outFilterMismatchNmax  10   \\ \n        --alignIntronMax  500000   \\ \n        --alignMatesGapMax  1000000   \\ \n        --alignSJDBoverhangMin  1   \\ \n        --sjdbScore  2   \\ \n        --twopassMode Basic",
            "title": "RNA-Seq"
        },
        {
            "location": "/guides/data/about-our-data/#data-access-units",
            "text": "A Data Access Unit is a grouping of data that typically corresponds to a dataset generated at the same time at the same institution, and can also correspond to a specific study. Each DAU has its own Data Access Committee, which contains the researchers who reside over the data. Each Data Access Committee has its own protocols for approving access to their DAU. Please  contact us  if you have questions about committee approval protocols.  Basic clinical data  is available for relevant subjects in each DAU.",
            "title": "Data Access Units"
        },
        {
            "location": "/guides/data/about-our-data/#pediatric-cancer-genome-project-pcgp",
            "text": "PCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer. \nThe Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients.",
            "title": "Pediatric Cancer Genome Project (PCGP)"
        },
        {
            "location": "/guides/data/about-our-data/#st-jude-lifetime-sjlife",
            "text": "SJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy. \nSt. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples.",
            "title": "St. Jude Lifetime (SJLIFE)"
        },
        {
            "location": "/guides/data/about-our-data/#clinical-genomics-clinical-pilot-g4k-and-rtcg",
            "text": "Clinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors. \nClinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of three studies: Clinical Pilot, Genomes4Kids, and Real-time Clinical Genomics. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. The RTCG study aims to release Clinical Genomics data in real time to the research community. The goal of these studies is to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors.",
            "title": "Clinical Genomics (Clinical Pilot, G4K, and RTCG)"
        },
        {
            "location": "/guides/data/about-our-data/#sickle-cell-genome-project-sgp",
            "text": "SGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood. \nThe Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.",
            "title": "Sickle Cell Genome Project (SGP)"
        },
        {
            "location": "/guides/data/about-our-data/#childhood-cancer-survivor-study-ccss",
            "text": "CCSS is a germline-only dataset consisting of whole genome sequencing of childhood cancer survivors. \nCCSS is a multi-institutional, multi-disciplinary, NCI-funded collaborative resource established to evaluate long-term outcomes among survivors of childhood cancer. It is a retrospective cohort consisting of >24,000 five-year survivors of childhood cancer who were diagnosed between 1970-1999 at one of 31 participating centers in the U.S. and Canada. The primary purpose of this sequencing of CCSS participants is to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy.    CCSS: Potential Bacterial Contamination  Samples for the Childhood Cancer Survivorship Study were collected by sending out Buccal swab kits to enrolled participants and having them complete the kits at home. This mechanism of collecting saliva and buccal cells for sequencing is highly desirable because of its non-invasive nature and ease of execution. However, collection of samples in this manner also has higher probability of contamination from external sources (as compared to, say, samples collected using blood). We have observed some samples in this cohort which suffer from bacterial contamination. To address this issue, we have taken the following steps:   We have estimated the bacterial contamination rate and annotated each of the samples in the CCSS cohort. For each sample, you will find the estimated contamination rate in the  Description  field of the  SAMPLE_INFO.txt  file that is vended with your data (and as a property on the DNAnexus file). For information on this field, see the  Metadata specification .  Using this estimated contamination rate, we have removed 82 samples which exhibited large rates of bacterial contamination.  For the remaining samples, we have provided the  BAM  file as aligned with  bwa mem  with default parameters. We have observed that there are instances of reads originating from bacterial contamination that are erroneously mapped to the human genome and display a  very  low mapping quality. Please be advised that we have kept these reads as they were aligned and have not yet made any attempt to unmap these reads. Any analysis you perform on these samples will need to take this into account!  Last, we will be working over the coming months to unmap the reads originating from bacterial contamination and release updated  BAM  files along with the associated  gVCF  files from Microsoft Genomics Service.   With any questions on the nature or implications of this warning, please contact us at  support@stjude.cloud .",
            "title": "Childhood Cancer Survivor Study (CCSS)"
        },
        {
            "location": "/guides/data/about-our-data/#metadata",
            "text": "Each data request includes a text file called  SAMPLE_INFO.txt  that provides a number of file level properties (sample identifiers, clinical attributes, etc).",
            "title": "Metadata"
        },
        {
            "location": "/guides/data/about-our-data/#standard-metadata",
            "text": "Below are the set of tags which  may  exist for any given file in St. Jude Cloud. All optional metadata will have  sj_  prepended to their tag name.     Property  Description      file_path  The path to the file in your St. Jude Cloud project.    subject_name  A unique subject identifier assigned internally at St. Jude.    sample_name  A unique sample identifier assigned internally at St. Jude.    sample_type  One of Autopsy, Cell line, Diagnosis, Germline, Metastasis, Relapse, or Xenograft.    sequencing_type  Whether the file was generated from Whole Genome (WGS), Whole Exome (WES), or RNA-Seq.    file_type  One of the  file types  available in St. Jude Cloud.    description  Optional field that may contain additional file information.    sj_diseases  Short disease identifier assigned at the time of genomic sequencing. Note that this diagnosis may be refined after undergoing genomic testing. When including diagnosis in your analysis, we recommend you use  attr_diagnosis , which is the most up to date value for diagnosis.    sj_datasets  If present, the datasets in the data browser which this file is associated with.    sj_pmid_accessions  If the file was associated with a paper, the related  Pubmed  accession number.    sj_ega_accessions  If the file was associated with a paper, the related  EGA  accession number.    sj_dataset_accession  If present, the permanent accession number assigned in St. Jude Cloud.    sj_embargo_date  The  embargo date , which specifies the first date which the files can be used in a publication.",
            "title": "Standard Metadata"
        },
        {
            "location": "/guides/data/about-our-data/#clinical-and-phenotypic-information",
            "text": "Also included is a set of phenotypic information queried from the physician or research team's records at the time of sample submission to St. Jude Cloud. These are all considered to be  optional , as the level of information gathered for each sample varies. If empty, the physician or research team did not indicate a value for the field. All basic clinical or phenotypic information will have  attr_  prepended to their tag name.     Property  Description      attr_age_at_diagnosis  Age at first diagnosis. This field is normalized as a decimal value. If empty, the physician or research team did not indicate a value for this field.    attr_diagnosis  Primary diagnosis reported by the clinic.    attr_ethnicity  Self-reported ethnicity. Values are normalized according to the  US Census Bureau classifications .    attr_race  Self-reported race. Values are normalized according to the  US Census Bureau classifications .    attr_sex  Self-reported sex.    attr_oncotree_disease_code  The disease code (assigned at the time of genomic sequencing) as specified by  Oncotree Version 2019-03-01 .",
            "title": "Clinical and Phenotypic Information"
        },
        {
            "location": "/guides/data/about-our-data/#short-disease-code-mapping",
            "text": "Embedded in both the filename and the  SAMPLE_INFO.txt  file that comes with your data request will be a list of short diagnosis codes ( sj_diseases ). These short codes were assigned at the time that the sample was sent for sequencing, and they are not necessarily the final diagnosis ( attr_diagnosis ). For instance, the diagnosis is often refined as the sample undergoes genomic testing. Below, we include a table of short disease code to long disease name mappings so you can interpret what these abbreviations mean.     Short Disease Code  Long Diagnosis Description      ACT  Adrenocortical Carcinoma    AEL  Acute erythroid leukemia (AML M6)    ALCL  Anaplastic Large Cell Lymphoma    ALL  Acute Lymphoblastic Leukemia    ALS  Amyotrophic Lateral Sclerosis (\"Lou Gehrig's Disease\"\")\"    ALZ  Alzheimer's Disease    AML  Acute Myeloid Leukemia    AMLM  Acute Megakaryoblastic Leukemia    ANEM  Anemia    ASPS  Alveolar Soft Part Sarcoma    AUL  Acute Undifferentiated Leukemia    BALL  B-cell Acute Lymphoblastic Leukemia    BCC  Basal Cell Carcinoma    BLACA  Bladder Cancer    BT  Brain Tumor    CA  Carcinoma    CBF  Acute Myeloid Leukemia - Core Binding Factor subtype    CLL  Chronic Lymphocytic Leukemia    CML  Chronic Myelogenous Leukemia    CMML  Chronic Myelomonocytic Leukemia    CMV  Cytomegalovirus    CNS  Central Nervous System    CPC  Choroid Plexus Carcinoma    CRC  Colorectal Cancer    CS  Chondrosarcoma    CTP  Congenital Thrombocytopenia    DIPG  Diffuse Intrinsic Pontine Glioma    DLBCL  Diffuse Large B-cell Lymphoma    DOWN  Down Syndrome    DSRCT  Desmoplastic Small Round Cell Tumor    E2A  B-Lineage Acute Lymphoblastic Leukemia - E2A-PBX1 subtype    ECD  Erdheim-Chester Disease    EPD  Ependymoma    ERG  Acute Lymphoblastic Leukemia - ERG alteration subtype    ETV  Acute Lymphoblastic Leukemia - ETV6-RUNX1 fusion subtype    EWS  Ewing's Sarcoma    GCT  Germ Cell Tumor    GENBN  General Bone    GENKY  General Kidney    GENLK  General Leukemia    GICT  Giant Cell Tumor    GIST  Gastrointestinal Stromal Tumor    HB  Hepatoblastoma    HGG  High Grade Glioma    HGS  High Grade Sarcoma    HIST  Histiocytosis    HL  Hodgkin's Lymphoma    HM  Hematopoietic Malignancies    HS  Hidradenitis Suppurativa    HYPER  Acute Lymphoblastic Leukemia - Hyperdiploid subtype    HYPO  Acute Lymphoblastic Leukemia - Hypodiploid subtype    IFS  Infantile Fibromyosarcoma    INF  Acute Lymphoblastic Leukemia (Infant)    ITP  Idiopathic Thrombocytopenia    JMML  Juvenile Myelomonocytic Leukemia    LCH  Langerhans Cell Histiocytocis    LGG  Low Grade Glioma    LM  Liver Malignancies    MB  Medulloblastoma    MDS  Myelodysplastic Syndrome    MEL  Melanoma    MLL  Mixed Lineage Leukemia    MM  Multiple Myeloma    MPAL  Acute Lymphoblastic Leukemia - Multi-phenotypic    MPNST  Malignant Peripheral Nerve Sheath Tumor    MRT  Malignant Rhabdoid Tumour    MYF  Myelofibrosis    NBL  Neuroblastoma    NEUTP  Neutropenia    NHL  Non-Hodgkin's Lymphoma    NM  Non-malignancy    NORM  Control Sample    NPC  Nasopharyngeal Carcinoma    NPCA  Nasopharyngeal Carcinoma    OS  Osteosarcoma    PF  Posterior Fossa    PGL  Paraganglioma    PHALL  Acute Lymphoblastic Leukemia - BCR-ABL1 fusion subtype    PHCML  Ph+ Chronic Myeloid Leukemia    PML  Promyelocitic Leukemia    PRAD  Prostate Adenocarcoma    PSO  Psoriasis    RB  Retinoblastoma    RCC  Renal cell carcinoma    RECA  Renal Cancer    RHB  Rhabdomyosarcoma    SBO  Spina Bifida Occulta    SCD  Sickle Cell Disease    SCZ  Schizophrenia    SS  Synovial Sarcoma    ST  Solid Tumor    STS  Soft Tissue Sarcoma    TALL  T-cell Acute Lymphoblastic Leukemia    TCP  Thrombocytopenia    TESCA  Testicular Cancer    THCA  Thyroid Carcinoma    WLM  Wilms' tumor",
            "title": "Short Disease Code Mapping"
        },
        {
            "location": "/guides/data/data-request/",
            "text": "Request Process Overview\n\n\nCreating a data request is the premier way to access raw St. Jude next \ngeneration sequencing data in the cloud. You can get a \nfree\n copy of \nthe data in a secure cloud environment powered by \nMicrosoft Azure\n and \n\nDNAnexus\n, or you can elect to download the data to your local computing \nenvironment.\n\n\n\n\nThings to Remember\n\n\n\n\nData in St. Jude Cloud is grouped into \nData Access Units (DAUs)\n, which usually correspond to large-scale sequencing initiatives at St. Jude. \n\n\nIndividuals can \napply for access\n to DAUs on a case-by-case basis for a specific amount of time (usually 1 year).\n\n\nAccess to data in a given DAU is assessed by the corresponding \nData Access Committee\n who reviews a variety of factors to grant access.\n\n\nThere are a number of terms of use and restrictions outlined in the \nData Access Agreement\n. Everyone who will be working with the data must understand and agree to these terms.\n\n\n\n\n\n\nSelecting Data\n\n\nThere are three main ways to make your data selection through our \nData Browser\n. You can peruse our raw genomic data by diagnosis, publication, or study and use a number of filtering options to narrow down your search. You may also be directed to the Genomics Platform through another App to request specific samples. \n\n\nYou must have \ncreated an account\n and be logged in to make a data request. If you have not yet created an account or you are not logged in, the red \nRequest Data\n button will say \nLog In\n.\n\n\n\n\nWe ask that you review your selection and make sure that these are the \nDAUs\n you would like to request. \n\n\n\n\nMaking the Request\n\n\nNow that you have selected your data, you will need to fill in some information to complete the request. From here, necessary information will be collected through a setup wizard. All of your progress will be automatically saved, and you can follow along with your progress on the left sidebar. \n\n\nThis information will be collected whether you are requesting open-access or controlled-access data. It helps us structure your project folder correctly when we vend the data to you. \n\n\n\n\nApplying for Data Access\n\n\nIf you already have access to the data or are requesting open-access data, you will not be prompted to go through this section.\n\n\nSigning the Data Access Agreement\n\n\nEvery person who requests access to our controlled-access data must sign the \nData Access Agreement (DAA)\n. If you are located in the United States of America, you can opt in to completing the DAA through an electronic setup wizard. If you are not located in the USA, or would like to complete the form manually, you can follow our instructions on \nFilling Out The Data Access Agreement\n.\n\n\nIf you opt to do the process through the setup wizard, the necessary information will be collected and added automatically to your agreement. Once you have completed the setup wizard, the form will be sent to you and necessary signatories through email via \nDocuSign\n. You can learn more about our electronic data access agreement process \nhere\n. \n\n\n\n\nTip\n\n\nIf you receive an email from us that your DAA is incomplete, you may edit your DAA and upload the revised copy using the 'Add a Form' button the on Manage Data page. \n\n\n\n\nChecking your Request Status\n\n\nOnce you start the Electronic Data Access Agreement process, you will have a draft autosaved for you on your \nManage Data\n page, accessible at any time.\n    \n\n\n\n\nPending Request Types\n\n\n\n\n\n\nRequest 1 is an Open Draft, meaning the requestor has not finished the setup wizard, and that the DocuSign envelope has not been sent to any of the signatories. \n\n\n\n\n\n\nRequest 2, listed in the Projects section, has been sent to the signatories, but has not been completed by all of them. This status will look like the Request 3 when all of the signatories sign the document and it is ready to be sent to the Data Access Committee(s). \n\n\n\n\n\n\nRequest 3 is pending approval from the Data Access Committee(s), and the status will change from Pending to either Approved or Rejected, based on their decision. All submitted manual-process Data Access Agreements will show up in your Manage Data page like Request 3. \n\n\n\n\n\n\n\n\nRequest approval typically takes a week or two if your data access agreement is correctly and completely filled out. You will receive automated emails from notifications@stjude.cloud at the time that your request is approved.\n\n\nAccessing Your Data\n\n\nFrom the Manage Data page, you can click on a request to navigate to the DNAnexus platform where a project will have been created with the project name that you entered through the setup wizard. Once your request is approved, the data will be vended to your DNAnexus project folder and will be accessible to you. You can also follow the link in the approval email from notifications@stjude.cloud to view your DNAnexus project folder. When the data is vended, the directory structure will typically look something like the following:\n\n\nproject_space/\n\n\n\u251c\u2500\u2500 restricted/\n\n\n\u2502   \u251c\u2500\u2500 bam/\n\n\n\u2502   \u251c\u2500\u2500 gVCF/\n\n\n\u2502   \u251c\u2500\u2500 Somatic_VCF/\n\n\n\u2502   \u2514\u2500\u2500 CNV/\n\n\n\u2514\u2500\u2500 SAMPLE_INFO.txt\n\n\n\n\n\n\nThe \nSAMPLE_INFO.txt\n file provides all the \nmetadata\n associated with the request, and the restricted folder contains all the data for which you were approved separated by file type. \n\n\n\n\nInfo\n\n\nIf you would like to download the data to local storage, there are\nextra steps you'll need to follow such as \ngetting additional signatures\n\non your data access agreement. We recommend that you work with the data\nin the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow \nthis guide\n to get started.",
            "title": "Making a Data Request"
        },
        {
            "location": "/guides/data/data-request/#request-process-overview",
            "text": "Creating a data request is the premier way to access raw St. Jude next \ngeneration sequencing data in the cloud. You can get a  free  copy of \nthe data in a secure cloud environment powered by  Microsoft Azure  and  DNAnexus , or you can elect to download the data to your local computing \nenvironment.   Things to Remember   Data in St. Jude Cloud is grouped into  Data Access Units (DAUs) , which usually correspond to large-scale sequencing initiatives at St. Jude.   Individuals can  apply for access  to DAUs on a case-by-case basis for a specific amount of time (usually 1 year).  Access to data in a given DAU is assessed by the corresponding  Data Access Committee  who reviews a variety of factors to grant access.  There are a number of terms of use and restrictions outlined in the  Data Access Agreement . Everyone who will be working with the data must understand and agree to these terms.",
            "title": "Request Process Overview"
        },
        {
            "location": "/guides/data/data-request/#selecting-data",
            "text": "There are three main ways to make your data selection through our  Data Browser . You can peruse our raw genomic data by diagnosis, publication, or study and use a number of filtering options to narrow down your search. You may also be directed to the Genomics Platform through another App to request specific samples.   You must have  created an account  and be logged in to make a data request. If you have not yet created an account or you are not logged in, the red  Request Data  button will say  Log In .   We ask that you review your selection and make sure that these are the  DAUs  you would like to request.",
            "title": "Selecting Data"
        },
        {
            "location": "/guides/data/data-request/#making-the-request",
            "text": "Now that you have selected your data, you will need to fill in some information to complete the request. From here, necessary information will be collected through a setup wizard. All of your progress will be automatically saved, and you can follow along with your progress on the left sidebar.   This information will be collected whether you are requesting open-access or controlled-access data. It helps us structure your project folder correctly when we vend the data to you.",
            "title": "Making the Request"
        },
        {
            "location": "/guides/data/data-request/#applying-for-data-access",
            "text": "If you already have access to the data or are requesting open-access data, you will not be prompted to go through this section.",
            "title": "Applying for Data Access"
        },
        {
            "location": "/guides/data/data-request/#signing-the-data-access-agreement",
            "text": "Every person who requests access to our controlled-access data must sign the  Data Access Agreement (DAA) . If you are located in the United States of America, you can opt in to completing the DAA through an electronic setup wizard. If you are not located in the USA, or would like to complete the form manually, you can follow our instructions on  Filling Out The Data Access Agreement .  If you opt to do the process through the setup wizard, the necessary information will be collected and added automatically to your agreement. Once you have completed the setup wizard, the form will be sent to you and necessary signatories through email via  DocuSign . You can learn more about our electronic data access agreement process  here .    Tip  If you receive an email from us that your DAA is incomplete, you may edit your DAA and upload the revised copy using the 'Add a Form' button the on Manage Data page.",
            "title": "Signing the Data Access Agreement"
        },
        {
            "location": "/guides/data/data-request/#checking-your-request-status",
            "text": "Once you start the Electronic Data Access Agreement process, you will have a draft autosaved for you on your  Manage Data  page, accessible at any time.\n       Pending Request Types    Request 1 is an Open Draft, meaning the requestor has not finished the setup wizard, and that the DocuSign envelope has not been sent to any of the signatories.     Request 2, listed in the Projects section, has been sent to the signatories, but has not been completed by all of them. This status will look like the Request 3 when all of the signatories sign the document and it is ready to be sent to the Data Access Committee(s).     Request 3 is pending approval from the Data Access Committee(s), and the status will change from Pending to either Approved or Rejected, based on their decision. All submitted manual-process Data Access Agreements will show up in your Manage Data page like Request 3.      Request approval typically takes a week or two if your data access agreement is correctly and completely filled out. You will receive automated emails from notifications@stjude.cloud at the time that your request is approved.",
            "title": "Checking your Request Status"
        },
        {
            "location": "/guides/data/data-request/#accessing-your-data",
            "text": "From the Manage Data page, you can click on a request to navigate to the DNAnexus platform where a project will have been created with the project name that you entered through the setup wizard. Once your request is approved, the data will be vended to your DNAnexus project folder and will be accessible to you. You can also follow the link in the approval email from notifications@stjude.cloud to view your DNAnexus project folder. When the data is vended, the directory structure will typically look something like the following:  project_space/  \u251c\u2500\u2500 restricted/  \u2502   \u251c\u2500\u2500 bam/  \u2502   \u251c\u2500\u2500 gVCF/  \u2502   \u251c\u2500\u2500 Somatic_VCF/  \u2502   \u2514\u2500\u2500 CNV/  \u2514\u2500\u2500 SAMPLE_INFO.txt   The  SAMPLE_INFO.txt  file provides all the  metadata  associated with the request, and the restricted folder contains all the data for which you were approved separated by file type.    Info  If you would like to download the data to local storage, there are\nextra steps you'll need to follow such as  getting additional signatures \non your data access agreement. We recommend that you work with the data\nin the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow  this guide  to get started.",
            "title": "Accessing Your Data"
        },
        {
            "location": "/guides/data/working-with-our-data/",
            "text": "You can follow \nthis guide\n to request access to\nSt. Jude data in a secure cloud environment. Before you can begin writing your\nown tools to run on our data, you'll need to understand a bit about how\ndata vending in St. Jude Cloud works. Behind the scenes, the \nDNAnexus\n genomic ecosystem is the backbone for the computation\nand storage in St. Jude Cloud. Each data request in St. Jude Cloud corresponds to a project in DNAnexus. We'll explain what this means below, but if you're so inclined, you can read an introduction to their ecosystem \nhere\n.\n\n\nAccessing your data request\n\n\nOnce your data access request is approved, the data you requested from St. Jude will automatically be distributed to a DNAnexus project with the same name as your data request. You can go to your \nManage Data\n page to see the status of the requests you have submitted and navigate directly to your data in DNAnexus.\n\n\n\n\nNote\n\n\nIf you have a question about the status of your data request which is not answered on the \"Manage Data\" page, you can email us at \nsupport@stjude.cloud\n.\n\n\n\n\nUsing our data\n\n\nThere are two primary ways you can interact with data vended to you in St. Jude Cloud:\n\n\n\n\nCloud access\n. You can choose to work with the data in DNAnexus' genomics cloud ecosystem. This is our suggested method of interaction, as you can avoid downloading the data to your local servers (which both takes time and is error prone). If you choose to leverage this approach, you can either wrap your own analysis pipeline as a cloud app (see \nour guide\n) or leverage any of DNAnexus' publicly available apps (see \nDNAnexus' guide\n.\n\n\nDownload the data\n (\nnot suggested\n). The second way to interact with data vended to you in St. Jude Cloud is by downloading the data to your local servers. If you wish to do this, you can either leverage the St. Jude Cloud Data Transfer Application (see \nour guide\n) or you can download the data on the command line (see \nour guide\n). \nNote that you must have indicated you wish to download the data in your \ndata access agreement\n.",
            "title": "Working with Our Data"
        },
        {
            "location": "/guides/data/working-with-our-data/#accessing-your-data-request",
            "text": "Once your data access request is approved, the data you requested from St. Jude will automatically be distributed to a DNAnexus project with the same name as your data request. You can go to your  Manage Data  page to see the status of the requests you have submitted and navigate directly to your data in DNAnexus.   Note  If you have a question about the status of your data request which is not answered on the \"Manage Data\" page, you can email us at  support@stjude.cloud .",
            "title": "Accessing your data request"
        },
        {
            "location": "/guides/data/working-with-our-data/#using-our-data",
            "text": "There are two primary ways you can interact with data vended to you in St. Jude Cloud:   Cloud access . You can choose to work with the data in DNAnexus' genomics cloud ecosystem. This is our suggested method of interaction, as you can avoid downloading the data to your local servers (which both takes time and is error prone). If you choose to leverage this approach, you can either wrap your own analysis pipeline as a cloud app (see  our guide ) or leverage any of DNAnexus' publicly available apps (see  DNAnexus' guide .  Download the data  ( not suggested ). The second way to interact with data vended to you in St. Jude Cloud is by downloading the data to your local servers. If you wish to do this, you can either leverage the St. Jude Cloud Data Transfer Application (see  our guide ) or you can download the data on the command line (see  our guide ).  Note that you must have indicated you wish to download the data in your  data access agreement .",
            "title": "Using our data"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/",
            "text": "Creating a Cloud Application\n\n\nThis guide will take you through the process of writing an application for working with and manipulating the St. Jude data you've requested. By creating your own application, you will be able to wrap genomic tools and packages from external sources, as well as any tool or application you might have written yourself.\n\n\n\n\nTip\n\n\nThe complete contents of this guide is hosted on the \nSt. Jude App Tutorial\n repository on GitHub. Feel free to clone the repository and use it as a reference while following this tutorial or try building the application and running it on your own project.\n\n\n\n\nOverview\n\n\nThe biggest difference between running an application in the cloud (as opposed to running it in a local environment) is the way we access that data and manipulate it. Writing and running your own cloud application grants numerous benefits. It allows you to submit numerous jobs in parallel, access your data from anywhere with an Internet connection, and utilize resources and compute power at a fraction of the cost (when compared to building your own infrastructure).\n\n\nWriting your own application will allow you to wrap custom tools to manipulate any data that you have previously requested. When you run your application, the request gets sent to a virtualized Linux container (Ubuntu 14.04 or 16.04) where any dependencies are installed and where your script will be run. Any tools or packages that you include (either through the included package managers, or bundled together in your project) will be available locally on the virtual Linux machine.\n\n\nHowever, there are differences in how we manage our data. When a job is submitted, a virtual machine is provisioned specifically for that job request, meaning that it is spun up at-will or when needed. It also implies that once the job has completed, the virtual machine will be reprovisioned or deleted. Any job output or data must be uploaded back to the project space.\n\n\nIn this tutorial, we will be wrapping the \nFastQC\n, a quality control tool for raw sequence data, into our application. This will allow us to run FastQC on any of the St. Jude next generation sequencing data in the cloud. For specific information about how FastQC works, please refer to the \nFastQC documentation\n.\n\n\nData\n\n\nFor this tutorial I have requested the PCGP dataset, and once my access request has been approved, my project directory space will look like the following.\n\n\n\n\n\n\nNote\n\n\nIf you do not yet have data in a DNAnexus project, you may request data from St. Jude Cloud by following the directions \nhere\n or you may upload your own data using the \ndata transfer app\n. In order to make a data request or upload your own data using the data transfer app, you must first \ncreate a St. Jude Cloud account\n.\n\n\n\n\nWriting the Application\n\n\nRequirements\n\n\n\n\n\n\n\n\n\n\nTool\n\n\nDownload\n\n\nWebsite\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\ndx-toolkit\n\n\nSource\n\n\nDNAnexus\n\n\nv0.291.1\n\n\n\n\n\n\nFastQC\n\n\nSource\n\n\nBabraham Bioinformatics\n\n\nv0.11.8\n\n\n\n\n\n\n\n\n\n\nThe easiest way to install \ndx-toolkit\n is through \npip\n, the Python package manager. Simply run the following command in your terminal:\n\n\npip install dxpy --upgrade\n\n\n\n\n\n\nFor this application, we will be using the \ndx-app-wizard\n command that is included in the \ndx-toolkit\n. \ndx-app-wizard\n is an interactive prompt that creates a boilerplate project that will allow you to quickly create an application. For more on \ndx-app-wizard\n, refer to the DNAnexus wiki article on \nIntro to Building Apps\n. Before continuing, be sure to refer to the \ncommand line interaction page\n for a walkthrough on how to install \ndx-toolkit\n and how to select your project workspace.\n\n\n\n\nTip\n\n\nIt is not necessary to use \ndx-app-wizard\n. All the necessary files and project directory structure can be created manually. However, \ndx-app-wizard\n provides a quick and easy way to get started. For more information, refer to the \nAdvanced App Tutorial\n.\n\n\n\n\nAll DNAnexus project applications will have the following structure:\n\n\ndx-fastqc-example-app/\n\n\n\u251c\u2500\u2500 dxapp.json\n\n\n\u251c\u2500\u2500 resources/\n\n\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/\n\n\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/\n\n\n\u2514\u2500\u2500 src/\n\n\n    \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\n\n\n\n\n\nThe \ndxapp.json\n file is a JSON file that contains metadata about the application we are writing that are needed to build and run the app on the DNAnexus Platform. Most notably, you will need to specify all of the inputs your app requires (both input files or any settings you can tune), output files, and other options such as the number of cores and memory required to run the tool. To see the full list of fields, refer to the \nDNAnexus wiki\n guide on the application metadata.\n\n\nThe \ndx-fastqc-example-app.sh\n file is a bash script is what will be executed when the application is run. Any executable binaries that accompany the application, such as other tools or scripts, are placed in the \nresources\n folder. From there, we can call the executable from within the app when it is run.\n\n\nCreating the Project\n\n\nStart by running the \ndx-app-wizard\n command from your terminal.\n\n\n\n\nInfo\n\n\nThis helper tool will create a \nlocal\n directory on your machine. Any code changes we make will be done \ninside\n this local project directory created by \ndx-app-wizard\n. This is because we can write our application locally, \nbuild the application\n, and then \nrun the application\n in the cloud.\n\n\nBuilding the application will compile \ndx-fastqc-example-app\n and then upload it into the project space on the cloud. When we run an application, it will be submitted as a job to be run in the cloud. With this process, we can write the application locally and run it on our data in the cloud, without ever having to utilize personal bandwidth and compute time.\n\n\n\n\n$ dx-app-wizard\n\n\n\n\n\nFor our inputs, we will enter the following:\n\n\n$ App Name: dx-fastqc-example-app\n...\n$ Title \n[]\n: FastQC Example Application\n...\n$ Summary \n[]\n: Uses FastQC to generate quality control reports on raw sequence data.\n...\n$ Version \n[\n0\n.0.1\n]\n: \n0\n.0.1\n...\n$ 1st input name \n(\n<ENTER> to finish\n)\n: bam_file\n$ Label \n(\noptional human-readable name\n)\n \n[]\n: BAM File\n...\n$ Choose a class \n(\n<TAB> twice \nfor\n choices\n)\n: file\n$ This is an optional parameter \n[\ny/n\n]\n: n\n...\n$ 1st output name \n(\n<ENTER> to finish\n)\n: fastqc_html\n$ Label \n(\noptional human-readable name\n)\n \n[]\n: FastQC HTML Report\n$ Choose a class \n(\n<TAB> twice \nfor\n choices\n)\n: file\n\n$ 2nd output name \n(\n<ENTER> to finish\n)\n: fastqc_zip\n$ Label \n(\noptional human-readable name\n)\n \n[]\n: FastQC Zip File\n$ Choose a class \n(\n<TAB> twice \nfor\n choices\n)\n: file\n...\n$ Timeout policy \n[\n48h\n]\n: 48h\n...\n$ Programming language: bash\n...\n$ Will this app need access to the Internet? \n[\ny/N\n]\n: N\n...\n$ Will this app need access to the parent project? \n[\ny/N\n]\n: y\n...\n$ Choose an instance \ntype\n \nfor\n your app \n[\nmem1_ssd1_x4\n]\n: azure:mem1_ssd1_x4\n\n\n\n\n\n\n\nTip\n\n\nAlthough our app doesn't need any Internet access in this example, it may be required for your project. Also be sure to check what instance type you will need in the \nAPI Specifications\n.\n\n\n\n\nThe FastQC executable supports a variety of file formats (BAM, SAM, FastQ, etc.), and outputs a HTML report and a zip file that contains all the graphs and data. We will use that knowledge to write the input and output parameters for our application. We can also specify other parameters such as the timeout policy, programming language, and instance type. For more information, refer to the \nIO and Run Specification\n guide.\n\n\nIntegrating Tools and Packages\n\n\nOnce we have finished creating the basic FastQC application using \ndx-app-wizard\n, the project structure should look like:\n\n\ndx-fastqc-example-app/\n\n\n\u251c\u2500\u2500 Readme.developer.md\n\n\n\u251c\u2500\u2500 Readme.md\n\n\n\u251c\u2500\u2500 dxapp.json\n\n\n\u251c\u2500\u2500 resources/\n\n\n\u251c\u2500\u2500 src/\n\n\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\n\n\u2514\u2500\u2500 test/\n\n\n\n\n\n\n\n\nInfo\n\n\nAnything in the resources folder is unpacked into the root directory (\n/\n) of the virtual Linux machine that your application will run on. If we create the directory path \ndx-fastqc-example-app/resources/usr/bin/\n, anything in the bin folder would be unpacked into \n/usr/bin/\n on the Linux machine. This is handy because that path is included in the default \n$PATH\n environment variable.\n\n\nYour application's executable will use \n/home/dnanexus/\n as its current working directory.\n\n\n\n\nThough \ndx-app-wizard\n does not create this, we can create it ourselves.  Paste the following lines into your terminal.\n\n\n$ mkdir -p dx-fastqc-example-app/resources/usr/bin\n\n\n\n\n\nPackaging FastQC\n\n\nTo incorporate FastQC into this project, we need to download the executable binary and package it within the \ndx-fastqc-example-app\n. Download the FastQC v0.11.8 (Win/Linux zip file) and unzip it. After unzipping, move the FastQC folder into the \nresources\n folder.\n\n\n$ unzip fastqc_v0.11.8.zip\n$ mv FastQC /path/to/project/dx-fastqc-example-app/resources/\n\n\n\n\n\nNow, our project will look like this:\n\n\ndx-fastqc-example-app/\n\n\n\u251c\u2500\u2500 LICENSE\n\n\n\u251c\u2500\u2500 README.md\n\n\n\u251c\u2500\u2500 Readme.developer.md\n\n\n\u251c\u2500\u2500 dxapp.json\n\n\n\u251c\u2500\u2500 test/\n\n\n\u251c\u2500\u2500 resources/\n\n\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FastQC/\n\n\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fastqc\n\n\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\n\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/\n\n\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/\n\n\n\u2514\u2500\u2500 src/\n\n\n    \u2514\u2500\u2500 dx-fastqc-example-app.sh\n\n\n\n\n\n\nInstalling Dependencies\n\n\n\n\nTip\n\n\nIf you are importing custom tools, or are using tools that rely on various packages and requirements, they can be specified in the \"runSpec\".\n\n\nFor more information on installing dependencies and available software packages, refer to the \nExecution Environment Reference\n.\n\n\n\n\nSome external package managers that we can leverage when building an app include:\n\n\n\n\n\n\n\n\n\n\nPackage Manager\n\n\nApplication\n\n\n\n\n\n\n\n\n\n\nAPT\n\n\nAdvanced Packaging Tool for Ubuntu\n\n\n\n\n\n\nCPAN\n\n\nComprehensive Perl Archive Network\n\n\n\n\n\n\nCRAN\n\n\nComprehensive R Archive Network\n\n\n\n\n\n\ngem\n\n\nPackage Manager for Ruby\n\n\n\n\n\n\npip\n\n\nPyPI (Python Package Index)\n\n\n\n\n\n\n\n\n\n\nOne requirement for FastQC is that it must have a suitable \nJava Runtime Environment\n. To include this in the app, we have to edit the \ndxapp.json\n file. Open \ndxapp.json\n and append the following line to \n\"runSpec\"\n:\n\n\n  \n\"execDepends\"\n:\n \n[\n\n    \n{\n\"name\"\n:\n \n\"openjdk-7-jre-headless\"\n,\n\n     \n\"package_manager\"\n:\n \n\"apt\"\n}\n\n  \n]\n\n\n\n\n\n\nBe sure to add a comma at the very end of the \"file\" object line to accommodate the new \"execDepends\" lines. Now, the \n\"runSpec\"\n object should look like the following:\n\n\n  \n...\n\n  \n\"runSpec\"\n:\n \n{\n\n    \n\"timeoutPolicy\"\n:\n \n{\n\n      \n\"*\"\n:\n \n{\n\n        \n\"hours\"\n:\n \n48\n\n      \n}\n\n    \n},\n\n    \n\"interpreter\"\n:\n \n\"bash\"\n,\n\n    \n\"release\"\n:\n \n\"14.04\"\n,\n\n    \n\"distribution\"\n:\n \n\"Ubuntu\"\n,\n\n    \n\"file\"\n:\n \n\"src/dx-fastqc-example-app.sh\"\n,\n\n    \n\"execDepends\"\n:\n \n[\n\n      \n{\n\"name\"\n:\n \n\"openjdk-7-jre-headless\"\n,\n\n       \n\"package_manager\"\n:\n \n\"apt\"\n}\n\n    \n]\n\n  \n}\n,\n\n  \n...\n\n\n\n\n\n\nWhen you build and run your application, the virtual environment will now download \nopenjdk-7\n from Ubuntu's APT package manager as a prerequisite. For more information on how to specify packages from Git, R, or Python, refer to the \nSoftware Packages\n wiki page.\n\n\nCalling FastQC\n\n\nThe last step is to call the FastQC executable from within the app. Open up \nsrc/dx-fastqc-example-app.sh\n with a text editor. Inside this Bash script is where we will be working with FastQC and our data. Before we dive in, its a good idea to add a few useful parameters for the script execution.\n\n\nRight after the Bash shebang (\n#!/bin/bash\n), add the following line:\n\n\nset\n -e -x\n\n\n\n\n\nBelow is a table describing what each flag does:\n\n\n\n\n\n\n\n\n\n\nFlag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-e\n\n\nExit immediately if a command exits with a non-zero status.\n\n\n\n\n\n\n-x\n\n\nPrint each command to standard error before execution.\n\n\n\n\n\n\n\n\n\n\nOur first change has to do with how our BAM file is downloaded. Although \ndx-app-wizard\n automatically generates a line that will download the input file and rename it, we want to keep the original file name because FastQC uses the input file as part of the report name. Remove the \n-o bam_file\n portion so the line looks like the following:\n\n\ndx download \n\"\n$bam_file\n\"\n       \n# Downloads our input BAM file without renaming\n\n\n\n\n\n\nAfter the application downloads the input file, we need to create the appropriate output directories and run FastQC on our BAM file. Add the following lines to the bash script within the \nmain\n function:\n\n\nmkdir ~/fastqc-out/                                    \n# FastQC Output Folder\n\n/FastQC/fastqc \n\"\n$bam_file_name\n\"\n -o ~/fastqc-out        \n# Runs FastQC on BAM File\n\n\n\n\n\n\n\n\nTip\n\n\nBe sure to use \n\"$bam_file_name\"\n as our input for FastQC. Using \n\"$bam_file\"\n only returns the DNAnexus file-id associated with the input file.\n\n\nFor more information on helper variables, refer to the \nAdvanced App Tutorial\n.\n\n\n\n\nUploading Files\n\n\nAfter FastQC finishes, the last thing to do is to upload the reports generated by FastQC to our project. These virtual Linux machines are provisioned at-will, meaning that they are only spun up when a job is submitted. When we create an application and run it in the cloud, we submit it as a job to be executed. When a job gets executed, a virtual machine will download all the necessary requirements (tools, packages, data, etc.) and run the job. Any output files on the machine must be uploaded back to the project space after a job finishes executing. Any information and data not uploaded to the project space will be inaccessible and lost.\n\n\nYou will see two lines generated for us by \ndx-app-wizard\n when we specified the outputs for our application. We need to change these to upload the correct files from our output directory that we specified for FastQC. Otherwise, it assumes they are in the home directory. Before this, we can also (optionally) rename the files to be uploaded. Add the following lines, making sure to replace the two original upload lines.\n\n\nLines to remove/overwrite:\n\n\n# Generated by dx-app-wizard\n\n\nfastqc_html\n=\n$(\ndx upload fastqc_html --brief\n)\n\n\nfastqc_zip\n=\n$(\ndx upload fastqc_zip --brief\n)\n\n\n\n\n\n\nLines to add:\n\n\n# (Optional) Renames the FastQC reports\n\nmv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\nmv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip\n\n\n# Uploads the respective HTML and Zip file (lines to change)\n\n\nfastqc_html\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.html --brief\n)\n\n\nfastqc_zip\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.zip --brief\n)\n\n\n\n\n\n\nWe are using \n\"$bam_file_prefix\"\n to help name the output report file. These helper variables are provided to help make file naming easy. For more information on helper variables, refer to the \nAdvanced App Tutorial\n.\n\n\nIn this step, we are also moving the HTML and Zip file generated by FastQC to the directories which will be uploaded.\n\n\nAfter this step, \ndx-fastqc-example-app.sh\n should look like:\n\n\n#!/bin/bash\n\n\n\nset\n -e -x\n\nmain\n()\n \n{\n\n    \necho\n \n\"Value of bam_file: '\n$bam_file\n'\"\n\n\n    \n# Downloads file from project to virtual machine workspace\n\n    dx download \n\"\n$bam_file\n\"\n\n\n    \n# Creating output directory for FastQC\n\n    mkdir ~/fastqc-out\n\n    \n# Runs FastQC on BAM file\n\n    /FastQC/fastqc \n\"\n$bam_file_name\n\"\n -o ~/fastqc-out\n\n    \n# Renames the FastQC reports to include the BAM file prefix\n\n    mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\n    mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip\n\n    \n# Uploads the respective HTML and Zip file\n\n    \nfastqc_html\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.html --brief\n)\n\n    \nfastqc_zip\n=\n$(\ndx upload ~/fastqc-out/fastqc-report.zip --brief\n)\n\n\n    \n# Adds and formats appropriate output variables for your app\n\n    dx-jobutil-add-output fastqc_html \n\"\n$fastqc_html\n\"\n --class\n=\nfile\n    dx-jobutil-add-output fastqc_zip \n\"\n$fastqc_zip\n\"\n --class\n=\nfile\n\n}\n\n\n\n\n\n\nBuilding Your App\n\n\nBefore building, ensure that you are in the parent directory of the \nlocal\n project folder generated by \ndx-app-wizard\n. To check, if you enter the command \nls\n, you should see the project folder \ndx-fastqc-example-app/\n appear in the output.\n\n\nTo build your application, enter the following into your terminal:\n\n\n$ dx build dx-fastqc-example-app\n\n\n\n\n\nThis command will package the tools and files as an application which can then be run on the DNAnexus Platform. In the screenshot below, you can see the compiled app in our project workspace selected and highlighted in blue.\n\n\nTo verify that the build was completed successfully, you can enter \ndx ls\n. This should show you all the files in your project space in the cloud.\n\n\n# This will show what files are in your root directory for your project space in the cloud\n\n$ dx ls\n\n\n\n\n\nYou should see something along the lines of this printed out in your terminal. Note that a compiled copy of our \ndx-fastqc-example-app\n now lives in the project.\n\n\n.\n\n\n\u251c\u2500\u2500 immediate/\n\n\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\n\n\u251c\u2500\u2500 dx-fastqc-example-app\n\n\n\u2514\u2500\u2500 SAMPLE_INFO.txt\n\n\n\n\n\n\nYou can also \nview the project\n directly from your browser. You will see a similar result.\n\n\n\n\nAny time you make any changes to the scripts or the application, you will need to rebuild the application. To overwrite a previous version of the app, specify the \n-f\n command.\n\n\nYou can also inspect and configure the application by clicking on it.\n\n\n\n\nRunning Your App\n\n\nTo run the \ndx-fastqc-example-app\n, enter the following into the terminal:\n\n\n$ dx run dx-fastqc-example-app -i \nbam_file\n=\n/path/to/<bam-file>.bam\n\n\n\n\n\nFor this example, I am using the PCGP dataset and my run command will look like the following:\n\n\n$ dx run dx-fastqc-example-app -i \nbam_file\n=\n/immediate/bam/SJBALL020073_D1.RNA-Seq.bam\n\n\n\n\n\nThe input path will vary depending on how the data looks inside your DNAnexus project, but it might look like the following: \n/restricted/bam/<bam-file>.bam\n\n\nYou will be prompted to confirm that you wish to run the application with the following JSON input and whether you would like to monitor the job in your terminal.\n\n\nUsing input JSON:\n\n{\n\n    \n\"bam_file\"\n: \n{\n\n        \n\"\n$dnanexus_link\n\"\n: \n{\n\n            \n\"project\"\n: \n\"project-FV9XFG0991ZbPVgQ2jx1vZv5\"\n,\n            \n\"id\"\n: \n\"file-FV9gzf8991ZXQ1kv7V3BqgjV\"\n\n        \n}\n\n    \n}\n\n\n}\n\n\nConfirm running the executable with this input \n[\nY/n\n]\n: Y\nCalling applet-FVbY8Qj991ZQ1863BGK6x0bk with output destination project-FV9XFG0991ZbPVgQ2jx1vZv5:/\n\nJob ID: job-FVbY8Z0991ZXx5v1Fk3QgJPV\nWatch launched job now? \n[\nY/n\n]\n Y\n\nJob Log\n-------\nWatching job job-FVbY8Z0991ZXx5v1Fk3QgJPV. Press Ctrl+C to stop.\n\n\n\n\n\nYou can also monitor active jobs by going to the project space and selecting the \"Monitor\" tab.\n\n\n\n\nJob Completion\n\n\nOnce the job finishes, you will receive an email from DNAnexus (\nnotification@dnanexus.com\n) about whether the job has completed successfully or failed.\n\n\nMake sure to check that these emails don't get sent to your spam folder.\n\n\n\n\nClicking the links in the email should open up a new tab in your browser and take you to the appropriate project. Here, we can see that FastQC has run successfully and that the two files generated by FastQC have been uploaded back into our project space.\n\n\n\n\nAgain, if we run the \ndx ls\n command, we can verify that two new files titled \"fastqc-report.html\" and \"fastqc-report.zip\" are in the root directory of our project.\n\n\n.\n\n\n\u251c\u2500\u2500 immediate/\n\n\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\n\n\u251c\u2500\u2500 dx-fastqc-example-app\n\n\n\u251c\u2500\u2500 fastqc-report.html\n\n\n\u251c\u2500\u2500 fastqc-report.zip\n\n\n\u2514\u2500\u2500 SAMPLE_INFO.txt\n\n\n\n\n\n\nConclusion\n\n\nIf you have made it this far, you have likely wrapped your first genomic analysis tool for use in the cloud. For your reference, we have included the final FastQC application at the \nSt. Jude App Tutorial Repository\n.\n\n\nIf you have any questions or suggestions on how we can improve this tutorial, please \nfile an issue\n, contact us at \nhttps://stjude.cloud/contact\n, or email us at \nsupport@stjude.cloud\n.",
            "title": "Creating a Cloud App"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#creating-a-cloud-application",
            "text": "This guide will take you through the process of writing an application for working with and manipulating the St. Jude data you've requested. By creating your own application, you will be able to wrap genomic tools and packages from external sources, as well as any tool or application you might have written yourself.   Tip  The complete contents of this guide is hosted on the  St. Jude App Tutorial  repository on GitHub. Feel free to clone the repository and use it as a reference while following this tutorial or try building the application and running it on your own project.",
            "title": "Creating a Cloud Application"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#overview",
            "text": "The biggest difference between running an application in the cloud (as opposed to running it in a local environment) is the way we access that data and manipulate it. Writing and running your own cloud application grants numerous benefits. It allows you to submit numerous jobs in parallel, access your data from anywhere with an Internet connection, and utilize resources and compute power at a fraction of the cost (when compared to building your own infrastructure).  Writing your own application will allow you to wrap custom tools to manipulate any data that you have previously requested. When you run your application, the request gets sent to a virtualized Linux container (Ubuntu 14.04 or 16.04) where any dependencies are installed and where your script will be run. Any tools or packages that you include (either through the included package managers, or bundled together in your project) will be available locally on the virtual Linux machine.  However, there are differences in how we manage our data. When a job is submitted, a virtual machine is provisioned specifically for that job request, meaning that it is spun up at-will or when needed. It also implies that once the job has completed, the virtual machine will be reprovisioned or deleted. Any job output or data must be uploaded back to the project space.  In this tutorial, we will be wrapping the  FastQC , a quality control tool for raw sequence data, into our application. This will allow us to run FastQC on any of the St. Jude next generation sequencing data in the cloud. For specific information about how FastQC works, please refer to the  FastQC documentation .",
            "title": "Overview"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#data",
            "text": "For this tutorial I have requested the PCGP dataset, and once my access request has been approved, my project directory space will look like the following.    Note  If you do not yet have data in a DNAnexus project, you may request data from St. Jude Cloud by following the directions  here  or you may upload your own data using the  data transfer app . In order to make a data request or upload your own data using the data transfer app, you must first  create a St. Jude Cloud account .",
            "title": "Data"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#writing-the-application",
            "text": "",
            "title": "Writing the Application"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#requirements",
            "text": "Tool  Download  Website  Version      dx-toolkit  Source  DNAnexus  v0.291.1    FastQC  Source  Babraham Bioinformatics  v0.11.8      The easiest way to install  dx-toolkit  is through  pip , the Python package manager. Simply run the following command in your terminal:  pip install dxpy --upgrade   For this application, we will be using the  dx-app-wizard  command that is included in the  dx-toolkit .  dx-app-wizard  is an interactive prompt that creates a boilerplate project that will allow you to quickly create an application. For more on  dx-app-wizard , refer to the DNAnexus wiki article on  Intro to Building Apps . Before continuing, be sure to refer to the  command line interaction page  for a walkthrough on how to install  dx-toolkit  and how to select your project workspace.   Tip  It is not necessary to use  dx-app-wizard . All the necessary files and project directory structure can be created manually. However,  dx-app-wizard  provides a quick and easy way to get started. For more information, refer to the  Advanced App Tutorial .   All DNAnexus project applications will have the following structure:  dx-fastqc-example-app/  \u251c\u2500\u2500 dxapp.json  \u251c\u2500\u2500 resources/  \u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/  \u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/  \u2514\u2500\u2500 src/      \u2514\u2500\u2500 dx-fastqc-example-app.sh   The  dxapp.json  file is a JSON file that contains metadata about the application we are writing that are needed to build and run the app on the DNAnexus Platform. Most notably, you will need to specify all of the inputs your app requires (both input files or any settings you can tune), output files, and other options such as the number of cores and memory required to run the tool. To see the full list of fields, refer to the  DNAnexus wiki  guide on the application metadata.  The  dx-fastqc-example-app.sh  file is a bash script is what will be executed when the application is run. Any executable binaries that accompany the application, such as other tools or scripts, are placed in the  resources  folder. From there, we can call the executable from within the app when it is run.",
            "title": "Requirements"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#creating-the-project",
            "text": "Start by running the  dx-app-wizard  command from your terminal.   Info  This helper tool will create a  local  directory on your machine. Any code changes we make will be done  inside  this local project directory created by  dx-app-wizard . This is because we can write our application locally,  build the application , and then  run the application  in the cloud.  Building the application will compile  dx-fastqc-example-app  and then upload it into the project space on the cloud. When we run an application, it will be submitted as a job to be run in the cloud. With this process, we can write the application locally and run it on our data in the cloud, without ever having to utilize personal bandwidth and compute time.   $ dx-app-wizard  For our inputs, we will enter the following:  $ App Name: dx-fastqc-example-app\n...\n$ Title  [] : FastQC Example Application\n...\n$ Summary  [] : Uses FastQC to generate quality control reports on raw sequence data.\n...\n$ Version  [ 0 .0.1 ] :  0 .0.1\n...\n$ 1st input name  ( <ENTER> to finish ) : bam_file\n$ Label  ( optional human-readable name )   [] : BAM File\n...\n$ Choose a class  ( <TAB> twice  for  choices ) : file\n$ This is an optional parameter  [ y/n ] : n\n...\n$ 1st output name  ( <ENTER> to finish ) : fastqc_html\n$ Label  ( optional human-readable name )   [] : FastQC HTML Report\n$ Choose a class  ( <TAB> twice  for  choices ) : file\n\n$ 2nd output name  ( <ENTER> to finish ) : fastqc_zip\n$ Label  ( optional human-readable name )   [] : FastQC Zip File\n$ Choose a class  ( <TAB> twice  for  choices ) : file\n...\n$ Timeout policy  [ 48h ] : 48h\n...\n$ Programming language: bash\n...\n$ Will this app need access to the Internet?  [ y/N ] : N\n...\n$ Will this app need access to the parent project?  [ y/N ] : y\n...\n$ Choose an instance  type   for  your app  [ mem1_ssd1_x4 ] : azure:mem1_ssd1_x4   Tip  Although our app doesn't need any Internet access in this example, it may be required for your project. Also be sure to check what instance type you will need in the  API Specifications .   The FastQC executable supports a variety of file formats (BAM, SAM, FastQ, etc.), and outputs a HTML report and a zip file that contains all the graphs and data. We will use that knowledge to write the input and output parameters for our application. We can also specify other parameters such as the timeout policy, programming language, and instance type. For more information, refer to the  IO and Run Specification  guide.",
            "title": "Creating the Project"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#integrating-tools-and-packages",
            "text": "Once we have finished creating the basic FastQC application using  dx-app-wizard , the project structure should look like:  dx-fastqc-example-app/  \u251c\u2500\u2500 Readme.developer.md  \u251c\u2500\u2500 Readme.md  \u251c\u2500\u2500 dxapp.json  \u251c\u2500\u2500 resources/  \u251c\u2500\u2500 src/  \u2502\u00a0\u00a0 \u2514\u2500\u2500 dx-fastqc-example-app.sh  \u2514\u2500\u2500 test/    Info  Anything in the resources folder is unpacked into the root directory ( / ) of the virtual Linux machine that your application will run on. If we create the directory path  dx-fastqc-example-app/resources/usr/bin/ , anything in the bin folder would be unpacked into  /usr/bin/  on the Linux machine. This is handy because that path is included in the default  $PATH  environment variable.  Your application's executable will use  /home/dnanexus/  as its current working directory.   Though  dx-app-wizard  does not create this, we can create it ourselves.  Paste the following lines into your terminal.  $ mkdir -p dx-fastqc-example-app/resources/usr/bin",
            "title": "Integrating Tools and Packages"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#packaging-fastqc",
            "text": "To incorporate FastQC into this project, we need to download the executable binary and package it within the  dx-fastqc-example-app . Download the FastQC v0.11.8 (Win/Linux zip file) and unzip it. After unzipping, move the FastQC folder into the  resources  folder.  $ unzip fastqc_v0.11.8.zip\n$ mv FastQC /path/to/project/dx-fastqc-example-app/resources/  Now, our project will look like this:  dx-fastqc-example-app/  \u251c\u2500\u2500 LICENSE  \u251c\u2500\u2500 README.md  \u251c\u2500\u2500 Readme.developer.md  \u251c\u2500\u2500 dxapp.json  \u251c\u2500\u2500 test/  \u251c\u2500\u2500 resources/  \u2502\u00a0\u00a0 \u251c\u2500\u2500 FastQC/  \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fastqc  \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...  \u2502\u00a0\u00a0 \u2514\u2500\u2500 usr/  \u2502\u00a0\u00a0     \u2514\u2500\u2500 bin/  \u2514\u2500\u2500 src/      \u2514\u2500\u2500 dx-fastqc-example-app.sh",
            "title": "Packaging FastQC"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#installing-dependencies",
            "text": "Tip  If you are importing custom tools, or are using tools that rely on various packages and requirements, they can be specified in the \"runSpec\".  For more information on installing dependencies and available software packages, refer to the  Execution Environment Reference .   Some external package managers that we can leverage when building an app include:      Package Manager  Application      APT  Advanced Packaging Tool for Ubuntu    CPAN  Comprehensive Perl Archive Network    CRAN  Comprehensive R Archive Network    gem  Package Manager for Ruby    pip  PyPI (Python Package Index)      One requirement for FastQC is that it must have a suitable  Java Runtime Environment . To include this in the app, we have to edit the  dxapp.json  file. Open  dxapp.json  and append the following line to  \"runSpec\" :     \"execDepends\" :   [ \n     { \"name\" :   \"openjdk-7-jre-headless\" , \n      \"package_manager\" :   \"apt\" } \n   ]   Be sure to add a comma at the very end of the \"file\" object line to accommodate the new \"execDepends\" lines. Now, the  \"runSpec\"  object should look like the following:     ... \n   \"runSpec\" :   { \n     \"timeoutPolicy\" :   { \n       \"*\" :   { \n         \"hours\" :   48 \n       } \n     }, \n     \"interpreter\" :   \"bash\" , \n     \"release\" :   \"14.04\" , \n     \"distribution\" :   \"Ubuntu\" , \n     \"file\" :   \"src/dx-fastqc-example-app.sh\" , \n     \"execDepends\" :   [ \n       { \"name\" :   \"openjdk-7-jre-headless\" , \n        \"package_manager\" :   \"apt\" } \n     ] \n   } , \n   ...   When you build and run your application, the virtual environment will now download  openjdk-7  from Ubuntu's APT package manager as a prerequisite. For more information on how to specify packages from Git, R, or Python, refer to the  Software Packages  wiki page.",
            "title": "Installing Dependencies"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#calling-fastqc",
            "text": "The last step is to call the FastQC executable from within the app. Open up  src/dx-fastqc-example-app.sh  with a text editor. Inside this Bash script is where we will be working with FastQC and our data. Before we dive in, its a good idea to add a few useful parameters for the script execution.  Right after the Bash shebang ( #!/bin/bash ), add the following line:  set  -e -x  Below is a table describing what each flag does:      Flag  Description      -e  Exit immediately if a command exits with a non-zero status.    -x  Print each command to standard error before execution.      Our first change has to do with how our BAM file is downloaded. Although  dx-app-wizard  automatically generates a line that will download the input file and rename it, we want to keep the original file name because FastQC uses the input file as part of the report name. Remove the  -o bam_file  portion so the line looks like the following:  dx download  \" $bam_file \"         # Downloads our input BAM file without renaming   After the application downloads the input file, we need to create the appropriate output directories and run FastQC on our BAM file. Add the following lines to the bash script within the  main  function:  mkdir ~/fastqc-out/                                     # FastQC Output Folder \n/FastQC/fastqc  \" $bam_file_name \"  -o ~/fastqc-out         # Runs FastQC on BAM File    Tip  Be sure to use  \"$bam_file_name\"  as our input for FastQC. Using  \"$bam_file\"  only returns the DNAnexus file-id associated with the input file.  For more information on helper variables, refer to the  Advanced App Tutorial .",
            "title": "Calling FastQC"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#uploading-files",
            "text": "After FastQC finishes, the last thing to do is to upload the reports generated by FastQC to our project. These virtual Linux machines are provisioned at-will, meaning that they are only spun up when a job is submitted. When we create an application and run it in the cloud, we submit it as a job to be executed. When a job gets executed, a virtual machine will download all the necessary requirements (tools, packages, data, etc.) and run the job. Any output files on the machine must be uploaded back to the project space after a job finishes executing. Any information and data not uploaded to the project space will be inaccessible and lost.  You will see two lines generated for us by  dx-app-wizard  when we specified the outputs for our application. We need to change these to upload the correct files from our output directory that we specified for FastQC. Otherwise, it assumes they are in the home directory. Before this, we can also (optionally) rename the files to be uploaded. Add the following lines, making sure to replace the two original upload lines.  Lines to remove/overwrite:  # Generated by dx-app-wizard  fastqc_html = $( dx upload fastqc_html --brief )  fastqc_zip = $( dx upload fastqc_zip --brief )   Lines to add:  # (Optional) Renames the FastQC reports \nmv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\nmv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip # Uploads the respective HTML and Zip file (lines to change)  fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief )  fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief )   We are using  \"$bam_file_prefix\"  to help name the output report file. These helper variables are provided to help make file naming easy. For more information on helper variables, refer to the  Advanced App Tutorial .  In this step, we are also moving the HTML and Zip file generated by FastQC to the directories which will be uploaded.  After this step,  dx-fastqc-example-app.sh  should look like:  #!/bin/bash  set  -e -x\n\nmain ()   { \n     echo   \"Value of bam_file: ' $bam_file '\" \n\n     # Downloads file from project to virtual machine workspace \n    dx download  \" $bam_file \" \n\n     # Creating output directory for FastQC \n    mkdir ~/fastqc-out\n\n     # Runs FastQC on BAM file \n    /FastQC/fastqc  \" $bam_file_name \"  -o ~/fastqc-out\n\n     # Renames the FastQC reports to include the BAM file prefix \n    mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html\n    mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip\n\n     # Uploads the respective HTML and Zip file \n     fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief ) \n     fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief ) \n\n     # Adds and formats appropriate output variables for your app \n    dx-jobutil-add-output fastqc_html  \" $fastqc_html \"  --class = file\n    dx-jobutil-add-output fastqc_zip  \" $fastqc_zip \"  --class = file }",
            "title": "Uploading Files"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#building-your-app",
            "text": "Before building, ensure that you are in the parent directory of the  local  project folder generated by  dx-app-wizard . To check, if you enter the command  ls , you should see the project folder  dx-fastqc-example-app/  appear in the output.  To build your application, enter the following into your terminal:  $ dx build dx-fastqc-example-app  This command will package the tools and files as an application which can then be run on the DNAnexus Platform. In the screenshot below, you can see the compiled app in our project workspace selected and highlighted in blue.  To verify that the build was completed successfully, you can enter  dx ls . This should show you all the files in your project space in the cloud.  # This will show what files are in your root directory for your project space in the cloud \n$ dx ls  You should see something along the lines of this printed out in your terminal. Note that a compiled copy of our  dx-fastqc-example-app  now lives in the project.  .  \u251c\u2500\u2500 immediate/  \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...  \u251c\u2500\u2500 dx-fastqc-example-app  \u2514\u2500\u2500 SAMPLE_INFO.txt   You can also  view the project  directly from your browser. You will see a similar result.   Any time you make any changes to the scripts or the application, you will need to rebuild the application. To overwrite a previous version of the app, specify the  -f  command.  You can also inspect and configure the application by clicking on it.",
            "title": "Building Your App"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#running-your-app",
            "text": "To run the  dx-fastqc-example-app , enter the following into the terminal:  $ dx run dx-fastqc-example-app -i  bam_file = /path/to/<bam-file>.bam  For this example, I am using the PCGP dataset and my run command will look like the following:  $ dx run dx-fastqc-example-app -i  bam_file = /immediate/bam/SJBALL020073_D1.RNA-Seq.bam  The input path will vary depending on how the data looks inside your DNAnexus project, but it might look like the following:  /restricted/bam/<bam-file>.bam  You will be prompted to confirm that you wish to run the application with the following JSON input and whether you would like to monitor the job in your terminal.  Using input JSON: { \n     \"bam_file\" :  { \n         \" $dnanexus_link \" :  { \n             \"project\" :  \"project-FV9XFG0991ZbPVgQ2jx1vZv5\" ,\n             \"id\" :  \"file-FV9gzf8991ZXQ1kv7V3BqgjV\" \n         } \n     }  } \n\nConfirm running the executable with this input  [ Y/n ] : Y\nCalling applet-FVbY8Qj991ZQ1863BGK6x0bk with output destination project-FV9XFG0991ZbPVgQ2jx1vZv5:/\n\nJob ID: job-FVbY8Z0991ZXx5v1Fk3QgJPV\nWatch launched job now?  [ Y/n ]  Y\n\nJob Log\n-------\nWatching job job-FVbY8Z0991ZXx5v1Fk3QgJPV. Press Ctrl+C to stop.  You can also monitor active jobs by going to the project space and selecting the \"Monitor\" tab.",
            "title": "Running Your App"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#job-completion",
            "text": "Once the job finishes, you will receive an email from DNAnexus ( notification@dnanexus.com ) about whether the job has completed successfully or failed.  Make sure to check that these emails don't get sent to your spam folder.   Clicking the links in the email should open up a new tab in your browser and take you to the appropriate project. Here, we can see that FastQC has run successfully and that the two files generated by FastQC have been uploaded back into our project space.   Again, if we run the  dx ls  command, we can verify that two new files titled \"fastqc-report.html\" and \"fastqc-report.zip\" are in the root directory of our project.  .  \u251c\u2500\u2500 immediate/  \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...  \u251c\u2500\u2500 dx-fastqc-example-app  \u251c\u2500\u2500 fastqc-report.html  \u251c\u2500\u2500 fastqc-report.zip  \u2514\u2500\u2500 SAMPLE_INFO.txt",
            "title": "Job Completion"
        },
        {
            "location": "/guides/data/creating-a-cloud-app/#conclusion",
            "text": "If you have made it this far, you have likely wrapped your first genomic analysis tool for use in the cloud. For your reference, we have included the final FastQC application at the  St. Jude App Tutorial Repository .  If you have any questions or suggestions on how we can improve this tutorial, please  file an issue , contact us at  https://stjude.cloud/contact , or email us at  support@stjude.cloud .",
            "title": "Conclusion"
        },
        {
            "location": "/guides/data/data-transfer-app/",
            "text": "Data Transfer App\n\n\nThe Data Transfer App is a downloadable tool with an easy to use graphical user interface that allows you to upload/download files to/from your DNAnexus projects on the cloud. \n\n\nFor users looking to upload/download files using the command line, please refer to the \ncommand line interaction guide\n.\n\n\nIf you are interested in viewing the source code, you can do so \nhere\n. If you\nwould like to file an issue you are experiencing with the application,\nyou can do so \nhere\n or let us know your feedback through our \ncontact us form\n.\n\n\nGetting Started\n\n\nClick here\n to download the latest release of the\nSt. Jude Cloud data transfer application. \n\n\nOnce you've completed installing the app, you will see a page that looks like this. \n\n\n\n\nLog in with your DNAnexus credentials (or click on \nI'm a St. Jude employee\n to log in with your St. Jude credentials). \n\n\nEach time you log in, the app will prompt you to grant it access to all files in your DNAnexus projects. Click \nGrant Access\n to proceed. Access is granted per session and will expire once you log out of the data transfer app.\n\n\n\n\nOnce you've given the app access to your DNANexus projects, you will see the projects listed in a sidebar on the left and an upload/download panel on the right.  \n\n\nBefore moving on, we encourage you to take the \nTOUR\n by clicking on the green button in the upper right corner.\n\n\n\n\nAs you will see in the tour, you have the option to \nShow All Files\n in your DNAnexus projects. It is a good idea to always have this option enabled.\n\n\n\n\nWarning\n\n\nYou can increase the concurrency (# of files that will upload or download at the same time) but this will affect the performance of the app. For example changing the concurrency from 1 to 100 will move files at roughly 100th of the speed.\n\n\n\n\nUploading Files\n\n\nSelect the DNAnexus project on the left that you would like to upload files to. Select \nUpload\n in the app's Upload/Download panel. To select files you may either (1) Click in the upload space to select files in your computer's file navigation application \n\n\n\n\nor (2) highlight all the files you want to upload, then drag and drop them into the app's upload space.\n\n\n\n\nReview the list of files to upload, and click \nUpload\n.\n\n\nDownloading Files\n\n\nSelect the DNAnexus project on the left that you would like to download files from. Select \nDownload\n in the app's Upload/Download panel. In may take a minute to display all the files in your project. Once all files are displayed, select the files you want to download and click \nDownload\n.\n\n\n\n\nNote that the Data Transfer App does not recognize any directory structure you may have within your DNAnexus projects. It is simply a dump of all the files in each project.",
            "title": "Data Transfer Application"
        },
        {
            "location": "/guides/data/data-transfer-app/#data-transfer-app",
            "text": "The Data Transfer App is a downloadable tool with an easy to use graphical user interface that allows you to upload/download files to/from your DNAnexus projects on the cloud.   For users looking to upload/download files using the command line, please refer to the  command line interaction guide .  If you are interested in viewing the source code, you can do so  here . If you\nwould like to file an issue you are experiencing with the application,\nyou can do so  here  or let us know your feedback through our  contact us form .",
            "title": "Data Transfer App"
        },
        {
            "location": "/guides/data/data-transfer-app/#getting-started",
            "text": "Click here  to download the latest release of the\nSt. Jude Cloud data transfer application.   Once you've completed installing the app, you will see a page that looks like this.    Log in with your DNAnexus credentials (or click on  I'm a St. Jude employee  to log in with your St. Jude credentials).   Each time you log in, the app will prompt you to grant it access to all files in your DNAnexus projects. Click  Grant Access  to proceed. Access is granted per session and will expire once you log out of the data transfer app.   Once you've given the app access to your DNANexus projects, you will see the projects listed in a sidebar on the left and an upload/download panel on the right.    Before moving on, we encourage you to take the  TOUR  by clicking on the green button in the upper right corner.   As you will see in the tour, you have the option to  Show All Files  in your DNAnexus projects. It is a good idea to always have this option enabled.   Warning  You can increase the concurrency (# of files that will upload or download at the same time) but this will affect the performance of the app. For example changing the concurrency from 1 to 100 will move files at roughly 100th of the speed.",
            "title": "Getting Started"
        },
        {
            "location": "/guides/data/data-transfer-app/#uploading-files",
            "text": "Select the DNAnexus project on the left that you would like to upload files to. Select  Upload  in the app's Upload/Download panel. To select files you may either (1) Click in the upload space to select files in your computer's file navigation application    or (2) highlight all the files you want to upload, then drag and drop them into the app's upload space.   Review the list of files to upload, and click  Upload .",
            "title": "Uploading Files"
        },
        {
            "location": "/guides/data/data-transfer-app/#downloading-files",
            "text": "Select the DNAnexus project on the left that you would like to download files from. Select  Download  in the app's Upload/Download panel. In may take a minute to display all the files in your project. Once all files are displayed, select the files you want to download and click  Download .   Note that the Data Transfer App does not recognize any directory structure you may have within your DNAnexus projects. It is simply a dump of all the files in each project.",
            "title": "Downloading Files"
        },
        {
            "location": "/guides/data/command-line/",
            "text": "Command Line Interaction\n\n\nBefore you begin interacting with St. Jude Cloud Platform from the\ncommand line, you'll need to understand some details on the underlying\narchitecture of the platform. The St. Jude Cloud Platform is built on\ntop of a genomics cloud ecosystem provided by \nDNAnexus\n. \n\n\nOverview\n\n\nWorkspaces in DNAnexus are organized by projects, which are essentially\nfolders in the cloud. Each data request and tool in St. Jude Cloud\ncreates its own unique cloud workspace (DNAnexus project). For instance,\na data request creates a DNAnexus project behind the scenes with the\nsame name as the request name you specify when you request data.\n\n\nInstallation\n\n\nOpen-source software provided by DNAnexus called the \ndx-toolkit\n is\nused to interact with the St. Jude Cloud Platform from the command line.\nYou can use this to create these projects, upload and download data, and\nmany other operations. You'll need to install that software on your\ncomputer by following \nthis guide\n.\n\n\n\n\nTip\n\n\nA quickstart to getting up and running with the dx-toolkit:\n\n\n\n\nInstall Python 2.7.13+. Note that using the system-level Python is\n    usually not a good idea (by default, system level Python is\n    typically too old/does not support the latest security protocols\n    required). You can install using \nAnaconda\n (recommended) or using\n    the default \nPython installer\n.\n\n\nRun \npip install dxpy\n.\n\n\nType \ndx --help\n at the command line.\n\n\n\n\n\n\nA quick tour\n\n\nLogging in\n\n\nTo log in using the dx-toolkit, run the following command:\n\n\ndx login --noprojects\n\n# enter username and password when prompted\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are a St. Jude employee, you'll need to follow \nthis\nguide\n to log in instead.\n\n\n\n\nSelecting a project\n\n\nFirst, you'll need to choose which cloud workspace you would like to\naccess. This depends on if you are downloading data from a request or\nworking with input/output files from a tool. You can see the workspaces\navailable to you by running the following command in your terminal:\n\n\ndx \nselect\n\n\n\n\n\n\nThis will present with a prompt similar to the below screenshot. A list\nof your available cloud workspaces will be shown with a number out to\nthe left of each. You should enter the number corresponding to the\nworkspace you are wanting to interact with. In the example below, the\nuser has selected the Rapid RNA-Seq tool.\n\n\n\n\nSome useful commands\n\n\nMoving data back and forth between the cloud and your local computer is\nsimple once you have selected the correct project for your tool.\n\n\nYou will find that many common Linux commands with \ndx\n prepended work as expected.\n\n\n# list available files for the tool for the main folder\n\ndx ls\n\n\n# list all available files for the tool\n\ndx find .\n\n\n# list all commands\n\ndx --help\n\n\n\n\n\nUploading data\n\n\nYou can use the following process to upload data to be used by St. Jude\nCloud Platform tools:\n\n\n\n\n\n\nFirst, click \"View\" on the tool you'd like to run from \nthis\n    page\n. In this example, we will\n    choose the Rapid RNA-Seq tool.\n\n\n\n\n\n\nIf you have not already, click \"Start\" on the tool you'd like to\n    run. This will create a cloud workspace for you to upload your\n    data to with the same name as the tool.\n\n\n\n\n\n\n\n\nOpen up your terminal application and select the cloud workspace\n    with the same name as the tool you are trying to run.\n\n\n\n\n\n\n\n\nLast, navigate to the local files you'd like to upload to the cloud\n    and use the \ndx upload\n command as specified in\n    [upload-download-data]{role=\"ref\"} to upload your data to St. Jude\n    Cloud.\n\n\n\n\n\n\n\n\nDownloading data\n\n\n\n\nWarning\n\n\nTo download data from a St. Jude Cloud data request, you must have\nindicated that you wished to download the data in your Data Access\nAgreement (DAA) during your submission. Any downloading of St. Jude data\nwithout completing this step is strictly PROHIBITED.\n\n\n\n\nYou can use the following steps to download data from a St. Jude Cloud\ndata request:\n\n\n\n\n\n\nComplete a data request using the St. Jude Cloud Platform. In this\n    example, we've created a request with the name \"Retinoblastoma\n    Data\".\n\n\n\n\n\n\n\n\nOpen up your terminal application and select the cloud workspace\n    relevant to your data request. For instance, in this case we\n    would type \ndx \nselect\n \n\"Retinoblastoma Data\"\n.\n\n\n\n\n\n\n\n\nYou can use typical commands like \ndx ls\n,\n    \ndx \npwd\n, and \ndx \ncd\n to navigate around\n    your cloud folder as you would a local folder. Your project may look\n    different based on what data you requested and whether you were\n    previously approved to access the data. Your data should either be\n    in the \nrestricted\n folder (if this is your first time\n    requesting access) or the \nimmediate\n folder (if you\n    were previously granted access permission).\n\n\n\n\n\n\n\n\nIn the root of every data request is a file called\n    \nSAMPLE_INFO.txt\n. This should contain all of the\n    information about the samples you checked out as well as the\n    associated metadata we provide.\n\n\n\n\n\n\nTo download data from the cloud to local storage, use the\n    \ndx download\n command as specified in\n    [upload-download-data]{role=\"ref\"}. For instance, if I wanted to\n    download all of the BAM files to my local computer, I would type\n    \ndx download immediate/bam/*\n.",
            "title": "Command Line Interaction"
        },
        {
            "location": "/guides/data/command-line/#command-line-interaction",
            "text": "Before you begin interacting with St. Jude Cloud Platform from the\ncommand line, you'll need to understand some details on the underlying\narchitecture of the platform. The St. Jude Cloud Platform is built on\ntop of a genomics cloud ecosystem provided by  DNAnexus .",
            "title": "Command Line Interaction"
        },
        {
            "location": "/guides/data/command-line/#overview",
            "text": "Workspaces in DNAnexus are organized by projects, which are essentially\nfolders in the cloud. Each data request and tool in St. Jude Cloud\ncreates its own unique cloud workspace (DNAnexus project). For instance,\na data request creates a DNAnexus project behind the scenes with the\nsame name as the request name you specify when you request data.",
            "title": "Overview"
        },
        {
            "location": "/guides/data/command-line/#installation",
            "text": "Open-source software provided by DNAnexus called the  dx-toolkit  is\nused to interact with the St. Jude Cloud Platform from the command line.\nYou can use this to create these projects, upload and download data, and\nmany other operations. You'll need to install that software on your\ncomputer by following  this guide .   Tip  A quickstart to getting up and running with the dx-toolkit:   Install Python 2.7.13+. Note that using the system-level Python is\n    usually not a good idea (by default, system level Python is\n    typically too old/does not support the latest security protocols\n    required). You can install using  Anaconda  (recommended) or using\n    the default  Python installer .  Run  pip install dxpy .  Type  dx --help  at the command line.",
            "title": "Installation"
        },
        {
            "location": "/guides/data/command-line/#a-quick-tour",
            "text": "",
            "title": "A quick tour"
        },
        {
            "location": "/guides/data/command-line/#logging-in",
            "text": "To log in using the dx-toolkit, run the following command:  dx login --noprojects # enter username and password when prompted    Note  If you are a St. Jude employee, you'll need to follow  this\nguide  to log in instead.",
            "title": "Logging in"
        },
        {
            "location": "/guides/data/command-line/#selecting-a-project",
            "text": "First, you'll need to choose which cloud workspace you would like to\naccess. This depends on if you are downloading data from a request or\nworking with input/output files from a tool. You can see the workspaces\navailable to you by running the following command in your terminal:  dx  select   This will present with a prompt similar to the below screenshot. A list\nof your available cloud workspaces will be shown with a number out to\nthe left of each. You should enter the number corresponding to the\nworkspace you are wanting to interact with. In the example below, the\nuser has selected the Rapid RNA-Seq tool.",
            "title": "Selecting a project"
        },
        {
            "location": "/guides/data/command-line/#some-useful-commands",
            "text": "Moving data back and forth between the cloud and your local computer is\nsimple once you have selected the correct project for your tool.  You will find that many common Linux commands with  dx  prepended work as expected.  # list available files for the tool for the main folder \ndx ls # list all available files for the tool \ndx find . # list all commands \ndx --help",
            "title": "Some useful commands"
        },
        {
            "location": "/guides/data/command-line/#uploading-data",
            "text": "You can use the following process to upload data to be used by St. Jude\nCloud Platform tools:    First, click \"View\" on the tool you'd like to run from  this\n    page . In this example, we will\n    choose the Rapid RNA-Seq tool.    If you have not already, click \"Start\" on the tool you'd like to\n    run. This will create a cloud workspace for you to upload your\n    data to with the same name as the tool.     Open up your terminal application and select the cloud workspace\n    with the same name as the tool you are trying to run.     Last, navigate to the local files you'd like to upload to the cloud\n    and use the  dx upload  command as specified in\n    [upload-download-data]{role=\"ref\"} to upload your data to St. Jude\n    Cloud.",
            "title": "Uploading data"
        },
        {
            "location": "/guides/data/command-line/#downloading-data",
            "text": "Warning  To download data from a St. Jude Cloud data request, you must have\nindicated that you wished to download the data in your Data Access\nAgreement (DAA) during your submission. Any downloading of St. Jude data\nwithout completing this step is strictly PROHIBITED.   You can use the following steps to download data from a St. Jude Cloud\ndata request:    Complete a data request using the St. Jude Cloud Platform. In this\n    example, we've created a request with the name \"Retinoblastoma\n    Data\".     Open up your terminal application and select the cloud workspace\n    relevant to your data request. For instance, in this case we\n    would type  dx  select   \"Retinoblastoma Data\" .     You can use typical commands like  dx ls ,\n     dx  pwd , and  dx  cd  to navigate around\n    your cloud folder as you would a local folder. Your project may look\n    different based on what data you requested and whether you were\n    previously approved to access the data. Your data should either be\n    in the  restricted  folder (if this is your first time\n    requesting access) or the  immediate  folder (if you\n    were previously granted access permission).     In the root of every data request is a file called\n     SAMPLE_INFO.txt . This should contain all of the\n    information about the samples you checked out as well as the\n    associated metadata we provide.    To download data from the cloud to local storage, use the\n     dx download  command as specified in\n    [upload-download-data]{role=\"ref\"}. For instance, if I wanted to\n    download all of the BAM files to my local computer, I would type\n     dx download immediate/bam/* .",
            "title": "Downloading data"
        },
        {
            "location": "/guides/tools/pecan-pie/",
            "text": "Pecan PIE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nMichael Edmonson, Aman Patel\n\n\n\n\n\n\nPublication\n\n\nEdmonson et al., Genome Research 2019\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nPecan PIE (the \nPe\ndiatric \nCan\ncer Variant \nP\nathogenicity\n\nI\nnformation \nE\nxchange) is a cloud-based variant classification\nand interpretation service. It annotates and ranks variants by putative\npathogenicity, then displays them in an interactive web interface for\nformal review and classification following \nACMG guidelines\n. The portal\nalso contains a repository of expert-reviewed germline mutations that\nmay predispose individuals to cancer. It is free for non-commercial use.\n\n\nPecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that\npowers our clinical and research genomics projects. Medal Ceremony\nprovides a 3-level ranking of putative pathogenicity - Gold, Silver or\nBronze - for mutations within disease-related genes. Medal assignment is\nbased on matches to 22 mutation databases, mutation type, population\nfrequency, tumor suppressor status and predicted functional impact. The\nevidence used for medal assignment is imported into an interactive\nvariant review page where an analyst can enter additional curated\ninformation such as primary diagnosis, presence of subsequent neoplasm,\nfamily history and related literature. Classification tags can be\nassigned to curated data enabling automated calculation of pathogenicity\nrating based on ACMG/AMP 2015 guidelines.\n\n\nSee \nfile_download\n PowerPoint slides\n\npresented at the ASHG 2017 annual meeting (note that some of this\ninformation is out of date, various improvements have been made since\nthen).\n\n\nGo to \nhttps://pecan.stjude.cloud/pie\n to get started!\n\n\nOverview\n\n\n\n\nAn overview of the Pecan PIE workflow:\n\n\n\n\nLog in and upload a VCF of SNVs and indels.\n\n\nThe portal will process your variants, notifying you upon\n    completion. Variants are annotated with VEP+ (VEP with\n    postprocessing for enhanced splice variant calling) then classified\n    with Medal Ceremony.\n\n\nBrowse results, which include a detailed page for each variation.\n    Variants may be formally classified with an interface based on ACMG\n    guidelines.\n\n\n\n\nGetting started\n\n\nStart by logging into the portal with a DNAnexus account, creating an\naccount if you need one. PIE uses DNAnexus as a secure cloud backend.\nLogging in is required for private storage of your data and so that we\ncan send you e-mail notifications when your analysis jobs are complete.\nPIE is free for non-commercial use. St. Jude pays the (small) cloud\ncomputing costs, your DNAnexus account will not be billed.\n\n\nUploading data\n\n\nPecan PIE takes standard VCF files as input, which may be either\nuncompressed or compressed with \nbgzip\n.\n\n\n\n\nClick the \"Securely upload a VCF file\" button.\n\n\nChoose the genome your variants were mapped to, which may be either\n   GRCh37-lite or GRCh38.\n\n\n\n\nAdvanced options\n\n\nThe \"Advanced option\" panel lets you customize the behavior of the\npipeline:\n\n\n\n\nGene list: Pick a gene list from the pulldown. This filters your\n  variants to genes in the specified list. This option is required and turned\n  on automatically if your uploaded file is 2 megabytes or larger. See the\n  \nfrequently asked questions\n for more\n  information. This option reduces the variant processing burden on PIE by\n  removing variants that will not be assigned a medal in any case because\n  they are not on the cancer predisposition gene list. You can review the\n  genes by clicking on the link that will appear just below the pull down\n  titled \"See gene list\".\n\n\nCustom gene list: Choosing \"custom\" as your gene list will open a\n  window that will let you paste in a list of genes. Any invalid genes will\n  be dropped from your list automatically. You can separate your genes by spaces\n  or new lines.\n\n\nMax Population frequency: PIE by default will not call medals for\n  variants present in the ExAC (ex-TCGA) database at an allele\n  frequency greater than 0.001. This option lets you override the\n  filtering threshold to whatever frequency you prefer. To disable\n  filtering altogether, specify a value of 1.\n\n\n\n\nProgress page\n\n\nAfter uploading is complete you will be taken to a status screen showing\nthe progress of your job through the system. Analysis typically takes\n10-15 minutes depending on file size and system availability.\n\n\nIt isn't necessary to keep your browser open on this page until your\nresults are ready: the system will e-mail you with a link to return to\nyour results. Optional browser notifications are also available.\n\n\nAnalysis of Results\n\n\nResults browser\n\n\nWhen your job is complete you will be taken to an overview page where\nyou can browse your results and examine a detailed results page for each\nvariant.\n\n\n\n\nThe variants in the results can be filtered by:\n\n\n\n\n\n\n\n\nFilter\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nClass\n\n\nPredicted effect of variant on protein coding, e.g. missense, nonsense, etc.\n\n\n\n\n\n\nSomatic medal\n\n\nMedal assigned to the variant by the somatic classifier.\n\n\n\n\n\n\nGermline medal\n\n\nMedal assigned to the variant by the germline classifier.\n\n\n\n\n\n\nCommittee Classification\n\n\nIf the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank.\n\n\n\n\n\n\n\n\nThe \"search\" box lets you filter the results by gene and/or amino acid\nchange. The view is dynamically filtered to matching variants as you\ntype.\n\n\nMedal meaning\n\n\nMedals are only assigned for coding and splice-related variants in\ndisease predisposition genes. Germline medals are only assigned for\nnovel variants or those present in the ExAC (ex-TCGA) database with a\nMAF no greater than 0.1% (0.001 expressed fractionally).\n\n\n\n\n\n\nGold medals\n are assigned to truncations in tumor suppressor genes,\nhotspots derived from the COSMIC database, as well as perfect matches to\nvariants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases.\n\n\nSilver medals\n are assigned to in-frame indels, truncations in non-tumor\nsuppressor genes, variants predicted deleterious by damage-prediction\nalgorithms, variants receiving a gold medal from the somatic classifier,\nand perfect matches to variants in the following databases: ClinVar\n(predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD.\n\n\nBronze medals\n are assigned to variants predicted tolerated by\ndamage-prediction algorithms. Variants having an imperfect match to a\ndatabase (i.e. different variants at the same genomic position or codon)\ntypically receive a lesser medal.\n\n\n\n\nA summary graphic can be found in slide 4 of the ASHG 2017 presentation (\ndownload here\n). \nFor additional details see \nZhang et al., NEJM\n2015\n\n(supplementary appendix pp. 7-10).\n\n\nVariant page\n\n\nEach variant links to a detailed variant page, which integrates data\nfrom a variety of sources. If either you or the St. Jude germline\nvariant review committee have annotated a variant, that information will\nbe pre-populated.\n\n\nSummary information\n\n\nThe top of the page shows a summary of the variant, including its\ngenomic and HGVS annotations, predicted effect on the protein, and\nsomatic and germline medals. A description of the gene from Entrez\nfollows, and a custom description or selection rationale may also be\nentered.\n\n\nMedal call information\n\n\nClicking on one of the medal icons (gold, silver, bronze, unknown) or\non the top of the page will show a summary of information related to the\nmedal call.\n\n\nProteinPaint\n\n\nAn embedded version of ProteinPaint (\nZhou et al., Nat. Genet.\n2016\n) appears next, showing\nthe variant in the context of a number of pediatric datasets including\nPCGP. A link is provided to the main ProteinPaint application which\nprovides visualizations for additional datasets, including COSMIC and\nClinVar.\n\n\nASHG pathogenicity classification\n\n\nFormal variant pathogenicity classification is supported by an interface\nimplementing ACMG guidelines (\nRichards et al., Genet Med.\n2015\n).\nThe analyst reviews a series of curated category tags, assigning\napplicable tags to the variant and optionally supplying additional\ninformation for each such as PubMed IDs and supporting evidence. The\nsystem will then compute an appropriate pathogenicity score based on the\nuser-flagged categories. Additional free-form custom evidence can also\nbe entered. This structured approach both helps eliminate arbitrary\ndecision-making from the pathogenicity classification process and also\nconstructs a concise summary of the logic and evidence supporting the\nfinal call.\n\n\nClinVar and allele frequency\n\n\nMatches of the variant in ClinVar are also provided, along with\npredicted clinical significance and review status.\n\n\nAllele frequencies for the variant in the PCGP (somatic and\ngermline), NHLBI ESP 6500, and ExAC databases are presented both as\nfractional values and on a log10 plot. Detailed allele population\nbreakdowns are provided for ExAC.\n\n\nDamage prediction algorithms\n\n\nPrecomputed damage-prediction algorithm calls for nonsynonymous coding\nSNVs are presented from the dbNSFP database. Available algorithms are\nPolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT.\nThe calls are presented in a circular diagram with entries color-coded\nbased on the predicted severity of the result.\n\n\nMedal ceremony and linkouts\n\n\nAdditional output from medal ceremony classification can also be\nreviewed. This is only loosely structured, additional fields here may\neventually be integrated into Pecan PIE.\n\n\nLinks are provided to relevant dbSNP entries and other information\nsources.\n\n\nFinal classification\n\n\nThe final 5-tier ACMG classification can be selected after which the\ndecision will be marked as reviewed. A checkbox is also available to\nindicate this variant is a potential candidate for functional review.\n\n\nStandalone usage\n\n\nThis section is intended only for users who want to invoke Pecan PIE's\nunderlying analysis pipelines independently on the\n\nDNAnexus\n platform. If you just want to use\nthe Pecan PIE website you can safely ignore this section of the\ndocumentation. We assume familiarity with the DNAnexus platform. If you aren't\nfamiliar with this, DNAnexus' \nquickstart guide\n is a\ngreat place to start.\n\n\n\n\nWarning\n\n\nThis section of the guide is only relevant to power users!\n\n\n\n\nTwo DNAnexus cloud application pipelines were created during the\ndevelopment of Pecan PIE:\n\n\n\n\n\n\n\n\nName\n\n\nCorresponding DNAnexus App\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVEP+\n\n\napp-stjude_vep_plus\n\n\nA cloud installation of \nVEP\n with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format.\n\n\n\n\n\n\nMedal Ceremony\n\n\napp-stjude_medal_ceremony\n\n\nAdditional annotation and automated variant classification. Requires a special input format which is produced by VEP+.\n\n\n\n\n\n\n\n\nPermissions\n\n\nIn order to run the cloud pipelines independently, your DNAnexus account\nneeds to be granted permissions to access them. After your initial login\nto St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single\nlogin is required even if you just want to use the standalone pipelines\nrather than the Pecan PIE portal (\ncontact\nus\n if you encounter problems accessing\nthe pipelines).\n\n\nThere are two methods of running pipelines on DNAnexus:\n\n\n\n\nDNAnexus GUI.\n DNAnexus provides a standardized graphical user interface for\n    configurating, launching, and monitoring jobs on the cloud. Our\n    pipelines can be run like any other DNAnexus pipeline.\n\n\nCommand line.\n Jobs may also be invoked via the \ndx\n command line\n    client. Command-line use allows submitting cloud jobs without\n    interacting with a GUI, and so supports scripting and easier\n    integration with local workflows. See \nthis section\n\n    for information on how to get set up with the \ndx-toolkit\n.\n\n\n\n\n\n\nNote\n\n\nThe following examples demonstrate command-line usage.\n\n\n\n\nUploading files\n\n\nAll input files must be uploaded onto the DNAnexus platform. When\nspecifying files for input you can use either the DNAnexus fie IDs (e.g.\n\nfile-FBgvp680gz1bGQ5p8yZKz69g\n), or the filenames if they are unique. For\nan idea of how to upload files to DNAnexus, see \nthis guide\n.\n\n\nStep 1: Running VEP+\n\n\nTo run the VEP+ DNAnexus app, you can use the following \ndx\n\ncommand with your own inputs in place of the example's:\n\n\ndx run app-stjude_vep_plus -iinput_file\n=\nmy_vcf.vcf -igenome_string\n=\nGRCh37-lite -igermline_reviewable_only\n=\ntrue\n\n\n\n\n\n\n\n\nTip\n\n\n\n\ngenome_string\n must be either \nGRCh37-lite\n or \nGRCh38\n. If \nGRCh38\n\nis specified, variants will be lifted over to \nGRCh37-lite\n in output,\ni.e. the output will always be \nGRCh37-lite\n (Medal Ceremony currently only supports \nGRCh37-lite\n).\n\n\nThe input VCF specified by \ninput_file\n may be either\nuncompressed, or compressed with \nbgzip\n \nonly\n (htslib/tabix\npackages).\n\n\nThe \ngermline_reviewable_only\n parameter is optional, but\nstrongly recommended. If specified, only variants in disease-gene\nrelated intervals will be annotated, which is appropriate for\nMedal Ceremony. If this option is not specified all variants will\nbe annotated, which depending on the size of your VCF might take a\nlot longer, and many of the resulting variants won't be usable by\nMedal Ceremony. If you want to do this anyway and have a large\nnumber of variants, consider submitting your job to an instance\nwith more CPU cores (e.g. \nmem1_ssd1_x16\n or \nmem1_ssd1_x32\n) as\nthe code will take advantage of the additional cores. If you are\nusing a custom gene list (below) that takes precedence and this\nparameter is not needed.\n\n\nThe optional parameter \ncustom_genes_file\n specifies a plain\ntext file of HUGO gene symbols to analyze (whitespace separated,\nor one per line). If specified, analysis will be restricted to\nthese genes only.\n\n\nThis pipeline produces two output files, \noutput_file\n contains\nannotations for all variants, while \nmedal_prep_output_file\n is\nthe specially-filtered and formatted file required as input to\nMedal Ceremony below.\n\n\n\n\n\n\nStep 2: Running Medal Ceremony\n\n\nTo run the medal ceremony DNAnexus app, you can use the following \ndx\n\ncommand with your own inputs in place of the example's:\n\n\ndx run app-stjude_medal_ceremony -iinfile\n=\nmedal_prep_output_file\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe optional parameter \ncustom_genes_file\n operates in the same\nway as in the VEP+ pipeline above. For custom gene lists to work\nproperly this parameter must be specified when running both the\nVEP+ and Medal Ceremony pipelines.\n\n\nThe optional parameter \nmax_population_frequency\n may be\nspecified, a fractional value representing the maximum population\nfrequency allowed for a variant in the ExAC (ex-TCGA) database to\nreceive a medal. The default is 0.001, a.k.a. \".1%\".\n\n\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to reach out on\n\nour contact form\n.\n\n\nQ: Which files are supported?\n\n\nPIE works with variants in VCF format:\n\n\n\n\nUploaded files must be compliant with the \nVCF specification\n.\n\n\nVCF files may be either uncompressed, or compressed with \nbgzip\n \nonly\n. \nbgzip\n is part of the htslib/tabix packages (see below).\n\n\n\n\nImproperly formatted VCF files will not work with PIE. Some common\nproblems include:\n\n\n\n\nMissing header line\n\n\nMissing required columns\n\n\nFiles were compressed by gzip, zip, or any method other than the required bgzip\n\n\n\n\nTo verify compatibility of your VCF you can try one of these methods:\n\n\n\n\nCompressing your VCF with\n   \nbgzip\n and indexing it with\n   \ntabix\n, both programs from\n   the \nHTSlib\n package (some systems also\n   use the earlier, pre-HTSlib \"tabix\" package). This process will\n   only succeed for compliant VCF files, and can help diagnose\n   failures.\n\n\nRunning \"vcf-validator\" program from the\n   \nvcftools\n package.\n\n\n\n\nWhile the VCF specification also requires that variants be sorted by\nchromosome name and position, PIE is now often able to automatically\ncorrect sorting issues in uploaded files. PIE requires sorted data in\norder to query data for targeted genes.\n\n\nQ: Are there limits on the size of VCF files?\n\n\nUploaded files must not exceed 4 gigabytes. If an uploaded file is larger\nthan 2 megabytes, the cancer predisposition gene list filter will be\nautomatically enabled unless you are using a custom gene list. This reduces\nthe processing burden on the system by removing variants outside of targeted\ngenes.\n\n\nQ: Is there an example/demo VCF I can try with PIE?\n\n\nA. You can use \nthis VCF\n\nfrom the Genome in a Bottle project. This ~133 megabyte\nbgzip-compressed VCF was used during testing of Pecan PIE and is known\nto work. These variants are mapped to GRCh37.\n\n\nQ. What genome versions are supported?\n\n\nA. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38.\nGRCh38 variants are automatically lifted over to 37, as the system\nuses 37 internally. The liftover process is able to compensate for\nstrand and reference/variant allele swaps which can occur. A native\nhg38 version is in development, but is not yet available.\n\n\nPecan PIE only works for human data.\n\n\nQ. What genes are on the curated gene list?\n\n\nA. The list consists of disease-related genes, both cancer and\nnon-cancer, see the \nfile_download\n Excel spreadsheet\n\nfor details. Filtering the source variants to a target list of genes\nreduces the processing burden on the system.\n\n\nWhen browsing the results the view may be filtered to disease\nsub-categories of interest.\n\n\nYou can also specify your own custom list of genes to process when\nsubmitting your VCF file (see the advanced options panel).\n\n\nQ. Why is the classification column blank in my results?\n\n\nQ. This column displays the classification assigned by the St. Jude\nGermline Committee reviewers. If a variant was not classified by this\ncommittee before, this field will be blank.\n\n\nPecan PIE provides classifications from the Medal Ceremony pipeline,\nwhich may assign variants gold, silver, or bronze medals. An \"Unknown\"\nmedal may be assigned for non-disease-predisposition genes, variants\npresent in the ExAC (ex-TCGA) database at an allele frequency >\n0.1%, or variants without functional annotations (which includes most\nsilent variants).\n\n\nQ. What do the medals mean?\n\n\nA. The medal column is a rough indicator of the likelihood of the variant\nbeing clinically significant as predicted by the medal ceremony\nsoftware. Variants with gold medals are most likely to be significant,\nand those with no medal are least likely. More details can be found in\nthe \nAnalysis of Results <results>\n\nsection.\n\n\nQ. Why are some of my variants missing?\n\n\nA. Currently only coding and splice-related variants in disease-related\ngenes make it to the medaling process. Intergenic, intronic, and UTR\nvariants are excluded, as are those in non-coding transcripts.\n\n\nQ. Why does the ExAC allele frequency shown differ from the ExAC portal?\n\n\nA. The reported ExAC frequency may differ for several reasons:\n\n\n\n\nPIE uses the TCGA-subtracted distribution of ExAC rather than the\n    main distribution.\n\n\nPIE reports the primary allele frequencies in the ExAC database,\n    specifically the AC, AN, and AF fields from the VCF distribution.\n    The \nExAC portal\n appears to use\n    the \"adjusted\" frequencies which may be different.\n\n\n\n\nQ. Is Pecan PIE free?\n\n\nA. Pecan PIE is free for non-commercial use. St. Jude covers the cost of\nrunning the pipeline and hosting. DNANexus accounts are required to\nkeep track of your jobs in the cloud so that you can retrieve and\nmanage from multiple locations. Accounts also make it possible to\nalert you of job completion via email.",
            "title": "Pecan PIE"
        },
        {
            "location": "/guides/tools/pecan-pie/#pecan-pie",
            "text": "Authors  Michael Edmonson, Aman Patel    Publication  Edmonson et al., Genome Research 2019    Technical Support  Contact Us     Pecan PIE (the  Pe diatric  Can cer Variant  P athogenicity I nformation  E xchange) is a cloud-based variant classification\nand interpretation service. It annotates and ranks variants by putative\npathogenicity, then displays them in an interactive web interface for\nformal review and classification following  ACMG guidelines . The portal\nalso contains a repository of expert-reviewed germline mutations that\nmay predispose individuals to cancer. It is free for non-commercial use.  Pecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that\npowers our clinical and research genomics projects. Medal Ceremony\nprovides a 3-level ranking of putative pathogenicity - Gold, Silver or\nBronze - for mutations within disease-related genes. Medal assignment is\nbased on matches to 22 mutation databases, mutation type, population\nfrequency, tumor suppressor status and predicted functional impact. The\nevidence used for medal assignment is imported into an interactive\nvariant review page where an analyst can enter additional curated\ninformation such as primary diagnosis, presence of subsequent neoplasm,\nfamily history and related literature. Classification tags can be\nassigned to curated data enabling automated calculation of pathogenicity\nrating based on ACMG/AMP 2015 guidelines.  See  file_download  PowerPoint slides \npresented at the ASHG 2017 annual meeting (note that some of this\ninformation is out of date, various improvements have been made since\nthen).  Go to  https://pecan.stjude.cloud/pie  to get started!",
            "title": "Pecan PIE"
        },
        {
            "location": "/guides/tools/pecan-pie/#overview",
            "text": "An overview of the Pecan PIE workflow:   Log in and upload a VCF of SNVs and indels.  The portal will process your variants, notifying you upon\n    completion. Variants are annotated with VEP+ (VEP with\n    postprocessing for enhanced splice variant calling) then classified\n    with Medal Ceremony.  Browse results, which include a detailed page for each variation.\n    Variants may be formally classified with an interface based on ACMG\n    guidelines.",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/pecan-pie/#getting-started",
            "text": "Start by logging into the portal with a DNAnexus account, creating an\naccount if you need one. PIE uses DNAnexus as a secure cloud backend.\nLogging in is required for private storage of your data and so that we\ncan send you e-mail notifications when your analysis jobs are complete.\nPIE is free for non-commercial use. St. Jude pays the (small) cloud\ncomputing costs, your DNAnexus account will not be billed.",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/pecan-pie/#uploading-data",
            "text": "Pecan PIE takes standard VCF files as input, which may be either\nuncompressed or compressed with  bgzip .   Click the \"Securely upload a VCF file\" button.  Choose the genome your variants were mapped to, which may be either\n   GRCh37-lite or GRCh38.",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/pecan-pie/#advanced-options",
            "text": "The \"Advanced option\" panel lets you customize the behavior of the\npipeline:   Gene list: Pick a gene list from the pulldown. This filters your\n  variants to genes in the specified list. This option is required and turned\n  on automatically if your uploaded file is 2 megabytes or larger. See the\n   frequently asked questions  for more\n  information. This option reduces the variant processing burden on PIE by\n  removing variants that will not be assigned a medal in any case because\n  they are not on the cancer predisposition gene list. You can review the\n  genes by clicking on the link that will appear just below the pull down\n  titled \"See gene list\".  Custom gene list: Choosing \"custom\" as your gene list will open a\n  window that will let you paste in a list of genes. Any invalid genes will\n  be dropped from your list automatically. You can separate your genes by spaces\n  or new lines.  Max Population frequency: PIE by default will not call medals for\n  variants present in the ExAC (ex-TCGA) database at an allele\n  frequency greater than 0.001. This option lets you override the\n  filtering threshold to whatever frequency you prefer. To disable\n  filtering altogether, specify a value of 1.",
            "title": "Advanced options"
        },
        {
            "location": "/guides/tools/pecan-pie/#progress-page",
            "text": "After uploading is complete you will be taken to a status screen showing\nthe progress of your job through the system. Analysis typically takes\n10-15 minutes depending on file size and system availability.  It isn't necessary to keep your browser open on this page until your\nresults are ready: the system will e-mail you with a link to return to\nyour results. Optional browser notifications are also available.",
            "title": "Progress page"
        },
        {
            "location": "/guides/tools/pecan-pie/#analysis-of-results",
            "text": "",
            "title": "Analysis of Results"
        },
        {
            "location": "/guides/tools/pecan-pie/#results-browser",
            "text": "When your job is complete you will be taken to an overview page where\nyou can browse your results and examine a detailed results page for each\nvariant.   The variants in the results can be filtered by:     Filter  Meaning      Class  Predicted effect of variant on protein coding, e.g. missense, nonsense, etc.    Somatic medal  Medal assigned to the variant by the somatic classifier.    Germline medal  Medal assigned to the variant by the germline classifier.    Committee Classification  If the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank.     The \"search\" box lets you filter the results by gene and/or amino acid\nchange. The view is dynamically filtered to matching variants as you\ntype.",
            "title": "Results browser"
        },
        {
            "location": "/guides/tools/pecan-pie/#medal-meaning",
            "text": "Medals are only assigned for coding and splice-related variants in\ndisease predisposition genes. Germline medals are only assigned for\nnovel variants or those present in the ExAC (ex-TCGA) database with a\nMAF no greater than 0.1% (0.001 expressed fractionally).    Gold medals  are assigned to truncations in tumor suppressor genes,\nhotspots derived from the COSMIC database, as well as perfect matches to\nvariants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases.  Silver medals  are assigned to in-frame indels, truncations in non-tumor\nsuppressor genes, variants predicted deleterious by damage-prediction\nalgorithms, variants receiving a gold medal from the somatic classifier,\nand perfect matches to variants in the following databases: ClinVar\n(predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD.  Bronze medals  are assigned to variants predicted tolerated by\ndamage-prediction algorithms. Variants having an imperfect match to a\ndatabase (i.e. different variants at the same genomic position or codon)\ntypically receive a lesser medal.   A summary graphic can be found in slide 4 of the ASHG 2017 presentation ( download here ). \nFor additional details see  Zhang et al., NEJM\n2015 \n(supplementary appendix pp. 7-10).",
            "title": "Medal meaning"
        },
        {
            "location": "/guides/tools/pecan-pie/#variant-page",
            "text": "Each variant links to a detailed variant page, which integrates data\nfrom a variety of sources. If either you or the St. Jude germline\nvariant review committee have annotated a variant, that information will\nbe pre-populated.  Summary information  The top of the page shows a summary of the variant, including its\ngenomic and HGVS annotations, predicted effect on the protein, and\nsomatic and germline medals. A description of the gene from Entrez\nfollows, and a custom description or selection rationale may also be\nentered.  Medal call information  Clicking on one of the medal icons (gold, silver, bronze, unknown) or\non the top of the page will show a summary of information related to the\nmedal call.  ProteinPaint  An embedded version of ProteinPaint ( Zhou et al., Nat. Genet.\n2016 ) appears next, showing\nthe variant in the context of a number of pediatric datasets including\nPCGP. A link is provided to the main ProteinPaint application which\nprovides visualizations for additional datasets, including COSMIC and\nClinVar.  ASHG pathogenicity classification  Formal variant pathogenicity classification is supported by an interface\nimplementing ACMG guidelines ( Richards et al., Genet Med.\n2015 ).\nThe analyst reviews a series of curated category tags, assigning\napplicable tags to the variant and optionally supplying additional\ninformation for each such as PubMed IDs and supporting evidence. The\nsystem will then compute an appropriate pathogenicity score based on the\nuser-flagged categories. Additional free-form custom evidence can also\nbe entered. This structured approach both helps eliminate arbitrary\ndecision-making from the pathogenicity classification process and also\nconstructs a concise summary of the logic and evidence supporting the\nfinal call.  ClinVar and allele frequency  Matches of the variant in ClinVar are also provided, along with\npredicted clinical significance and review status.  Allele frequencies for the variant in the PCGP (somatic and\ngermline), NHLBI ESP 6500, and ExAC databases are presented both as\nfractional values and on a log10 plot. Detailed allele population\nbreakdowns are provided for ExAC.  Damage prediction algorithms  Precomputed damage-prediction algorithm calls for nonsynonymous coding\nSNVs are presented from the dbNSFP database. Available algorithms are\nPolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT.\nThe calls are presented in a circular diagram with entries color-coded\nbased on the predicted severity of the result.  Medal ceremony and linkouts  Additional output from medal ceremony classification can also be\nreviewed. This is only loosely structured, additional fields here may\neventually be integrated into Pecan PIE.  Links are provided to relevant dbSNP entries and other information\nsources.  Final classification  The final 5-tier ACMG classification can be selected after which the\ndecision will be marked as reviewed. A checkbox is also available to\nindicate this variant is a potential candidate for functional review.",
            "title": "Variant page"
        },
        {
            "location": "/guides/tools/pecan-pie/#standalone-usage",
            "text": "This section is intended only for users who want to invoke Pecan PIE's\nunderlying analysis pipelines independently on the DNAnexus  platform. If you just want to use\nthe Pecan PIE website you can safely ignore this section of the\ndocumentation. We assume familiarity with the DNAnexus platform. If you aren't\nfamiliar with this, DNAnexus'  quickstart guide  is a\ngreat place to start.   Warning  This section of the guide is only relevant to power users!   Two DNAnexus cloud application pipelines were created during the\ndevelopment of Pecan PIE:     Name  Corresponding DNAnexus App  Description      VEP+  app-stjude_vep_plus  A cloud installation of  VEP  with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format.    Medal Ceremony  app-stjude_medal_ceremony  Additional annotation and automated variant classification. Requires a special input format which is produced by VEP+.",
            "title": "Standalone usage"
        },
        {
            "location": "/guides/tools/pecan-pie/#permissions",
            "text": "In order to run the cloud pipelines independently, your DNAnexus account\nneeds to be granted permissions to access them. After your initial login\nto St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single\nlogin is required even if you just want to use the standalone pipelines\nrather than the Pecan PIE portal ( contact\nus  if you encounter problems accessing\nthe pipelines).  There are two methods of running pipelines on DNAnexus:   DNAnexus GUI.  DNAnexus provides a standardized graphical user interface for\n    configurating, launching, and monitoring jobs on the cloud. Our\n    pipelines can be run like any other DNAnexus pipeline.  Command line.  Jobs may also be invoked via the  dx  command line\n    client. Command-line use allows submitting cloud jobs without\n    interacting with a GUI, and so supports scripting and easier\n    integration with local workflows. See  this section \n    for information on how to get set up with the  dx-toolkit .    Note  The following examples demonstrate command-line usage.",
            "title": "Permissions"
        },
        {
            "location": "/guides/tools/pecan-pie/#uploading-files",
            "text": "All input files must be uploaded onto the DNAnexus platform. When\nspecifying files for input you can use either the DNAnexus fie IDs (e.g. file-FBgvp680gz1bGQ5p8yZKz69g ), or the filenames if they are unique. For\nan idea of how to upload files to DNAnexus, see  this guide .",
            "title": "Uploading files"
        },
        {
            "location": "/guides/tools/pecan-pie/#step-1-running-vep",
            "text": "To run the VEP+ DNAnexus app, you can use the following  dx \ncommand with your own inputs in place of the example's:  dx run app-stjude_vep_plus -iinput_file = my_vcf.vcf -igenome_string = GRCh37-lite -igermline_reviewable_only = true    Tip   genome_string  must be either  GRCh37-lite  or  GRCh38 . If  GRCh38 \nis specified, variants will be lifted over to  GRCh37-lite  in output,\ni.e. the output will always be  GRCh37-lite  (Medal Ceremony currently only supports  GRCh37-lite ).  The input VCF specified by  input_file  may be either\nuncompressed, or compressed with  bgzip   only  (htslib/tabix\npackages).  The  germline_reviewable_only  parameter is optional, but\nstrongly recommended. If specified, only variants in disease-gene\nrelated intervals will be annotated, which is appropriate for\nMedal Ceremony. If this option is not specified all variants will\nbe annotated, which depending on the size of your VCF might take a\nlot longer, and many of the resulting variants won't be usable by\nMedal Ceremony. If you want to do this anyway and have a large\nnumber of variants, consider submitting your job to an instance\nwith more CPU cores (e.g.  mem1_ssd1_x16  or  mem1_ssd1_x32 ) as\nthe code will take advantage of the additional cores. If you are\nusing a custom gene list (below) that takes precedence and this\nparameter is not needed.  The optional parameter  custom_genes_file  specifies a plain\ntext file of HUGO gene symbols to analyze (whitespace separated,\nor one per line). If specified, analysis will be restricted to\nthese genes only.  This pipeline produces two output files,  output_file  contains\nannotations for all variants, while  medal_prep_output_file  is\nthe specially-filtered and formatted file required as input to\nMedal Ceremony below.",
            "title": "Step 1: Running VEP+"
        },
        {
            "location": "/guides/tools/pecan-pie/#step-2-running-medal-ceremony",
            "text": "To run the medal ceremony DNAnexus app, you can use the following  dx \ncommand with your own inputs in place of the example's:  dx run app-stjude_medal_ceremony -iinfile = medal_prep_output_file   Tip   The optional parameter  custom_genes_file  operates in the same\nway as in the VEP+ pipeline above. For custom gene lists to work\nproperly this parameter must be specified when running both the\nVEP+ and Medal Ceremony pipelines.  The optional parameter  max_population_frequency  may be\nspecified, a fractional value representing the maximum population\nfrequency allowed for a variant in the ExAC (ex-TCGA) database to\nreceive a medal. The default is 0.001, a.k.a. \".1%\".",
            "title": "Step 2: Running Medal Ceremony"
        },
        {
            "location": "/guides/tools/pecan-pie/#frequently-asked-questions",
            "text": "If you have any questions not covered here, feel free to reach out on our contact form .  Q: Which files are supported?  PIE works with variants in VCF format:   Uploaded files must be compliant with the  VCF specification .  VCF files may be either uncompressed, or compressed with  bgzip   only .  bgzip  is part of the htslib/tabix packages (see below).   Improperly formatted VCF files will not work with PIE. Some common\nproblems include:   Missing header line  Missing required columns  Files were compressed by gzip, zip, or any method other than the required bgzip   To verify compatibility of your VCF you can try one of these methods:   Compressing your VCF with\n    bgzip  and indexing it with\n    tabix , both programs from\n   the  HTSlib  package (some systems also\n   use the earlier, pre-HTSlib \"tabix\" package). This process will\n   only succeed for compliant VCF files, and can help diagnose\n   failures.  Running \"vcf-validator\" program from the\n    vcftools  package.   While the VCF specification also requires that variants be sorted by\nchromosome name and position, PIE is now often able to automatically\ncorrect sorting issues in uploaded files. PIE requires sorted data in\norder to query data for targeted genes.  Q: Are there limits on the size of VCF files?  Uploaded files must not exceed 4 gigabytes. If an uploaded file is larger\nthan 2 megabytes, the cancer predisposition gene list filter will be\nautomatically enabled unless you are using a custom gene list. This reduces\nthe processing burden on the system by removing variants outside of targeted\ngenes.  Q: Is there an example/demo VCF I can try with PIE?  A. You can use  this VCF \nfrom the Genome in a Bottle project. This ~133 megabyte\nbgzip-compressed VCF was used during testing of Pecan PIE and is known\nto work. These variants are mapped to GRCh37.  Q. What genome versions are supported?  A. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38.\nGRCh38 variants are automatically lifted over to 37, as the system\nuses 37 internally. The liftover process is able to compensate for\nstrand and reference/variant allele swaps which can occur. A native\nhg38 version is in development, but is not yet available.  Pecan PIE only works for human data.  Q. What genes are on the curated gene list?  A. The list consists of disease-related genes, both cancer and\nnon-cancer, see the  file_download  Excel spreadsheet \nfor details. Filtering the source variants to a target list of genes\nreduces the processing burden on the system.  When browsing the results the view may be filtered to disease\nsub-categories of interest.  You can also specify your own custom list of genes to process when\nsubmitting your VCF file (see the advanced options panel).  Q. Why is the classification column blank in my results?  Q. This column displays the classification assigned by the St. Jude\nGermline Committee reviewers. If a variant was not classified by this\ncommittee before, this field will be blank.  Pecan PIE provides classifications from the Medal Ceremony pipeline,\nwhich may assign variants gold, silver, or bronze medals. An \"Unknown\"\nmedal may be assigned for non-disease-predisposition genes, variants\npresent in the ExAC (ex-TCGA) database at an allele frequency >\n0.1%, or variants without functional annotations (which includes most\nsilent variants).  Q. What do the medals mean?  A. The medal column is a rough indicator of the likelihood of the variant\nbeing clinically significant as predicted by the medal ceremony\nsoftware. Variants with gold medals are most likely to be significant,\nand those with no medal are least likely. More details can be found in\nthe  Analysis of Results <results> \nsection.  Q. Why are some of my variants missing?  A. Currently only coding and splice-related variants in disease-related\ngenes make it to the medaling process. Intergenic, intronic, and UTR\nvariants are excluded, as are those in non-coding transcripts.  Q. Why does the ExAC allele frequency shown differ from the ExAC portal?  A. The reported ExAC frequency may differ for several reasons:   PIE uses the TCGA-subtracted distribution of ExAC rather than the\n    main distribution.  PIE reports the primary allele frequencies in the ExAC database,\n    specifically the AC, AN, and AF fields from the VCF distribution.\n    The  ExAC portal  appears to use\n    the \"adjusted\" frequencies which may be different.   Q. Is Pecan PIE free?  A. Pecan PIE is free for non-commercial use. St. Jude covers the cost of\nrunning the pipeline and hosting. DNANexus accounts are required to\nkeep track of your jobs in the cloud so that you can retrieve and\nmanage from multiple locations. Accounts also make it possible to\nalert you of job completion via email.",
            "title": "Frequently asked questions"
        },
        {
            "location": "/guides/tools/neoepitope/",
            "text": "Authors\n\n\nTi-Cheng Chang\n\n\n\n\n\n\nPublication\n\n\nThe Neoepitope Landscape in Pediatric Cancers. Genome Medicine. 2017. 9.1: 78\n.\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nCancers are caused by somatically acquired alterations including single \nnucleotide variations (SNVs), small insertion/deletions (indels),\ntranslocations, and other types of rearrangements. The genes affected by\nthese mutations may produce altered proteins, some of which may lead to\nthe emergence of tumor-specific immunogenic epitopes. We developed an\nanalytical workflow for identification of putative neoepitopes based on\nsomatic missense mutations and gene fusions using whole genome\nsequencing data. The workflow has been used to characterize neoepitope\nlandscape of 23 subtypes of pediatric cancer in the Pediatric Cancer\nGenome Project\n1\n.\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n if using FastQ inputs)\n\n\nInput file\n\n\nGzipped FastQ files generated by experiment.\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\nBAM files (\nrequired\n if using BAM inputs)\n\n\nInput file\n\n\nBAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq).\n\n\nSample.bam\n\n\n\n\n\n\nBAM indices (\nrequired\n if using BAM inputs)\n\n\nInput file\n\n\nCorresponding BAM index of the BAM files above.\n\n\nSample.bam.bai\n\n\n\n\n\n\nMutation file (\nrequired\n)\n\n\nInput file\n\n\nFile describing the mutations present in the sample (special format, see below).\n\n\n*.txt (tab-delimited)\n\n\n\n\n\n\nSNV or fusion\n\n\nParameter\n\n\nSpecify the mutation file contains SNV or gene fusion.\n\n\nSNV\n\n\n\n\n\n\nPeptide size\n\n\nParameter\n\n\nSize of the peptide.\n\n\n9\n\n\n\n\n\n\nAffinity threshold\n\n\nParameter\n\n\nAffinity cutoff for epitope prediction report.\n\n\n500\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEpitope affinity prediction (html)\n\n\nEpitope affinity. The peptide with affinity < cutoff will be highlighted.\n\n\n\n\n\n\nEpitope affinity prediction (xlsx)\n\n\nExcel tables for the information of all epitopes\n\n\n\n\n\n\nAffinity (raw output)\n\n\nEpitope affinity\n\n\n\n\n\n\nPeptide sequence (raw output)\n\n\nPeptide sequences in Fasta format\n\n\n\n\n\n\n\n\nHLA Typing Algorithm\n\n\nThe HLA typing algorithm is used to predict the HLA class I alleles.\nUsers can either provide FastQ (paired or single end reads) or a BAM\nfile as input. When using a BAM file as input, the reads surrounding the\nHLA loci and unmapped reads will be extracted. The reads will be fed\ninto Optitype for HLA typing. The default settings for Optitype are\nused. The output of the HLA type can be combined with the our epitope\ndetection algorithm to perform affinity prediction of neoepitopes.\n\n\nIf you use \nFastQ\n files as input:  \n\n\n\n\nThe input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see \nhttps://github.com/FRED-2/OptiType\n).\n\n\nThe fished FastQs will be used for HLA typing using Opitype.\n\n\n\n\nIf you use \nBAM\n files as input:  \n\n\n\n\nThe reads falling within the HLA loci and their paralogous loci will\n    be extracted.\n\n\nThe reads unmapped to the human genome will be extracted.\n\n\nThe reads from step 1 and 2 will be combined and deduplicated (in\n    FastQ format).\n\n\nThe input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see \nhttps://github.com/FRED-2/OptiType\n).\n\n\nThe fished FastQs will be used for HLA typing using Opitype.\n\n\n\n\nEpitope Prediction Algorithm\n\n\nThe epitope prediction algorithm first extracts peptides covering an array\nof tiling peptides (size defined by users) overlapping each missense\nmutation or gene fusion. Fusion junctions can be identified using RNA-Seq\nby fusion detection tools (Li et. al, unpublished). NetMHCcons\n3\n is subsequently \nused to predict affinities of the peptide array for each HLA receptor in \neach sample. The neoepitope with affinity lower than the threshold will \nbe highlighted in output file (default 500 nM). Below is an outline of internal steps the algorithm performs in order to generate the final report.\n\n\n\n\nCheck the version of the genomic position of the input SNV/fusion\n    file.\n\n\nLift over the genomic coordinations if the reference genomic\n    position is not HG19. Currently, the internal genome annotation was\n    based on HG19 and the genome coordinates of the mutation files will\n    be adjusted to HG19 for peptide extraction.\n\n\nExtract the peptide flanking the mutations.\n\n\nRun NetMHCcons to obtain the affinity prediction of the peptides.\n\n\nProduce the affinity report of each peptide.\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nNeoepitopePred tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nInput file configuration\n\n\nUsers need to provide a mutation file for SNV or gene fusion. The format\nof the mutation file is shown in the following example. The file can be\nprepared in Excel and saved as a tab-delimited text file to use as\ninput.\n\n\nThe HLA alleles for testing will be derived from the HLA typing module\nusing the workflow. The peptide size and affinity cutoff can be modified\nby users. \n\n\nMutation file format\n\n\n\n\n\n\n\n\nGeneName\n\n\nSample\n\n\nChr\n\n\nPostion_hg19\n\n\nClass\n\n\nAAChange\n\n\nmRNA_acc\n\n\nReferenceAllele\n\n\nMutantAllele\n\n\n\n\n\n\n\n\n\n\nGene1\n\n\nSampleA\n\n\nchr10\n\n\n106150600\n\n\nmissense\n\n\nR663H\n\n\nNM_00101\n\n\nA\n\n\nT\n\n\n\n\n\n\nGene2\n\n\nSampleA\n\n\nchr2\n\n\n32330151\n\n\nmissense\n\n\nN329N\n\n\nNM_00102\n\n\nT\n\n\nG\n\n\n\n\n\n\n\n\n\n\nNotes on preparing the above file\n\n\n\n\nThe chromosome requires a 'chr' prefix.\n\n\nThe position requires a suffix of HG19/HG38 to indicate the human genome assembly version.\n\n\nOnly the missense mutations/gene fusion is supported currently and\n    the other types of mutations will not be processed.\n\n\n\n\n\n\nMutation file example\n\n\n\n\nUploading data\n\n\nNeoepitopePred takes the following files as input:\n\n\n\n\nA pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be\n  generated from whole genome sequencing, whole exome sequencing, or RNA-Seq. \n\n\nA file describing the mutations in a sample. \n\n\n\n\nYou can upload these files using the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\n\n\nCaution\n\n\nThis pipeline assumes HG19 coordinates in the mutation file. If the\ncoordinates are based on HG38, the coordinates will lifted over to HG19\nto perform epitope affinity prediction.\n\n\n\n\nOnce you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the \ntool's landing page\n. \nA dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.\n\n\n\n\nSelecting parameters\n\n\nThere are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Neoepitope Prediction\" substep. For a full list of the parameters and their\ndescriptions, see \nthe input section\n (specifically, you are \nlooking at the items in the table labeled \"parameters\").\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ/BAM and mutation files you uploaded in \n\nthe upload data section\n. You can do this by \nclicking on the \nBAM alignment file\n and \nBAM index file\n and \nMutation array\n slots and\nselecting the respective files.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more NeoepitopePred runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can refer to the \nDNAnexus Monitoring Executions Documentation\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n \n\n\nInterpreting results\n\n\nHLA typing\n\n\nThe output of this app contain the prediction of the HLA class I alleles\nfrom OptiType.\n\n\n\n\nA folder stamped with the time will present in the output folder\n  (optitype), which contains the raw output.\n\n\n\n\n\n\n\n\nThe file contains the predicted HLA alleles of the sample.\n\n\n\n\n\n\nNeoepitope prediction\n\n\nThe output contains one summary HTML, one folder with raw output, and one\nfolder with outputs in Excel formats:\n\n\n\n\nEpitope_affinity_prediction.html\n (shown below):   \n\n\n\n\nThis file provides a summary of the epitope prediction that can be visualized directly from web browser.  \n\n\nThe peptides with affinity lower than user-defined cutoff will be highlighted in green in the webpage.  \n\n\n\n\n\n\nRaw_output\n (shown below): this folder contains the raw output of the affinity\nprediction. There will two major types files present here: affinity.out and flanking.seq.\n\n\n\n\n\n\naffinity.out\n: these files are the prediction results from the\n    netMHCcons for each peptide.\n\n\n\n\nThe following columns will be shown in the output:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGene name\n\n\nthe name of the genes\n\n\n\n\n\n\nSample\n\n\nthe name of the samples\n\n\n\n\n\n\nChromosome (chr)\n\n\nthe chromosome location of the variation\n\n\n\n\n\n\nPosition\n\n\nthe chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19.\n\n\n\n\n\n\nClass\n\n\nclass of the variation\n\n\n\n\n\n\nReference allele\n\n\nreference allele at the position\n\n\n\n\n\n\nMutant allele\n\n\nmutated allele at the position\n\n\n\n\n\n\nmRNA_acc\n\n\nNCBI accession number of the mRNA\n\n\n\n\n\n\nAllele\n\n\nHLA allele tested\n\n\n\n\n\n\nPeptide\n\n\nthe neoepitope sequences tested\n\n\n\n\n\n\nGene_variant\n\n\nthe gene and variant residues\n\n\n\n\n\n\n1-log50k\n\n\nPrediction score from netMHCcons\n\n\n\n\n\n\nnM\n\n\nAffinity as IC50 values in nM\n\n\n\n\n\n\n%Rank\n\n\n% Rank of prediction score to a set of 200.000 random natural 9mer peptides\n\n\n\n\n\n\nHLAtype\n\n\nAll of the hla alleles predicted in the specific sample\n\n\n\n\n\n\n\n\nflanking.seq\n: these files contain the sequences used for the prediction.\n\n\n\n\nXLSX\n: this folder contains the raw output of the affinity prediction as\ndescribed above in Excel files. The files can be downloaded and opened with Excel for downstream filtering and analyses.\n\n\n\n\nFrequently asked questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact form\n.\n\n\n\n\n\n\n\n\n\n\nDowning JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome\nProject. Nature genetics. 2012;44(6):619-622.\u00a0\n\u21a9\n\n\n\n\n\n\nSzolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O:\nOptiType: precision HLA typing from next-generation sequencing\ndata. Bioinformatics 2014, 30:3310-3316.\u00a0\n\u21a9\n\n\n\n\n\n\nKarosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a\nconsensus method for the major histocompatibility complex class I\npredictions. Immunogenetics 2012, 64:177-186.\u00a0\n\u21a9",
            "title": "NeoepitopePred"
        },
        {
            "location": "/guides/tools/neoepitope/#overview",
            "text": "Inputs     Name  Type  Description  Example      FastQ files ( required  if using FastQ inputs)  Input file  Gzipped FastQ files generated by experiment.  Sample_R1.fastq.gz and Sample_R2.fastq.gz    BAM files ( required  if using BAM inputs)  Input file  BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq).  Sample.bam    BAM indices ( required  if using BAM inputs)  Input file  Corresponding BAM index of the BAM files above.  Sample.bam.bai    Mutation file ( required )  Input file  File describing the mutations present in the sample (special format, see below).  *.txt (tab-delimited)    SNV or fusion  Parameter  Specify the mutation file contains SNV or gene fusion.  SNV    Peptide size  Parameter  Size of the peptide.  9    Affinity threshold  Parameter  Affinity cutoff for epitope prediction report.  500     Outputs     Name  Description      Epitope affinity prediction (html)  Epitope affinity. The peptide with affinity < cutoff will be highlighted.    Epitope affinity prediction (xlsx)  Excel tables for the information of all epitopes    Affinity (raw output)  Epitope affinity    Peptide sequence (raw output)  Peptide sequences in Fasta format     HLA Typing Algorithm  The HLA typing algorithm is used to predict the HLA class I alleles.\nUsers can either provide FastQ (paired or single end reads) or a BAM\nfile as input. When using a BAM file as input, the reads surrounding the\nHLA loci and unmapped reads will be extracted. The reads will be fed\ninto Optitype for HLA typing. The default settings for Optitype are\nused. The output of the HLA type can be combined with the our epitope\ndetection algorithm to perform affinity prediction of neoepitopes.  If you use  FastQ  files as input:     The input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see  https://github.com/FRED-2/OptiType ).  The fished FastQs will be used for HLA typing using Opitype.   If you use  BAM  files as input:     The reads falling within the HLA loci and their paralogous loci will\n    be extracted.  The reads unmapped to the human genome will be extracted.  The reads from step 1 and 2 will be combined and deduplicated (in\n    FastQ format).  The input FastQs will be aligned against the Optitype HLA reference\n    sequences using razers3 (see  https://github.com/FRED-2/OptiType ).  The fished FastQs will be used for HLA typing using Opitype.   Epitope Prediction Algorithm  The epitope prediction algorithm first extracts peptides covering an array\nof tiling peptides (size defined by users) overlapping each missense\nmutation or gene fusion. Fusion junctions can be identified using RNA-Seq\nby fusion detection tools (Li et. al, unpublished). NetMHCcons 3  is subsequently \nused to predict affinities of the peptide array for each HLA receptor in \neach sample. The neoepitope with affinity lower than the threshold will \nbe highlighted in output file (default 500 nM). Below is an outline of internal steps the algorithm performs in order to generate the final report.   Check the version of the genomic position of the input SNV/fusion\n    file.  Lift over the genomic coordinations if the reference genomic\n    position is not HG19. Currently, the internal genome annotation was\n    based on HG19 and the genome coordinates of the mutation files will\n    be adjusted to HG19 for peptide extraction.  Extract the peptide flanking the mutations.  Run NetMHCcons to obtain the affinity prediction of the peptides.  Produce the affinity report of each peptide.",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/neoepitope/#getting-started",
            "text": "To get started, you need to navigate to the  NeoepitopePred tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/neoepitope/#input-file-configuration",
            "text": "Users need to provide a mutation file for SNV or gene fusion. The format\nof the mutation file is shown in the following example. The file can be\nprepared in Excel and saved as a tab-delimited text file to use as\ninput.  The HLA alleles for testing will be derived from the HLA typing module\nusing the workflow. The peptide size and affinity cutoff can be modified\nby users.   Mutation file format     GeneName  Sample  Chr  Postion_hg19  Class  AAChange  mRNA_acc  ReferenceAllele  MutantAllele      Gene1  SampleA  chr10  106150600  missense  R663H  NM_00101  A  T    Gene2  SampleA  chr2  32330151  missense  N329N  NM_00102  T  G      Notes on preparing the above file   The chromosome requires a 'chr' prefix.  The position requires a suffix of HG19/HG38 to indicate the human genome assembly version.  Only the missense mutations/gene fusion is supported currently and\n    the other types of mutations will not be processed.    Mutation file example",
            "title": "Input file configuration"
        },
        {
            "location": "/guides/tools/neoepitope/#uploading-data",
            "text": "NeoepitopePred takes the following files as input:   A pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be\n  generated from whole genome sequencing, whole exome sequencing, or RNA-Seq.   A file describing the mutations in a sample.    You can upload these files using the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/neoepitope/#running-the-tool",
            "text": "Caution  This pipeline assumes HG19 coordinates in the mutation file. If the\ncoordinates are based on HG38, the coordinates will lifted over to HG19\nto perform epitope affinity prediction.   Once you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the  tool's landing page . \nA dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.",
            "title": "Running the tool"
        },
        {
            "location": "/guides/tools/neoepitope/#selecting-parameters",
            "text": "There are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Neoepitope Prediction\" substep. For a full list of the parameters and their\ndescriptions, see  the input section  (specifically, you are \nlooking at the items in the table labeled \"parameters\").",
            "title": "Selecting parameters"
        },
        {
            "location": "/guides/tools/neoepitope/#hooking-up-inputs",
            "text": "Next, you'll need to hook up the FastQ/BAM and mutation files you uploaded in  the upload data section . You can do this by \nclicking on the  BAM alignment file  and  BAM index file  and  Mutation array  slots and\nselecting the respective files.",
            "title": "Hooking up inputs"
        },
        {
            "location": "/guides/tools/neoepitope/#starting-the-workflow",
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.",
            "title": "Starting the workflow"
        },
        {
            "location": "/guides/tools/neoepitope/#monitoring-run-progress",
            "text": "Once you have started one or more NeoepitopePred runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.      Tip  Power users can refer to the  DNAnexus Monitoring Executions Documentation  for advanced capabilities for monitoring jobs.",
            "title": "Monitoring run progress"
        },
        {
            "location": "/guides/tools/neoepitope/#analysis-of-results",
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.",
            "title": "Analysis of results"
        },
        {
            "location": "/guides/tools/neoepitope/#finding-the-raw-results-files",
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.",
            "title": "Finding the raw results files"
        },
        {
            "location": "/guides/tools/neoepitope/#interpreting-results",
            "text": "HLA typing  The output of this app contain the prediction of the HLA class I alleles\nfrom OptiType.   A folder stamped with the time will present in the output folder\n  (optitype), which contains the raw output.     The file contains the predicted HLA alleles of the sample.    Neoepitope prediction  The output contains one summary HTML, one folder with raw output, and one\nfolder with outputs in Excel formats:   Epitope_affinity_prediction.html  (shown below):      This file provides a summary of the epitope prediction that can be visualized directly from web browser.    The peptides with affinity lower than user-defined cutoff will be highlighted in green in the webpage.      Raw_output  (shown below): this folder contains the raw output of the affinity\nprediction. There will two major types files present here: affinity.out and flanking.seq.    affinity.out : these files are the prediction results from the\n    netMHCcons for each peptide.   The following columns will be shown in the output:     Column  Description      Gene name  the name of the genes    Sample  the name of the samples    Chromosome (chr)  the chromosome location of the variation    Position  the chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19.    Class  class of the variation    Reference allele  reference allele at the position    Mutant allele  mutated allele at the position    mRNA_acc  NCBI accession number of the mRNA    Allele  HLA allele tested    Peptide  the neoepitope sequences tested    Gene_variant  the gene and variant residues    1-log50k  Prediction score from netMHCcons    nM  Affinity as IC50 values in nM    %Rank  % Rank of prediction score to a set of 200.000 random natural 9mer peptides    HLAtype  All of the hla alleles predicted in the specific sample     flanking.seq : these files contain the sequences used for the prediction.   XLSX : this folder contains the raw output of the affinity prediction as\ndescribed above in Excel files. The files can be downloaded and opened with Excel for downstream filtering and analyses.",
            "title": "Interpreting results"
        },
        {
            "location": "/guides/tools/neoepitope/#frequently-asked-questions",
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact form .      Downing JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome\nProject. Nature genetics. 2012;44(6):619-622.\u00a0 \u21a9    Szolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O:\nOptiType: precision HLA typing from next-generation sequencing\ndata. Bioinformatics 2014, 30:3310-3316.\u00a0 \u21a9    Karosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a\nconsensus method for the major histocompatibility complex class I\npredictions. Immunogenetics 2012, 64:177-186.\u00a0 \u21a9",
            "title": "Frequently asked questions"
        },
        {
            "location": "/guides/tools/chipseq/",
            "text": "Authors\n\n\nXing Tang, Yong Cheng\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nThe ChIP-Seq Peak Calling workflow follows ENCODE best practices to call \nbroad or narrow peaks on Illumina-generated ChIP-Seq data. \nHere, a Gzipped FastQ file from an Immunoprecipitation (IP) experiment \nis considered the \"case sample file\" and a Gzipped FastQ file from a control \nexperiment is considered the \"control sample file\". The pipeline can run on\nmatched case/control samples (recommended for better results) or just a \ncase sample.\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n if using FastQ inputs)\n\n\nInput file\n\n\nGzipped FastQ files generated by experiment.\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nFormat\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBED file\n\n\n.bed\n\n\nPeak calls\n\n\n\n\n\n\nBinary file\n\n\n.bb\n\n\nBinary format for BED file\n\n\n\n\n\n\nBigWig file\n\n\n.bw\n\n\nShows read coverage\n\n\n\n\n\n\nMetrics file\n\n\n.txt\n\n\nShows mapping and duplication rate\n\n\n\n\n\n\nCross correlation plot\n\n\n.pdf\n\n\nQuality plot showing if the forward and reverse reads tend to be centered around binding sites.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nThe reads of the FastQ file(s) are aligned to the specified reference genome. \n\n\nThe aligned reads are then post-processed based on best-practice QC techniques\n(removing multiple mapped reads, removing duplicated reads, etc). \n\n\nPeaks are called by SICER (broad peak analysis) or MACS2 (narrow peak\nanalysis). \n\n\nQualified peaks will be output as BED (.bed) and big BED (.bb)\nfiles. \n\n\nThe coverage information will be output as a bigWig (.bw)\nfile. \n\n\nA cross correlation plot and general metrics file are generated to help check\nthe quality of experiment.\n\n\n\n\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nChIP-Seq tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an\nIP experiment as input. You can upload your input FastQ files by\nusing the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\n\n\nTip\n\n\nIf you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!\n\n\nFor more information, check out the \ndata transfer application\n guide.\n\n\n\n\nRunning the tool\n\n\nOnce you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the \ntool's landing page\n. \nA dropdown will present the different presets for running the ChIP-Seq workflow.\nYou'll need to decide \n(1)\n whether you'd like to run broad/narrow peak\ncalling and \n(2)\n whether you have a case sample and a control sample (preferred)\nor just a case sample. This will determine which preset you should\nclick in this dropdown. There are various other parameters that you can \nset, but they are covered in further sections of this guide.\n\n\n\n\nBroad vs. narrow peak calling\n\n\nChoosing between broad and narrow peak calling depends on the experiment\ndesign. The following are good rules of thumb for choosing between the\ntwo configurations. If you are not sure which configuration to use,\nplease consult with an expert at your institution or \ncontact us\n.\n\n\nNarrow Peak Calling\n\n\nIf your target protein is a transcription factor, you should probably\nchoose narrow peak calling. You can also try the narrow peak calling\nworkflows for the following histone marks:\n\n\n\n\nH3K4me3\n\n\nH3K4me2\n\n\nH3K9-14ac\n\n\nH3K27ac\n\n\nH2A.Z\n\n\n\n\nBroad Peak Calling\n\n\nYou should try the broad peak calling workflows for the following\nhistone marks:\n\n\n\n\nH3K36me3\n\n\nH3K79me2\n\n\nH3K27me3\n\n\nH3K9me3\n\n\nH3K9me1\n\n\n\n\nSpecial Cases\n\n\nIn some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between\nnarrow and broad shape, you might need to look into each peak region and\nconsult experts.\n\n\n\n\nWarning\n\n\nIf your fragment size is less than 50 base pairs, please refer to the\n\nfrequently asked questions\n.\n\n\n\n\nSelecting parameters\n\n\nThere are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Parameter Wrapper\" substep.\n\n\n\n\nThe following are the parameters that can be set, a short\ndescription of each parameter, and an example value. If you\nhave questions, please \ncontact us\n.\n\n\n\n\n\n\n\n\nParameter Name\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nOutput prefix (\nrequired\n)\n\n\nA name used a prefix for all outputs in the run\n\n\nSAMPLE1\n\n\n\n\n\n\nReference genome (\nrequired\n)\n\n\nSupported reference genome from one of hg19, GRCh38, mm9, mm10, dm3\n\n\nGRCh38\n\n\n\n\n\n\nOutput bigWig\n\n\nWhether or not to include a bigwig file in the output\n\n\nTrue\n\n\n\n\n\n\nRemove blacklist peaks\n\n\nWhether or not to remove known problem areas\n\n\nTrue\n\n\n\n\n\n\nFragment length\n\n\nHardcoded fragment length of your reads. 'NA' for auto-detect.\n\n\nNA\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nPlease be aware of the following stumbling points when setting parameters:\n\n\n\n\nDo not use spaces anywhere in your input file names, your output\n  prefix, or any of the other parameters. This is generally bad\n  practice and doesn't play well with the pipeline (consider using\n  \"_\" instead).\n\n\nDo not change the output directory when you run the pipeline. At\n  the top of parameter input page, there is a text box that allows\n  you to change the output folder. \nPlease ignore that setting\n. You\n  only need to specify an output prefix as described above. All of\n  the results will be put under \n/\nResults\n/[\nOUTPUT_PREFIX\n]\n.\n\n\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files you uploaded in \n\nthe upload data section\n. You can do this by \nclicking on the \nChIP Reads\n and \nControl Reads\n slots and\nselecting the respective files. If you are not doing a case/control\nrun, you only need to hook up the case sample.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that:\n\n\n\n\nall of the inputs are correctly hooked up (see \nhooking up inputs\n), and \n\n\nall of the required parameters are set (see \nsetting parameters\n).\n\n\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more ChIP-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand the job logs can be accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can refer to the \nDNAnexus Monitoring Executions Documentation\n for advanced capabilities for monitoring jobs.\n\n\n\n\nInteractive visualizations\n\n\nToday, the ChIP-Seq pipeline does not produce an interactive visualization. We are\nworking on adding this! In the meantime, you can view the cross-correlation plot(s)\nas outlined in the sections below.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view of your cloud workspace. This is similar to the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files. To access ChIP-Seq results, you should click on the \n\nResults\n folder, then select the output folder name you gave in the \nselecting parameters\n part of the guide.\n\n\n\n\nInterpreting results\n\n\nFor the ChIP-Seq pipeline, every pipeline run outputs a \nREADME.doc\n file\nwhich contains the latest information on which results are included.\nYou can refer to that file for the most up to date information on raw outputs.\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to \n\ncontact us\n.\n\n\nQ: Should I choose narrow peak calling pipeline or broad peak calling pipeline?\n\n\nA. We built two workflows: one for narrow peak calling and another broad\npeak calling. If your target protein is a transcription factor, please\nuse narrow peak calling workflow. For histone marks H3K4me3, H3K4me2,\nH3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling\nworkflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and\nH3K9me1, you could try broad peak calling workflow. In some scenario,\nH3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad\nshape, you might need to look into each peak region and consult\nexperts.\n\n\nQ. What to do if your fragment size is less than 50 base pairs?\n\n\nA. We estimate fragment size from the data based on the cross correlation\nplot. Usually the fragment size is above 50bp. If the estimated\nfragment size lower than 50bp, the workflow will stop at the peak\ncalling stage (MACS2/SICER) after BWA mapping finishes. You can rerun\nthe analysis with a specified fragment length.",
            "title": "ChIP-Seq Peak Calling"
        },
        {
            "location": "/guides/tools/chipseq/#overview",
            "text": "Inputs     Name  Type  Description  Example      FastQ files ( required  if using FastQ inputs)  Input file  Gzipped FastQ files generated by experiment.  Sample_R1.fastq.gz and Sample_R2.fastq.gz     Outputs     Name  Format  Description      BED file  .bed  Peak calls    Binary file  .bb  Binary format for BED file    BigWig file  .bw  Shows read coverage    Metrics file  .txt  Shows mapping and duplication rate    Cross correlation plot  .pdf  Quality plot showing if the forward and reverse reads tend to be centered around binding sites.     Process   The reads of the FastQ file(s) are aligned to the specified reference genome.   The aligned reads are then post-processed based on best-practice QC techniques\n(removing multiple mapped reads, removing duplicated reads, etc).   Peaks are called by SICER (broad peak analysis) or MACS2 (narrow peak\nanalysis).   Qualified peaks will be output as BED (.bed) and big BED (.bb)\nfiles.   The coverage information will be output as a bigWig (.bw)\nfile.   A cross correlation plot and general metrics file are generated to help check\nthe quality of experiment.",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/chipseq/#getting-started",
            "text": "To get started, you need to navigate to the  ChIP-Seq tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/chipseq/#uploading-data",
            "text": "The ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an\nIP experiment as input. You can upload your input FastQ files by\nusing the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.   Tip  If you plan to upload data through the St. Jude Cloud Data Transfer application\n(recommended), you can click the \"Upload Data\" button in the left panel. If you\nhave not already downloaded the app, do so by clicking \"Download app\". Once you\nhave the app, you can click \"Open app\" to open the app with the tool's cloud \nworkspace already opened and ready to drag-and-drop files into it!  For more information, check out the  data transfer application  guide.",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/chipseq/#running-the-tool",
            "text": "Once you've uploaded data to your cloud workspace, \nclick \"Launch Tool\" on the  tool's landing page . \nA dropdown will present the different presets for running the ChIP-Seq workflow.\nYou'll need to decide  (1)  whether you'd like to run broad/narrow peak\ncalling and  (2)  whether you have a case sample and a control sample (preferred)\nor just a case sample. This will determine which preset you should\nclick in this dropdown. There are various other parameters that you can \nset, but they are covered in further sections of this guide.",
            "title": "Running the tool"
        },
        {
            "location": "/guides/tools/chipseq/#broad-vs-narrow-peak-calling",
            "text": "Choosing between broad and narrow peak calling depends on the experiment\ndesign. The following are good rules of thumb for choosing between the\ntwo configurations. If you are not sure which configuration to use,\nplease consult with an expert at your institution or  contact us .  Narrow Peak Calling  If your target protein is a transcription factor, you should probably\nchoose narrow peak calling. You can also try the narrow peak calling\nworkflows for the following histone marks:   H3K4me3  H3K4me2  H3K9-14ac  H3K27ac  H2A.Z   Broad Peak Calling  You should try the broad peak calling workflows for the following\nhistone marks:   H3K36me3  H3K79me2  H3K27me3  H3K9me3  H3K9me1   Special Cases  In some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between\nnarrow and broad shape, you might need to look into each peak region and\nconsult experts.   Warning  If your fragment size is less than 50 base pairs, please refer to the frequently asked questions .",
            "title": "Broad vs. narrow peak calling"
        },
        {
            "location": "/guides/tools/chipseq/#selecting-parameters",
            "text": "There are a number of other parameters that can be customized. To \nsee the options available, click the gear cog next to the \n\"Parameter Wrapper\" substep.   The following are the parameters that can be set, a short\ndescription of each parameter, and an example value. If you\nhave questions, please  contact us .     Parameter Name  Description  Example      Output prefix ( required )  A name used a prefix for all outputs in the run  SAMPLE1    Reference genome ( required )  Supported reference genome from one of hg19, GRCh38, mm9, mm10, dm3  GRCh38    Output bigWig  Whether or not to include a bigwig file in the output  True    Remove blacklist peaks  Whether or not to remove known problem areas  True    Fragment length  Hardcoded fragment length of your reads. 'NA' for auto-detect.  NA      Caution  Please be aware of the following stumbling points when setting parameters:   Do not use spaces anywhere in your input file names, your output\n  prefix, or any of the other parameters. This is generally bad\n  practice and doesn't play well with the pipeline (consider using\n  \"_\" instead).  Do not change the output directory when you run the pipeline. At\n  the top of parameter input page, there is a text box that allows\n  you to change the output folder.  Please ignore that setting . You\n  only need to specify an output prefix as described above. All of\n  the results will be put under  / Results /[ OUTPUT_PREFIX ] .",
            "title": "Selecting parameters"
        },
        {
            "location": "/guides/tools/chipseq/#hooking-up-inputs",
            "text": "Next, you'll need to hook up the FastQ files you uploaded in  the upload data section . You can do this by \nclicking on the  ChIP Reads  and  Control Reads  slots and\nselecting the respective files. If you are not doing a case/control\nrun, you only need to hook up the case sample.",
            "title": "Hooking up inputs"
        },
        {
            "location": "/guides/tools/chipseq/#starting-the-workflow",
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that:   all of the inputs are correctly hooked up (see  hooking up inputs ), and   all of the required parameters are set (see  setting parameters ).   If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.",
            "title": "Starting the workflow"
        },
        {
            "location": "/guides/tools/chipseq/#monitoring-run-progress",
            "text": "Once you have started one or more ChIP-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the ChIP-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand the job logs can be accessed by clicking around the sub-items.      Tip  Power users can refer to the  DNAnexus Monitoring Executions Documentation  for advanced capabilities for monitoring jobs.",
            "title": "Monitoring run progress"
        },
        {
            "location": "/guides/tools/chipseq/#interactive-visualizations",
            "text": "Today, the ChIP-Seq pipeline does not produce an interactive visualization. We are\nworking on adding this! In the meantime, you can view the cross-correlation plot(s)\nas outlined in the sections below.",
            "title": "Interactive visualizations"
        },
        {
            "location": "/guides/tools/chipseq/#finding-the-raw-results-files",
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view of your cloud workspace. This is similar to the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files. To access ChIP-Seq results, you should click on the  Results  folder, then select the output folder name you gave in the  selecting parameters  part of the guide.",
            "title": "Finding the raw results files"
        },
        {
            "location": "/guides/tools/chipseq/#interpreting-results",
            "text": "For the ChIP-Seq pipeline, every pipeline run outputs a  README.doc  file\nwhich contains the latest information on which results are included.\nYou can refer to that file for the most up to date information on raw outputs.",
            "title": "Interpreting results"
        },
        {
            "location": "/guides/tools/chipseq/#frequently-asked-questions",
            "text": "If you have any questions not covered here, feel free to  contact us .  Q: Should I choose narrow peak calling pipeline or broad peak calling pipeline?  A. We built two workflows: one for narrow peak calling and another broad\npeak calling. If your target protein is a transcription factor, please\nuse narrow peak calling workflow. For histone marks H3K4me3, H3K4me2,\nH3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling\nworkflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and\nH3K9me1, you could try broad peak calling workflow. In some scenario,\nH3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad\nshape, you might need to look into each peak region and consult\nexperts.  Q. What to do if your fragment size is less than 50 base pairs?  A. We estimate fragment size from the data based on the cross correlation\nplot. Usually the fragment size is above 50bp. If the estimated\nfragment size lower than 50bp, the workflow will stop at the peak\ncalling stage (MACS2/SICER) after BWA mapping finishes. You can rerun\nthe analysis with a specified fragment length.",
            "title": "Frequently asked questions"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/",
            "text": "Authors\n\n\nScott Newman, Clay McLeod, Yongjin Li\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nFusion genes are important for cancer diagnosis, subtype definition and\ntargeted therapy. RNASeq is useful for detecting fusion transcripts; however, computational methods face challenges to identify fusion transcripts\ndue to events such as internal tandem duplication (ITD), multiple genes, low\nexpression, or non-templated insertions. To address some of these challenges, St. Jude Cloud offers \"Rapid RNA-Seq\", an end-to-end\nclinically validated pipeline that detects gene fusions and ITDs from human RNA-Seq.\n\n\nOverview\n\n\nInputs\n\n\nThe input can be either of the two entries below, based on whether you want to start\nwith FastQ files or a BAM file.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPaired FastQ files\n\n\nGzipped FastQ files generated by human RNA-Seq\n\n\nSample_R1.fastq.gz and Sample_R2.fastq.gz\n\n\n\n\n\n\nBAM file\n\n\nAligned reads file from human RNA-Seq\n\n\nSample.bam\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nIf you provide a BAM file to the pipeline, it \nmust\n be aligned to GRCh37-lite.\nRunning a BAM aligned to any other reference genome is not supported. Maybe more\nimportantly, we do not check the genome build of the BAM, so errors in computation\nor the results can occur. If your BAM is \nnot\n aligned to this genome build, we \nrecommend converting the BAM back to FastQ files using \n\nPicard's SamToFastq\n\nfunctionality and using the FastQ version of the pipeline.\n\n\n\n\nOutputs\n\n\nThe Rapid RNA-Seq pipeline produces the following outputs:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPredicted gene fusions (.txt)\n\n\nFile containing putative gene fusions.\n\n\n\n\n\n\nCoverage file (.bw)\n\n\nbigWig file containing coverage information.\n\n\n\n\n\n\nSplice junction read counts (.txt)\n\n\nRead counts for the splice junction detected.\n\n\n\n\n\n\nInteractive fusion visualization\n\n\nFusion visualization produced by ProteinPaint.\n\n\n\n\n\n\nInteractive coverage visualization\n\n\nCoverage visualization produced by ProteinPaint.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nThe raw sequence data is aligned to GRCh37-lite using standard STAR\n   mapping.\n\n\nA coverage bigWig (.bw) file is produced to allow the user to assess\n   sample quality across the genome.\n\n\nTwo gene fusion detection algorithms are run in parallel.\n\n\nThe \nFuzzion\n (Rice et al. unpublished data) fusion detection\n  algorithm is run to provide high sensitivity for recurrent gene\n  fusions.\n\n\nThe \nRNAPEG\n (Edmonson et al. unpublished data) splice\n  junction read counting algorithm is run to quantify read counts\n  for splice junctions. These splice junction read counts are then\n  used by \nCicero\n (Li et al. unpublished data) to detect\n  putative gene fusions.\n\n\n\n\n\n\nCustom visualizations for putative gene fusions and genome coverage\n   are produced by ProteinPaint.\n\n\n\n\nMapping\n\n\nWe use the \nSTAR aligner\n \nto rapidly map reads to the GRCh37 human reference genome. This step generally \ntakes around one hour to complete assuming approximately 55-75 million paired reads\nare supplied.\n\n\nCoverage\n\n\nInternally developed scripts calculate the coverage of mapped reads\ngenome wide. The resulting bigWig file can be viewed in ProteinPaint or\nused for quality control.\n\n\nSplice junction read quantification\n\n\nWe use our RNAPEG software to quantify reads spanning known and novel\nsplice junctions. RNAPEG also corrects improper mappings at splice\njunction boundaries for more accurate definition of novel splice\njunctions. The resulting junctions file can be viewed along with the\ncoverage bigWig file to gain insights into gene expression and splicing\npatterns\n\n\nGenome-wide fusion prediction\n\n\nWe developed an assembly-based algorithm CICERO (Clipped-reads Extended\nfor RNA Optimization) that is able to extend the read-length spanning\nfusion junctions for detecting complex fusions. CICERO finds clipped\nreads and junction spanning reads, assembles them into a contig and maps\nthe contig back to the reference genome. Mapped contigs are then\nannotated and filtered. Those with potential genic effects including\ngene fusion, ITD, readthrough or circular RNA are reported in the\n\nfinal_fusions.txt\n file. An interactive version of this file with\npredictions sorted by quality can be inspected with the ProteinPaint\ninteractive fusion viewer.\n\n\nAn abstract describing CICERO was presented at ASHG, 2014:\n\nhttp://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm\n\n\nLow stringency fusion gene \"Hotpot\" search\n\n\nWe have observed that certain fusions such as KIAA1549-BRAF in low-grade\nglioma have apparently limited read support in the bam file \u2014 either due\nto low expression or low tumor purity. In these cases, we use a\nsecondary tool, FUZZION, that performs fuzzy matching for known fusion\ngene junctions for every read in the bam file (both mapped and\nunmapped). FUZZION can recover even a single low quality read\npotentially supporting a known fusion gene junction. The FUZZION output\nis a simple text file with read IDs and sequences supporting a\nparticular gene fusion. The fusion point is indicated with square\nbrackets \n[]\n.\n\n\nGetting started\n\n\n\n\nCaution\n\n\nThis pipeline assumes GRCh37-lite coordinates. If your BAM is \n\nnot\n aligned to this genome build, we recommend converting the BAM \nback to FastQ files using \nPicard's SamToFastq\n\nfunctionality.\n\n\n\n\nTo get started, you need to navigate to the \nRapid RNA-Seq tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe Rapid RNA-Seq pipeline takes either a paired set of Gzipped FastQ files or \na GRCh37-lite aligned BAM from human RNA-Seq. You can upload your input file(s)\nusing the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\nRunning the tool\n\n\nOnce you've uploaded data to your cloud workspace, click \"Launch Tool\" on the \ntool's landing page\n. A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files you uploaded in \n\nthe upload data section\n. In this example,\nwe are using the FastQ version of the pipeline, so you can \nhook up the inputs by clicking on the \nFastq/R1\n and \nFastq/R2\n\nslots and selecting the respective files. If you are using \nthe BAM-based workflow, the process is similar.\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.\n\n\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nMonitoring run progress\n\n\nOnce you have started one or more Rapid RNA-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.\n\n\n \n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the Rapid RNA-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n \n\n\n\n\nTip\n\n\nPower users can refer to the \nDNAnexus Monitoring Executions Documentation\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with Excel spreadsheets or tab delimited\nfiles. This is the primary way we recommend you work with your Rapid RNA-Seq results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n \n\n\nNavigating Results\n\n\nRaw result files\n\n\nNavigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is\n\nhere\n). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"View Results Files\".\n\n\n\n\nYou should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.\n\n\n\n\nCustom visualization results\n\n\nNavigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is\n\nhere\n). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"Visualize Results\".\n\n\n\n\nYou should now see a list of visualization files like the ones shown below.\n\n\n\n\nInterpreting results\n\n\nThe complete output file specification is listed in the \noverview section\n\nof this guide. Here, we will discuss each of the different output files in more detail.\n\n\n\n\nPredicted gene fusions\n: The putative gene fusions will be\n  contained in the file \n[\nSAMPLE\n]\n.\nfinal_fusions\n.\ntxt\n. This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsample\n\n\nSample name\n\n\n\n\n\n\ngene*\n\n\nGene name\n\n\n\n\n\n\nchr*\n\n\nChromosome name\n\n\n\n\n\n\npos*\n\n\nGenomic Location\n\n\n\n\n\n\nort*\n\n\nStrand\n\n\n\n\n\n\nreads*\n\n\nSupporting reads\n\n\n\n\n\n\nmedal\n\n\nEstimated pathogenicity assessment using St. Jude Medal Ceremony\n\n\n\n\n\n\n\n\n\n\nCoverage file\n: A standard\n    \nbigWig\n file\n    used to describe genomic read coverage.\n\n\nSplice junction read counts\n: A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njunction\n\n\nSplice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in \n.bed\n output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.\n\n\n\n\n\n\ncount\n\n\nRaw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.\n\n\n\n\n\n\ntype\n\n\nEither \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).\n\n\n\n\n\n\ngenes\n\n\nGene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.\n\n\n\n\n\n\ntranscripts\n\n\nList of known transcript IDs matching the junction.\n\n\n\n\n\n\nqc_flanking\n\n\nCount of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).\n\n\n\n\n\n\nqc_plus\n\n\nCount of supporting reads aligned to the + strand.\n\n\n\n\n\n\nqc_minus\n\n\nCount of supporting reads aligned to the - strand.\n\n\n\n\n\n\nqc_perfect_reads\n\n\nCount of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).\n\n\n\n\n\n\nqc_clean_reads\n\n\nCount of supporting reads whose alignments are not perfect but which have a ratio of <= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.\n\n\n\n\n\n\n\n\nKnown issues\n\n\n\n\nAdapter contamination\n\n\nThis pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.\n\n\n\n\n\n\nHigh coverage regions\n\n\nCertain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.\n\n\n\n\n\n\nInteractive Visualizations Exon vs Intron Nomenclature\n\n\nWhen a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.\n\n\n\n\nFrequently asked questions\n\n\nIf you have any questions not covered here, feel free to reach\nout on \nour contact form\n.\n\n\nSubmit batch jobs on command line\n\n\nSee \nHow can I run an analysis workflow on multiple sample files at the same time?",
            "title": "Rapid RNA-Seq Fusion Detection"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#overview",
            "text": "Inputs  The input can be either of the two entries below, based on whether you want to start\nwith FastQ files or a BAM file.     Name  Description  Example      Paired FastQ files  Gzipped FastQ files generated by human RNA-Seq  Sample_R1.fastq.gz and Sample_R2.fastq.gz    BAM file  Aligned reads file from human RNA-Seq  Sample.bam      Caution  If you provide a BAM file to the pipeline, it  must  be aligned to GRCh37-lite.\nRunning a BAM aligned to any other reference genome is not supported. Maybe more\nimportantly, we do not check the genome build of the BAM, so errors in computation\nor the results can occur. If your BAM is  not  aligned to this genome build, we \nrecommend converting the BAM back to FastQ files using  Picard's SamToFastq \nfunctionality and using the FastQ version of the pipeline.   Outputs  The Rapid RNA-Seq pipeline produces the following outputs:     Name  Description      Predicted gene fusions (.txt)  File containing putative gene fusions.    Coverage file (.bw)  bigWig file containing coverage information.    Splice junction read counts (.txt)  Read counts for the splice junction detected.    Interactive fusion visualization  Fusion visualization produced by ProteinPaint.    Interactive coverage visualization  Coverage visualization produced by ProteinPaint.     Process   The raw sequence data is aligned to GRCh37-lite using standard STAR\n   mapping.  A coverage bigWig (.bw) file is produced to allow the user to assess\n   sample quality across the genome.  Two gene fusion detection algorithms are run in parallel.  The  Fuzzion  (Rice et al. unpublished data) fusion detection\n  algorithm is run to provide high sensitivity for recurrent gene\n  fusions.  The  RNAPEG  (Edmonson et al. unpublished data) splice\n  junction read counting algorithm is run to quantify read counts\n  for splice junctions. These splice junction read counts are then\n  used by  Cicero  (Li et al. unpublished data) to detect\n  putative gene fusions.    Custom visualizations for putative gene fusions and genome coverage\n   are produced by ProteinPaint.   Mapping  We use the  STAR aligner  \nto rapidly map reads to the GRCh37 human reference genome. This step generally \ntakes around one hour to complete assuming approximately 55-75 million paired reads\nare supplied.  Coverage  Internally developed scripts calculate the coverage of mapped reads\ngenome wide. The resulting bigWig file can be viewed in ProteinPaint or\nused for quality control.  Splice junction read quantification  We use our RNAPEG software to quantify reads spanning known and novel\nsplice junctions. RNAPEG also corrects improper mappings at splice\njunction boundaries for more accurate definition of novel splice\njunctions. The resulting junctions file can be viewed along with the\ncoverage bigWig file to gain insights into gene expression and splicing\npatterns  Genome-wide fusion prediction  We developed an assembly-based algorithm CICERO (Clipped-reads Extended\nfor RNA Optimization) that is able to extend the read-length spanning\nfusion junctions for detecting complex fusions. CICERO finds clipped\nreads and junction spanning reads, assembles them into a contig and maps\nthe contig back to the reference genome. Mapped contigs are then\nannotated and filtered. Those with potential genic effects including\ngene fusion, ITD, readthrough or circular RNA are reported in the final_fusions.txt  file. An interactive version of this file with\npredictions sorted by quality can be inspected with the ProteinPaint\ninteractive fusion viewer.  An abstract describing CICERO was presented at ASHG, 2014: http://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm  Low stringency fusion gene \"Hotpot\" search  We have observed that certain fusions such as KIAA1549-BRAF in low-grade\nglioma have apparently limited read support in the bam file \u2014 either due\nto low expression or low tumor purity. In these cases, we use a\nsecondary tool, FUZZION, that performs fuzzy matching for known fusion\ngene junctions for every read in the bam file (both mapped and\nunmapped). FUZZION can recover even a single low quality read\npotentially supporting a known fusion gene junction. The FUZZION output\nis a simple text file with read IDs and sequences supporting a\nparticular gene fusion. The fusion point is indicated with square\nbrackets  [] .",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#getting-started",
            "text": "Caution  This pipeline assumes GRCh37-lite coordinates. If your BAM is  not  aligned to this genome build, we recommend converting the BAM \nback to FastQ files using  Picard's SamToFastq \nfunctionality.   To get started, you need to navigate to the  Rapid RNA-Seq tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#uploading-data",
            "text": "The Rapid RNA-Seq pipeline takes either a paired set of Gzipped FastQ files or \na GRCh37-lite aligned BAM from human RNA-Seq. You can upload your input file(s)\nusing the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#running-the-tool",
            "text": "Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the  tool's landing page . A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with FastQ files or a BAM file.",
            "title": "Running the tool"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#hooking-up-inputs",
            "text": "Next, you'll need to hook up the FastQ files you uploaded in  the upload data section . In this example,\nwe are using the FastQ version of the pipeline, so you can \nhook up the inputs by clicking on the  Fastq/R1  and  Fastq/R2 \nslots and selecting the respective files. If you are using \nthe BAM-based workflow, the process is similar.",
            "title": "Hooking up inputs"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#starting-the-workflow",
            "text": "Once your input files are hooked up, you should be able to start the workflow\nby clicking the \"Run as Analysis...\" button in the top right hand corner\nof the workflow dialog.    Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.",
            "title": "Starting the workflow"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#monitoring-run-progress",
            "text": "Once you have started one or more Rapid RNA-Seq runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kicked off\ngets one row in this table.     You can click the \"+\" on any of the runs to check \nthe status of individual steps of the Rapid RNA-Seq pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.      Tip  Power users can refer to the  DNAnexus Monitoring Executions Documentation  for advanced capabilities for monitoring jobs.",
            "title": "Monitoring run progress"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#analysis-of-results",
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with Excel spreadsheets or tab delimited\nfiles. This is the primary way we recommend you work with your Rapid RNA-Seq results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.",
            "title": "Analysis of results"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#finding-the-raw-results-files",
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.",
            "title": "Finding the raw results files"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#navigating-results",
            "text": "",
            "title": "Navigating Results"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#raw-result-files",
            "text": "Navigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is here ). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"View Results Files\".   You should now be in the tool's workspace with access to files that you\nuploaded and results files that are generated. How/where the result\nfiles are generated are specific to each pipeline. Please refer to your\nindividual pipeline's documentation on where the output files are kept.",
            "title": "Raw result files"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#custom-visualization-results",
            "text": "Navigate to your tool's description page (for instance, Rapid RNA-Seq's\ndescription page is here ). You should\nsee a screen similar to the one in the screenshot below. In the left\nhand pane, select \"Visualize Results\".   You should now see a list of visualization files like the ones shown below.",
            "title": "Custom visualization results"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#interpreting-results",
            "text": "The complete output file specification is listed in the  overview section \nof this guide. Here, we will discuss each of the different output files in more detail.   Predicted gene fusions : The putative gene fusions will be\n  contained in the file  [ SAMPLE ] . final_fusions . txt . This file is a\n  tab-delimited file containing many fields for each of the predicted\n  SV. The most important columns are the following.      Field Name  Description      sample  Sample name    gene*  Gene name    chr*  Chromosome name    pos*  Genomic Location    ort*  Strand    reads*  Supporting reads    medal  Estimated pathogenicity assessment using St. Jude Medal Ceremony      Coverage file : A standard\n     bigWig  file\n    used to describe genomic read coverage.  Splice junction read counts : A custom file format describing the\n    junction read counts. The following fields are included in the\n    tab-delimited output file.      Field Name  Description      junction  Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in  .bed  output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future.    count  Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file.    type  Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection).    genes  Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table.    transcripts  List of known transcript IDs matching the junction.    qc_flanking  Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt).    qc_plus  Count of supporting reads aligned to the + strand.    qc_minus  Count of supporting reads aligned to the - strand.    qc_perfect_reads  Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips).    qc_clean_reads  Count of supporting reads whose alignments are not perfect but which have a ratio of <= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.",
            "title": "Interpreting results"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#known-issues",
            "text": "Adapter contamination  This pipeline does not, at present, remove adapter sequences. If the\nsequencing library is contaminated with adapters, CICERO runtimes can\nincrease exponentially. We recommend running FastQ files through a QC\npipeline such as FastQC and trimming adapters with tools such as\nTrimmomatic if adapters are found.    High coverage regions  Certain cell types show very high transcription of certain loci, for\nexample, the immunoglobulin heavy chain locus in plasma cells. The\npresence of very highly covered regions (typically 100,000-1,000,000+ X)\nhas an adverse effect on CICERO runtimes. Presently, we have no good\nsolution to this problem as strategies such as down-sampling may reduce\nsensitivity over important regions of the genome.    Interactive Visualizations Exon vs Intron Nomenclature  When a codon is split over a fusion gene junction, the annotation\nsoftware marks the event as intronic when really, the event should be\nexonic. We are working to fix this bug. In the mean time, if a fusion is\npredicted to be in frame but the interactive plot shows \"intronic\", we\nsuggest the user blat the contig shown just below to clarify if the true\njunction is either in the intron or exon.",
            "title": "Known issues"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#frequently-asked-questions",
            "text": "If you have any questions not covered here, feel free to reach\nout on  our contact form .",
            "title": "Frequently asked questions"
        },
        {
            "location": "/guides/tools/rapid-rnaseq/#submit-batch-jobs-on-command-line",
            "text": "See  How can I run an analysis workflow on multiple sample files at the same time?",
            "title": "Submit batch jobs on command line"
        },
        {
            "location": "/guides/tools/warden/",
            "text": "Authors\n\n\nLance Palmer\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nThe WARDEN (\nW\norkflow for the \nA\nnalysis of \nR\nNA-Seq \nD\nifferential \nE\nxpressio\nN\n)\nsoftware uses RNA-Seq sequence files to perform alignment, coverage\nanalysis, gene counts and differential expression analysis.\n\n\nOverview\n\n\nInputs\n\n\nThe WARDEN workflow requires two types of input files and that two parameters be set manually. All other parameters are preset with reasonable defaults.\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFastQ files (\nrequired\n)\n\n\nInput file(s)\n\n\nGzipped FastQ files generated by experiment\n\n\nSample1.fastq.gz, Sample2.fastq.gz\n\n\n\n\n\n\nSample sheet (\nrequired\n)\n\n\nInput file\n\n\nSample sheet generated and uploaded by the user\n\n\n*.txt\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nFastQC Report\n\n\nQuality control analysis by FastQC.\n\n\n\n\n\n\nAligned BAM\n\n\nAligned BAM files from STAR mapping.\n\n\n\n\n\n\nSplice junctions\n\n\nSplice junction information from STAR mapping.\n\n\n\n\n\n\nCoverage files\n\n\nbigWig (\n.bw\n) and BED (\n.bed\n) files detailing coverage.\n\n\n\n\n\n\nGene counts\n\n\nGene counts generated by HT-Seq count.\n\n\n\n\n\n\nVOOM/LIMMA results\n\n\nPairwise comparisons of expression data. Requires at least 3 samples vs 3 samples.\n\n\n\n\n\n\nSimple DE analysis\n\n\nNo statistical analysis, requires only a 1 samples vs 1 sample comparison.\n\n\n\n\n\n\nMA/Volcano plots\n\n\nBoth of the above produce tabular outputs, MA plots and volcano plots.\n\n\n\n\n\n\n\n\nProcess\n\n\n\n\nFastQ files generated by RNA-Seq are mapped to a reference genome using the STAR.\n\n\nHT-Seq count is used to assign mapped reads to genes. \n\n\nDifferential expression analysis is performed using VOOM normalization of counts and\nLIMMA analysis. \n\n\nCoverage plots of mapped reads are generated as interactive visualizations.\n\n\n\n\nGetting started\n\n\nTo get started, you need to navigate to the \nWARDEN tool page\n. You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.\n\n\n\n\n\n\nNote\n\n\nIf you can't see the \"Start\" button, one of these two scenarios is likely the case:\n\n\n\n\nYou see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.\n\n\nIf you cannot see \nany\n buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.\n\n\n\n\nIf neither of these are the case and you still can't click \"Start\",\n\ncontact us\n.\n\n\n\n\nUploading data\n\n\nThe WARDEN Differential Expression analysis pipeline takes Gzipped FastQ\nfiles generated by an RNA-Seq experiment as input. You can upload your input \nfile(s) using the \ndata transfer application\n\nor by uploading them through \nthe command line\n.\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.\n\n\nSample sheet\n\n\nOnce your data is uploaded, you'll need to create a sample sheet which\ndescribes the relationship between case and control samples,\nphenotype/condition information, and the comparisons you would like to\nperform. The sample sheet is a tab-delimited text document that can be\ncreated in Microsoft Excel (recommended) or a text editor.\n\n\n\n\nNote\n\n\nYou will need to upload your sample sheet in a similar manner as your\nFastQ files, so you can follow the \nsame uploading instructions\n \nto achieve this.\n\n\n\n\nPrepare using Microsoft Excel\n\n\n\n\nTip\n\n\nDownload the \nfile_download\n sample excel spreadsheet\n as a starting\npoint!\n\n\n\n\nThe final product for the excel spreadsheet will look like the\nscreenshot below. If you create the sample sheet from scratch, please\nensure the the columns are \nexactly\n in this order.\n\n\n\n\n\n\n\n\nSample rows\n\n\nEach row in the spreadsheet (except for the last row, which we will talk \nabout in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below:\n\n\n\n\nGuidelines\n\n\n\n\nThe sample name should be unique and should only contain letters,\nnumbers and underscores.\n\n\nThe condition/phenotype column associates similar samples together.\nThe values should contain only letters, numbers and underscores.\n\n\nReadFile1 should contain forward reads (e.g. \n*.R1.fastq.gz\n or \n*_1.fastq.gz\n).\n\n\nReadFile2 will contain reads in reverse orientation to ReadFile2\n(e.g. \n*.R2.fastq.gz\n or \n*_2.fastq.gz\n).\n\n\nFor single end reads a single dash ('-') should be entered in the ReadFile2 column.\n\n\n\n\n\n\nComparison row\n\n\nThe last line in the sample sheet is called the \"comparison row\". This\nline specifies the comparisons to be done between conditions/phenotypes.\nAll pairwise combinations of the values in the \"Phenotype\" column can be \nanalyzed. To specify the comparisons, on a separate line, include \n#comparisons=\n followed be a comma delimited list of two conditions separated by a dash. \n\n\n\n\nExample\n\n\nThe following lines are all valid examples.\n\n\n\n\n#comparisons=KO-WT\n\n\n#comparisons=Condition1-Control,Condition2-Control\n\n\n#comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1\n\n\n\n\n\n\n\n\nNote\n\n\nIf a comparison has at least 3 samples for each condition/phenotype,\nVOOM/LIMMA will be run. A simple differential comparison will be run on\nall samples.\n\n\n\n\nFinalizing the sample sheet\n\n\nTo finalize the sample sheet, save the Microsoft Excel file with\nwhatever name you like. Save the file as an Excel Workbook with the\n.xlsx extension.\n\n\nPrepare using a text editor\n\n\n\n\nTip\n\n\nDownload the \nfile_download\n sample text file\n as a starting\npoint!\n\n\n\n\nCreating a sample sheet with a text editor is an option for advanced\nusers. The process of creating a sample sheet with a text editor is the same as\ncreating one with Microsoft Excel, with the small difference that you\nmust manually create your columns using the tab character. Save the file\nwith a .txt extension.\n\n\nRunning the tool\n\n\n\n\nNote\n\n\nThe WARDEN tool operation is slightly different than the other pipelines\nbecause it accepts a variable number of samples. \nFirst\n, you will run\na \"bootstrapping\" step that creates a custom executable for your\nanalysis. \nSecond\n, you will need to manually execute the generated\nworkflow from the first step. This allows us to take advantage of many\nnice features, like check-pointing and cost reduction. Don't worry, \nwe'll show you how to do this step by step below.\n\n\n\n\nOnce you've uploaded data to your cloud workspace, click \"Launch Tool\" on the \ntool's landing page\n. You will be redirected to the virtual\ncloud workspace with the workflow screen opened for you.\n\n\n\n\nHooking up inputs\n\n\nNext, you'll need to hook up the FastQ files and sample sheet\n you uploaded in \nthe upload data section\n. \nClick the \nFASTQ_FILES\n input field and select \nall\n FastQ files.\nNext, click the \nsampleList\n input field and select the corresponding\nsamplesheet.\n\n\n\n\nSelecting parameters\n\n\nWe now need to configure the parameters for the pipeline, such as reference\ngenome and sequencing method. You can access all of the available parameters \nby clicking on the \nWARDEN WORKFLOW GENERATOR\n substep.\n\n\n\n\nParameter setup steps\n\n\n\n\nIn the \nOutput Folder\n field, select a folder to output to. You can\nstructure your experiments however you like (e.g. \n/My_Outputs\n)\n\n\nIn the \nanalysisName\n field, enter a prefix for all of the output files. This\ncan be any value you want to use to remember this run. \nBe sure to use underscores\ninstead of spaces here!\n\n\nSelect the \nsequenceStandedness\n from the drop down menu. \nThis information can be determined from the sequencing or source \nof the data. If you don't know what to put here, select \"no\".\n\n\nSelect the \nGenome\n pulldown menu. Choose the appropriate box.\n\n\nThe LIMMA parameters can be left alone for most analyses. If you are\nan advanced LIMMA user, you can change the various settings exposed\nbelow the required parameters.\n\n\nWhen all parameters have been set, press the save button.\n\n\n\n\n\n\n\n\nStarting the workflow\n\n\nOnce your input files are hooked up and your parameters are set, \nyou should be able to start the workflow by clicking the \"Run as Analysis...\" \nbutton in the top right hand corner of the workflow dialog.\n\n\n\n\nTip\n\n\nIf you cannot click this button, please ensure that all of the inputs are correctly hooked up (see \nhooking up inputs\n).\n\n\nIf you're still have trouble, please \ncontact us\n and include\na screenshot of the workflow screen above.\n\n\n\n\nThe tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\".\n\n\n\n\nWhen the custom workflow has finished generating, the word 'Done' will\nappear in green in the status column. This indicates that the\nbootstrapping step has completed successfully. \n\n\n\n\nCustom Workflow Process\n\n\n\n\nWait for the workflow generator to finish.\n\n\n\n\nClick on the WARDEN name in the name column.\n\n\n\n\n\n\nYou will now be on a page specific to the running of the workflow.\n    On the left side, you will see the inputs you selected for the\n    workflow generator. On the right side are the output files\n    (including the generated workflow). Select the generated workflow as\n    shown in the picture below.\n\n\n\n\n\n\n\n\nYou will now be within the output folder you specified earlier.\n    Select the file that begins with 'WARDEN WORKFLOW:'\n\n\n\n\n\n\n\n\nA workflow generated for your data will be presented to you. Select\n    'Run as analysis' in the upper right.\n\n\n\n\n\n\n\n\nThe workflow will initiate, and you will be brought to the 'Monitor'\n    page. (Note to get back to this page, you can select 'Monitor' on\n    one of the menu bars near the top ) Expand the the workflow progress\n    be selecting the '+' sign next to 'In Progress'\n\n\n\n\n\n\n\n\nAs parts of the pipeline are run, you will see different tasks in\n    different colors. Green means done, blue is running, orange is\n    waiting, and red means error.\n\n\n\n\n\n\n\n\nWhen done the status will be shown as 'Done'. Select the Workflow\n    name under Status.\n\n\n\n\n\n\n\n\nYou will be brought to a page that show more information about the\n    workflow analysis. Click on the output folder to go to the output.\n\n\n\n\n\n\n\n\nThe output folders will now be shown.\n\n\n\n\n\n\n\n\nFor a description of the output, please refer to \nNavigating Results\n.\n\n\nMonitoring run progress\n\n\nOnce you have started one or more WARDEN runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the \ntool's landing page\n. \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kick off\ngets one row in the Monitor section.\n\n\nYou can click the \"+\" on any of the runs to check \nthe status of individual steps of the pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.\n\n\n\n\nTip\n\n\nPower users can refer to the \nDNAnexus Monitoring Executions Documentation\n for advanced capabilities for monitoring jobs.\n\n\n\n\nAnalysis of results\n\n\nEach tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your WARDEN results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.\n\n\nFinding the raw results files\n\n\nNavigate to the \ntool's landing page\n. \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.\n\n\n\n\nNavigating Results\n\n\nNavigating to the raw results of your runs is the same for all pipelines. Hence, we refer you to the \nNavigating Results\n section of the Rapid RNA-Seq pipeline documentation. You can follow along for WARDEN, and then return to this page for interpreting results.\n\n\nPrimary Results\n\n\nAlignment statistics\n\n\nSeveral files should be examined initially to determine the quality of\nthe results. \nalignmentStatistics.txt\n shows alignment statistics for\nall samples. This file is a plain text tab-delimited file that can be\nopened in Excel or a text editor such as Notepad++. This file contains\ninformation on the total reads per sample, the percentage of duplicate\nreads and the percentage of mapped reads. An example of this file is\nbelow. (Within the DNAnexus output directory structure, these files will\nbe in the COMBINED_FLAGSTAT directory.)\n\n\n\n\n\n\n\n\nMultidimensional scaling (MDS) Plot\n\n\nThe second set of files to look at are the Multidimensional scaling\n(MDS) plots (\nhttps://en.wikipedia.org/wiki/Multidimensional_scaling\n)\nusing the plotMDS function within LIMMA. Similar to PCA, these graphs\nwill show how similar samples are to each other. There are different\nsets of MDS plots. For comparisons where there are 3 or more samples per\ncondition, an MDS plot using Voom (Limma) normalized values are\ngenerated. An example can be seen below. These files will be labeled\n\nmdsPlot.png\n. For all comparisons, regardless of sample size, and MDS\nplot will also be generated with Counts per million (CPM) normalized\ngene counts. These files will be labeled \nmdsPlot.normCPM.png\n.\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)\n\n\n\n\nMDS plot from just CPM normalized data.\n\n\n\n\nProteinPaint Visualizations\n\n\nSeveral files on DNAnexus allow the data to be viewed in the Protein\nPaint viewer. (Note: We plan to have links downloaded in the future to\nallow the viewing of these files off of DNAnexus.)\n\n\nLIMMA differential expression viewer\n\n\nWithin LIMMA/VIEWERS directory (note if no comparisons meet the 3 sample\ncondition, the LIMMA folder will not exist), there will be a viewer file\nfor each valid comparison ( *\nresults.\n.txt.viewer**). Simply select\nthe file and press 'Launch viewer' in the lower right. A viewer will pop\nup showing both the MA Plot and Volcano plot. By moving the mouse over a\ncircle, the circle will highlight and the corresponding gene on the other\ngraph will also highlight. Additional information about the gene and its\nexpression values will also be shown. One can also type in multiple gene\nsymbols in the provided text box. By pressing 'Show gene labels' all\nthese genes will show up on the plots.\n\n\n\n\nSimple differential expression viewer\n\n\nThere will also be a viewer for the simple differential expression\nanalysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all\nbeen set to 1, so the volcano plot will not be relevant.\n\n\nbigWig viewer\n\n\nIn the BIGWIG_VIEWER directory there will be a bigwigViewer file.\nSelect this file and then 'Launch viewer'. A graph of coverage for the\ngenome should be visible.\n\n\nSecondary Results\n\n\n\n\nInteractive MA/Volcano Plots\n\n\nIn addition to viewing the MA and volcano plots through the visualization tool\n\n\n\n\nDifferential expression results\n \n\n\nOther useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition, \nresults.*.txt\n will be produced.\n\n\n\n\nGSEA.input.txt\n and \nGSEA.tStat.txt\n \n\n\nInput files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram.\n- (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)\n\n\n\n\n\n\nFor plain text results from the simple differential expression analysis, the files will be named \nsimpleDE.*.txt\n. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)\n\n\n\n\n\n\nPrelabelled MA and volcano plots are provided for the analysis. These files are labeled \nmaPlot.*.png\n and \nvolcanoPlot.*.png\n where \n*\n is the comparison (e.g. ko_vs_wt)\n\n\n\n\n\n\nThe MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An example MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)\n\n\n\n\n\n\n\n\nThe volcano plot shows the Log2Fold change between the conditions on the\nX-axis, and the -Log10 of the multiple testing corrected P-value on the\nY-axis.\n\n\n\n\nAn MA plot is generated for all comparisons regardless of number of\nsamples. This is the \nsimpleDEPlot.*.png\n no statistics are shown\nand genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)\n\n\n\n\nDifferential analysis input\n\n\nInputs and commands are provided for rerunning differential expression\nanalysis on ones own computer. The R commands used for the analysis are\nfound in \nvoomLimma.R\n. An experienced R user can rerun the analysis\nwith any desired changes. This analysis requires the input\n\ncountFile.txt\n which contains counts per genes, the\n\nRparameters.txt\n file containing input parameters, and a processed\nsample list file \nsampleList.txt\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)\n\n\nThe input for the simple differential analysis expression will be\n\nRparameters_simple.txt\n, \nsimpleDE.R\n, \ncountFile.txt\n and\n\nsampleList.txt\n. \ncountFile.txt\n and \nsampleList.txt\n are the\nsame files used by the LIMMA analysis.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe SIMPLE_DIFEX directory.)\n\n\n\n\nCoverage results\n\n\nbigWig files will be generated for use in genome browsers (such as IGV\n\nhttp://software.broadinstitute.org/software/igv/\n). For each sample,\nmultiple bigWig files will be found. For all types of sequencing\nstrandedness, there will be bigWig files labeled,\n\n*.sortedCoverageFile.bed.bw\n where '\n' is the sample name. For\nstranded data there will also be*.sortedPosCoverageFile.bed.bw\n* and\n\n*.sortedNegCoverageFile.bed.bw\n which contains coverage information\nfor the positive and negative strand of the genome.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe BIGWIG directory.)\n\n\n\n\nQuality Control Results (FastQC)\n\n\nWithin the FastQC directory, for each sample and read direction there\nwill be an html file and a zip file (\n*.FastQc.html\n\n\n*.FastQc.zip\n where '*' is the base FastQ name), containing results\nfrom FastQTC. For the average user the html file is sufficient. This\nfile can give some basic statistics on the quality of the data.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe FastQC directory.)\n\n\n\n\nBAM alignment files\n\n\nThere are two BAM files generated per sample that contain mapping\ninformation for all reads. The first is labeled\n\n*.Aligned.sortedByCoord.dup.bam\n where '\n' is the sample name. The\nBAM file is sorted by coordinates and has duplicates marked. The second\nfile is*.Aligned.toTranscriptome.out.bam\n* and contains reads mapped\nto transcripts.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)\n\n\n\n\nChimeric reads and junction files\n\n\nAdditional files created by STAR are provided. More information on these\nfiles can be found at\n\nhttp://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.posix/doc/STARmanual.pdf\n.\n\n*.SJ.out.tab\n contain splice junction information. Fusion detection\nfiles are labeled \n*.Chimeric.out.bam\n and\n\n*.Chimeric.out.junction\n.\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)\n\n\n\n\nFPKM and count files (per sample)\n\n\nPer sample files containing FPKM and raw count values for each gene can\nbe found in \n*.fpkm.txt\n and \n*.htseq_counts.txt\n where '*' is\nthe sample name. Within the DNAnexus output directory structure, these files will be in\nthe HTSEQ directory.\n\n\n\n\nMethods Files\n\n\nA more human readable explanation is found in \nmethods.docx\n. Detailed\ndocumentation can be found in \nmethods.txt\n\n\n(Within the DNAnexus output directory structure, these files will be in\nthe METHODS directory.)\n\n\nAuxiliary Files\n\n\nThis section describes the files that exist within the DNAnexus output\nfolder. Most of these files will not be of interest to the average user.\nHowever, interactive viewers are describe in \nLIMMA differential\nexpression viewer\n and \nSimple\ndifferential expression viewer\n.\n\n\nThe output will be divided into multiple folders. The results being the\nmost useful will be the differential expression analysis results in the\nLIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage\nwill be in the BIGWIG folder. Other folder contain different types of\ndata and are explained in further detail below.\n\n\n\n\n\n\n\n\nThe following description of files is sorted by their output directory.\n\n\n\n\nALIGN\n\n\nThis directory contains the BAM files described in \nBAM alignment\nfiles\n and the chimeric and junction files are\ndescribed in \nChimeric reads and junction\nfiles\n. In addition there are 2 log\nfiles. \n*Log.final.out\n has relevant statistics for the alignment.\nThe \n*.Log.out\n file just contains a log of the analysis run,\nincluding input parameters. Per sample FLAGSTAT results are found in\n\n*.flagStatOut.txt\n. Finally the ALIGN directory has\nmultiple \n.starAlign.methods.txt files. These files can be ignored as\nthey are summarized in the finalmethods.docx\n* and \nmethods.txt\n\nfiles described in \nMethods Files\n.\n\n\n\n\nBIGWIG\n\n\nAll of the files here are described in section \nCoverage\nresults\n. The \nbgToBw.methods.txt\n files can be\nignored as they are summarized in the files described in \nMethods\nFiles\n.\n\n\n\n\nBIGWIG_VIEWER\n\n\nSee \nbigWig viewer\n\n\n\n\nCOMBINED_FLAGSTAT\n\n\nThis directory contains the \nalignmentStatistics.txt\n file, which contains the combined alignment statistics from all samples.  It is generated from the flagstat files describe in the ALIGN directory.\n\n\n\n\nCOMBINED_HTSEQ\n\n\nUsed for input in differential expression analysis. The\ncombineCountFile.txt is the same as countFile.txt described in\n\nDifferential analysis input\n\n\n\n\nCOVERAGE\n\n\nBED graph files used to generate bigWig files are here.\n\n\n\n\nFastQC\n\n\nSee \nQuality Control Results (FastQC)\n\n\n\n\nHTSEQ\n\n\nPer-sample HTSEQ-count results (\n*.htseq_counts.txt\n) and FPKM\nresults (\n*.fpkm.txt\n). Temporary methods files are found as\n*.htseq-count.methods.txt\n\n\n\n\nLIMMA\n\n\nmdsPlot.png\n, \nmaPlot.\n.png,volcanoPlot.\n.png\n are described in\n\nInitial analysis of results\n\n\nresults.\n.txt,GSEA.input.\n.txt\n and *\nGSEA.tStat.\n.txt** are\ndescribe in \nDifferential expression\nresults\n\n\nvoomLimma.R\n, \ncountFile.txt\n, \nRparameters.txt\n, and\n\nsampleList.txt\n are described in \nDifferential analysis\ninput\n\n\nSee \nLIMMA differential expression\nviewer\n for a description of the\nVIEWERS directory.\n\n\nOther files in the LIMMA directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nlimmaMethods.txt is an intermediate file describing methods. Out.tar.gz\nis used for testing purposes. The sessionInfo.txt file describe the R\nsession working parameters and modules loaded. meanVariance.png is a\nplot for assessing quality of count data\n(\nhttps://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29\n)\n\n\n\n\nMETHODS\n\n\nThe files here are described in \nMethods Files\n.\n\n\n\n\nSAMPLELIST\n\n\nThese files are used internally by the pipeline.\n\n\n\n\nSIMPLE_DIFEX\n\n\nmdsPlot.normCPM.png\n and *\nsimpleDEPlot.\n.png** are described in\n\nInitial analysis of results\n\n\n*\nsimpleDE.\n.txt** are describe in \nDifferential expression\nresults\n\n\nsimpleDE.R\n, \ncountFile.txt\n, \nRparameters_simple.txt\n, and\n\nsampleList.txt\n are described in \nDifferential analysis\ninput\n\n\nSee \nSimple differential expression\nviewer\n for a description of the\nVIEWERS directory.\n\n\nOther files in the SIMPLE_DIFEX directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nsimpleDifEx.methods.txt is an intermediate file describing methods.\nOut.tar.gz is used for testing purposes. The sessionInfo.txt file\ndescribe the R session working parameters and modules loaded.\n\n\nFrequently Asked Questions\n\n\nNone yet! If you have any questions not covered here, feel free to reach\nout on \nour contact\nform\n.",
            "title": "WARDEN Differential Expression Analysis"
        },
        {
            "location": "/guides/tools/warden/#overview",
            "text": "Inputs  The WARDEN workflow requires two types of input files and that two parameters be set manually. All other parameters are preset with reasonable defaults.     Name  Type  Description  Example      FastQ files ( required )  Input file(s)  Gzipped FastQ files generated by experiment  Sample1.fastq.gz, Sample2.fastq.gz    Sample sheet ( required )  Input file  Sample sheet generated and uploaded by the user  *.txt     Outputs     Name  Description      FastQC Report  Quality control analysis by FastQC.    Aligned BAM  Aligned BAM files from STAR mapping.    Splice junctions  Splice junction information from STAR mapping.    Coverage files  bigWig ( .bw ) and BED ( .bed ) files detailing coverage.    Gene counts  Gene counts generated by HT-Seq count.    VOOM/LIMMA results  Pairwise comparisons of expression data. Requires at least 3 samples vs 3 samples.    Simple DE analysis  No statistical analysis, requires only a 1 samples vs 1 sample comparison.    MA/Volcano plots  Both of the above produce tabular outputs, MA plots and volcano plots.     Process   FastQ files generated by RNA-Seq are mapped to a reference genome using the STAR.  HT-Seq count is used to assign mapped reads to genes.   Differential expression analysis is performed using VOOM normalization of counts and\nLIMMA analysis.   Coverage plots of mapped reads are generated as interactive visualizations.",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/warden/#getting-started",
            "text": "To get started, you need to navigate to the  WARDEN tool page . You'll need to click\nthe \"Start\" button in the left hand pane. This creates a cloud workspace\nin DNAnexus with the same name as the tool. After this, you will be able \nto upload your input files to that workspace.    Note  If you can't see the \"Start\" button, one of these two scenarios is likely the case:   You see three buttons on the left sidebar instead of one. In this case,\n  you've already clicked the \"Start\" button previously, and a cloud workspace has\n  already been created for you. In this case, you're good! You can move\n  on to the next section.  If you cannot see  any  buttons on the left side, you probably have not\n  logged in yet. If you see a sentence that says \"Log in to launch this \n  tool\", simply login and try again.   If neither of these are the case and you still can't click \"Start\", contact us .",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/warden/#uploading-data",
            "text": "The WARDEN Differential Expression analysis pipeline takes Gzipped FastQ\nfiles generated by an RNA-Seq experiment as input. You can upload your input \nfile(s) using the  data transfer application \nor by uploading them through  the command line .\nBoth of the guides linked here will contain more details on how to upload\ndata using that method, so we defer to those guides here.",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/warden/#sample-sheet",
            "text": "Once your data is uploaded, you'll need to create a sample sheet which\ndescribes the relationship between case and control samples,\nphenotype/condition information, and the comparisons you would like to\nperform. The sample sheet is a tab-delimited text document that can be\ncreated in Microsoft Excel (recommended) or a text editor.   Note  You will need to upload your sample sheet in a similar manner as your\nFastQ files, so you can follow the  same uploading instructions  \nto achieve this.",
            "title": "Sample sheet"
        },
        {
            "location": "/guides/tools/warden/#prepare-using-microsoft-excel",
            "text": "Tip  Download the  file_download  sample excel spreadsheet  as a starting\npoint!   The final product for the excel spreadsheet will look like the\nscreenshot below. If you create the sample sheet from scratch, please\nensure the the columns are  exactly  in this order.     Sample rows  Each row in the spreadsheet (except for the last row, which we will talk \nabout in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below:   Guidelines   The sample name should be unique and should only contain letters,\nnumbers and underscores.  The condition/phenotype column associates similar samples together.\nThe values should contain only letters, numbers and underscores.  ReadFile1 should contain forward reads (e.g.  *.R1.fastq.gz  or  *_1.fastq.gz ).  ReadFile2 will contain reads in reverse orientation to ReadFile2\n(e.g.  *.R2.fastq.gz  or  *_2.fastq.gz ).  For single end reads a single dash ('-') should be entered in the ReadFile2 column.    Comparison row  The last line in the sample sheet is called the \"comparison row\". This\nline specifies the comparisons to be done between conditions/phenotypes.\nAll pairwise combinations of the values in the \"Phenotype\" column can be \nanalyzed. To specify the comparisons, on a separate line, include  #comparisons=  followed be a comma delimited list of two conditions separated by a dash.    Example  The following lines are all valid examples.   #comparisons=KO-WT  #comparisons=Condition1-Control,Condition2-Control  #comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1     Note  If a comparison has at least 3 samples for each condition/phenotype,\nVOOM/LIMMA will be run. A simple differential comparison will be run on\nall samples.   Finalizing the sample sheet  To finalize the sample sheet, save the Microsoft Excel file with\nwhatever name you like. Save the file as an Excel Workbook with the\n.xlsx extension.",
            "title": "Prepare using Microsoft Excel"
        },
        {
            "location": "/guides/tools/warden/#prepare-using-a-text-editor",
            "text": "Tip  Download the  file_download  sample text file  as a starting\npoint!   Creating a sample sheet with a text editor is an option for advanced\nusers. The process of creating a sample sheet with a text editor is the same as\ncreating one with Microsoft Excel, with the small difference that you\nmust manually create your columns using the tab character. Save the file\nwith a .txt extension.",
            "title": "Prepare using a text editor"
        },
        {
            "location": "/guides/tools/warden/#running-the-tool",
            "text": "Note  The WARDEN tool operation is slightly different than the other pipelines\nbecause it accepts a variable number of samples.  First , you will run\na \"bootstrapping\" step that creates a custom executable for your\nanalysis.  Second , you will need to manually execute the generated\nworkflow from the first step. This allows us to take advantage of many\nnice features, like check-pointing and cost reduction. Don't worry, \nwe'll show you how to do this step by step below.   Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the  tool's landing page . You will be redirected to the virtual\ncloud workspace with the workflow screen opened for you.",
            "title": "Running the tool"
        },
        {
            "location": "/guides/tools/warden/#hooking-up-inputs",
            "text": "Next, you'll need to hook up the FastQ files and sample sheet\n you uploaded in  the upload data section . \nClick the  FASTQ_FILES  input field and select  all  FastQ files.\nNext, click the  sampleList  input field and select the corresponding\nsamplesheet.",
            "title": "Hooking up inputs"
        },
        {
            "location": "/guides/tools/warden/#selecting-parameters",
            "text": "We now need to configure the parameters for the pipeline, such as reference\ngenome and sequencing method. You can access all of the available parameters \nby clicking on the  WARDEN WORKFLOW GENERATOR  substep.   Parameter setup steps   In the  Output Folder  field, select a folder to output to. You can\nstructure your experiments however you like (e.g.  /My_Outputs )  In the  analysisName  field, enter a prefix for all of the output files. This\ncan be any value you want to use to remember this run.  Be sure to use underscores\ninstead of spaces here!  Select the  sequenceStandedness  from the drop down menu. \nThis information can be determined from the sequencing or source \nof the data. If you don't know what to put here, select \"no\".  Select the  Genome  pulldown menu. Choose the appropriate box.  The LIMMA parameters can be left alone for most analyses. If you are\nan advanced LIMMA user, you can change the various settings exposed\nbelow the required parameters.  When all parameters have been set, press the save button.",
            "title": "Selecting parameters"
        },
        {
            "location": "/guides/tools/warden/#starting-the-workflow",
            "text": "Once your input files are hooked up and your parameters are set, \nyou should be able to start the workflow by clicking the \"Run as Analysis...\" \nbutton in the top right hand corner of the workflow dialog.   Tip  If you cannot click this button, please ensure that all of the inputs are correctly hooked up (see  hooking up inputs ).  If you're still have trouble, please  contact us  and include\na screenshot of the workflow screen above.   The tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\".   When the custom workflow has finished generating, the word 'Done' will\nappear in green in the status column. This indicates that the\nbootstrapping step has completed successfully.",
            "title": "Starting the workflow"
        },
        {
            "location": "/guides/tools/warden/#custom-workflow-process",
            "text": "Wait for the workflow generator to finish.   Click on the WARDEN name in the name column.    You will now be on a page specific to the running of the workflow.\n    On the left side, you will see the inputs you selected for the\n    workflow generator. On the right side are the output files\n    (including the generated workflow). Select the generated workflow as\n    shown in the picture below.     You will now be within the output folder you specified earlier.\n    Select the file that begins with 'WARDEN WORKFLOW:'     A workflow generated for your data will be presented to you. Select\n    'Run as analysis' in the upper right.     The workflow will initiate, and you will be brought to the 'Monitor'\n    page. (Note to get back to this page, you can select 'Monitor' on\n    one of the menu bars near the top ) Expand the the workflow progress\n    be selecting the '+' sign next to 'In Progress'     As parts of the pipeline are run, you will see different tasks in\n    different colors. Green means done, blue is running, orange is\n    waiting, and red means error.     When done the status will be shown as 'Done'. Select the Workflow\n    name under Status.     You will be brought to a page that show more information about the\n    workflow analysis. Click on the output folder to go to the output.     The output folders will now be shown.     For a description of the output, please refer to  Navigating Results .",
            "title": "Custom Workflow Process"
        },
        {
            "location": "/guides/tools/warden/#monitoring-run-progress",
            "text": "Once you have started one or more WARDEN runs, you can safely close your\nbrowser and come back later to check the status of the jobs. To do this,\nnavigate to the  tool's landing page . \nNext, click \"View Results\" then select the \"View Running Jobs\" option. \nYou will be redirected to the job monitoring page. Each job you kick off\ngets one row in the Monitor section.  You can click the \"+\" on any of the runs to check \nthe status of individual steps of the pipeline.\nOther information, such as time, cost of individual steps in the pipeline, \nand even viewing the job logs can accessed by clicking around the sub-items.   Tip  Power users can refer to the  DNAnexus Monitoring Executions Documentation  for advanced capabilities for monitoring jobs.",
            "title": "Monitoring run progress"
        },
        {
            "location": "/guides/tools/warden/#analysis-of-results",
            "text": "Each tool in St. Jude Cloud produces a visualization that makes understanding\nresults more accessible than working with excel spreadsheet or tab delimited\nfiles. This is the primary way we recommend you work with your WARDEN results. We also\ninclude the raw output files for you to dig into if the visualization is not \nsufficient to answer your research question.",
            "title": "Analysis of results"
        },
        {
            "location": "/guides/tools/warden/#finding-the-raw-results-files",
            "text": "Navigate to the  tool's landing page . \nIn the left hand pane, click \"View Results\" then \"View Results Files\". You will\nbe taken to the filesystem view your cloud workspace. This is similar to your the\nfilesystem on your computer, and you can do many common operations such as deleting,\nrenaming, and moving files.",
            "title": "Finding the raw results files"
        },
        {
            "location": "/guides/tools/warden/#navigating-results",
            "text": "Navigating to the raw results of your runs is the same for all pipelines. Hence, we refer you to the  Navigating Results  section of the Rapid RNA-Seq pipeline documentation. You can follow along for WARDEN, and then return to this page for interpreting results.",
            "title": "Navigating Results"
        },
        {
            "location": "/guides/tools/warden/#primary-results",
            "text": "",
            "title": "Primary Results"
        },
        {
            "location": "/guides/tools/warden/#alignment-statistics",
            "text": "Several files should be examined initially to determine the quality of\nthe results.  alignmentStatistics.txt  shows alignment statistics for\nall samples. This file is a plain text tab-delimited file that can be\nopened in Excel or a text editor such as Notepad++. This file contains\ninformation on the total reads per sample, the percentage of duplicate\nreads and the percentage of mapped reads. An example of this file is\nbelow. (Within the DNAnexus output directory structure, these files will\nbe in the COMBINED_FLAGSTAT directory.)",
            "title": "Alignment statistics"
        },
        {
            "location": "/guides/tools/warden/#multidimensional-scaling-mds-plot",
            "text": "The second set of files to look at are the Multidimensional scaling\n(MDS) plots ( https://en.wikipedia.org/wiki/Multidimensional_scaling )\nusing the plotMDS function within LIMMA. Similar to PCA, these graphs\nwill show how similar samples are to each other. There are different\nsets of MDS plots. For comparisons where there are 3 or more samples per\ncondition, an MDS plot using Voom (Limma) normalized values are\ngenerated. An example can be seen below. These files will be labeled mdsPlot.png . For all comparisons, regardless of sample size, and MDS\nplot will also be generated with Counts per million (CPM) normalized\ngene counts. These files will be labeled  mdsPlot.normCPM.png .\n(Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)   MDS plot from just CPM normalized data.",
            "title": "Multidimensional scaling (MDS) Plot"
        },
        {
            "location": "/guides/tools/warden/#proteinpaint-visualizations",
            "text": "Several files on DNAnexus allow the data to be viewed in the Protein\nPaint viewer. (Note: We plan to have links downloaded in the future to\nallow the viewing of these files off of DNAnexus.)  LIMMA differential expression viewer  Within LIMMA/VIEWERS directory (note if no comparisons meet the 3 sample\ncondition, the LIMMA folder will not exist), there will be a viewer file\nfor each valid comparison ( * results. .txt.viewer**). Simply select\nthe file and press 'Launch viewer' in the lower right. A viewer will pop\nup showing both the MA Plot and Volcano plot. By moving the mouse over a\ncircle, the circle will highlight and the corresponding gene on the other\ngraph will also highlight. Additional information about the gene and its\nexpression values will also be shown. One can also type in multiple gene\nsymbols in the provided text box. By pressing 'Show gene labels' all\nthese genes will show up on the plots.   Simple differential expression viewer  There will also be a viewer for the simple differential expression\nanalysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all\nbeen set to 1, so the volcano plot will not be relevant.  bigWig viewer  In the BIGWIG_VIEWER directory there will be a bigwigViewer file.\nSelect this file and then 'Launch viewer'. A graph of coverage for the\ngenome should be visible.",
            "title": "ProteinPaint Visualizations"
        },
        {
            "location": "/guides/tools/warden/#secondary-results",
            "text": "Interactive MA/Volcano Plots  In addition to viewing the MA and volcano plots through the visualization tool   Differential expression results    Other useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition,  results.*.txt  will be produced.   GSEA.input.txt  and  GSEA.tStat.txt    Input files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram.\n- (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)    For plain text results from the simple differential expression analysis, the files will be named  simpleDE.*.txt . (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)    Prelabelled MA and volcano plots are provided for the analysis. These files are labeled  maPlot.*.png  and  volcanoPlot.*.png  where  *  is the comparison (e.g. ko_vs_wt)    The MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An example MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.)     The volcano plot shows the Log2Fold change between the conditions on the\nX-axis, and the -Log10 of the multiple testing corrected P-value on the\nY-axis.   An MA plot is generated for all comparisons regardless of number of\nsamples. This is the  simpleDEPlot.*.png  no statistics are shown\nand genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)   Differential analysis input  Inputs and commands are provided for rerunning differential expression\nanalysis on ones own computer. The R commands used for the analysis are\nfound in  voomLimma.R . An experienced R user can rerun the analysis\nwith any desired changes. This analysis requires the input countFile.txt  which contains counts per genes, the Rparameters.txt  file containing input parameters, and a processed\nsample list file  sampleList.txt  (Within the DNAnexus output directory structure, these files will be in\nthe LIMMA directory.)  The input for the simple differential analysis expression will be Rparameters_simple.txt ,  simpleDE.R ,  countFile.txt  and sampleList.txt .  countFile.txt  and  sampleList.txt  are the\nsame files used by the LIMMA analysis.  (Within the DNAnexus output directory structure, these files will be in\nthe SIMPLE_DIFEX directory.)   Coverage results  bigWig files will be generated for use in genome browsers (such as IGV http://software.broadinstitute.org/software/igv/ ). For each sample,\nmultiple bigWig files will be found. For all types of sequencing\nstrandedness, there will be bigWig files labeled, *.sortedCoverageFile.bed.bw  where ' ' is the sample name. For\nstranded data there will also be*.sortedPosCoverageFile.bed.bw * and *.sortedNegCoverageFile.bed.bw  which contains coverage information\nfor the positive and negative strand of the genome.  (Within the DNAnexus output directory structure, these files will be in\nthe BIGWIG directory.)   Quality Control Results (FastQC)  Within the FastQC directory, for each sample and read direction there\nwill be an html file and a zip file ( *.FastQc.html  *.FastQc.zip  where '*' is the base FastQ name), containing results\nfrom FastQTC. For the average user the html file is sufficient. This\nfile can give some basic statistics on the quality of the data.  (Within the DNAnexus output directory structure, these files will be in\nthe FastQC directory.)   BAM alignment files  There are two BAM files generated per sample that contain mapping\ninformation for all reads. The first is labeled *.Aligned.sortedByCoord.dup.bam  where ' ' is the sample name. The\nBAM file is sorted by coordinates and has duplicates marked. The second\nfile is*.Aligned.toTranscriptome.out.bam * and contains reads mapped\nto transcripts.  (Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)   Chimeric reads and junction files  Additional files created by STAR are provided. More information on these\nfiles can be found at http://labshare.cshl.edu/shares/gingeraslab/www-data/dobin/STAR/STAR.posix/doc/STARmanual.pdf . *.SJ.out.tab  contain splice junction information. Fusion detection\nfiles are labeled  *.Chimeric.out.bam  and *.Chimeric.out.junction .  (Within the DNAnexus output directory structure, these files will be in\nthe ALIGN directory.)   FPKM and count files (per sample)  Per sample files containing FPKM and raw count values for each gene can\nbe found in  *.fpkm.txt  and  *.htseq_counts.txt  where '*' is\nthe sample name. Within the DNAnexus output directory structure, these files will be in\nthe HTSEQ directory.",
            "title": "Secondary Results"
        },
        {
            "location": "/guides/tools/warden/#methods-files",
            "text": "A more human readable explanation is found in  methods.docx . Detailed\ndocumentation can be found in  methods.txt  (Within the DNAnexus output directory structure, these files will be in\nthe METHODS directory.)",
            "title": "Methods Files"
        },
        {
            "location": "/guides/tools/warden/#auxiliary-files",
            "text": "This section describes the files that exist within the DNAnexus output\nfolder. Most of these files will not be of interest to the average user.\nHowever, interactive viewers are describe in  LIMMA differential\nexpression viewer  and  Simple\ndifferential expression viewer .  The output will be divided into multiple folders. The results being the\nmost useful will be the differential expression analysis results in the\nLIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage\nwill be in the BIGWIG folder. Other folder contain different types of\ndata and are explained in further detail below.     The following description of files is sorted by their output directory.   ALIGN  This directory contains the BAM files described in  BAM alignment\nfiles  and the chimeric and junction files are\ndescribed in  Chimeric reads and junction\nfiles . In addition there are 2 log\nfiles.  *Log.final.out  has relevant statistics for the alignment.\nThe  *.Log.out  file just contains a log of the analysis run,\nincluding input parameters. Per sample FLAGSTAT results are found in *.flagStatOut.txt . Finally the ALIGN directory has\nmultiple  .starAlign.methods.txt files. These files can be ignored as\nthey are summarized in the finalmethods.docx * and  methods.txt \nfiles described in  Methods Files .   BIGWIG  All of the files here are described in section  Coverage\nresults . The  bgToBw.methods.txt  files can be\nignored as they are summarized in the files described in  Methods\nFiles .   BIGWIG_VIEWER  See  bigWig viewer   COMBINED_FLAGSTAT  This directory contains the  alignmentStatistics.txt  file, which contains the combined alignment statistics from all samples.  It is generated from the flagstat files describe in the ALIGN directory.   COMBINED_HTSEQ  Used for input in differential expression analysis. The\ncombineCountFile.txt is the same as countFile.txt described in Differential analysis input   COVERAGE  BED graph files used to generate bigWig files are here.   FastQC  See  Quality Control Results (FastQC)   HTSEQ  Per-sample HTSEQ-count results ( *.htseq_counts.txt ) and FPKM\nresults ( *.fpkm.txt ). Temporary methods files are found as\n*.htseq-count.methods.txt   LIMMA  mdsPlot.png ,  maPlot. .png,volcanoPlot. .png  are described in Initial analysis of results  results. .txt,GSEA.input. .txt  and * GSEA.tStat. .txt** are\ndescribe in  Differential expression\nresults  voomLimma.R ,  countFile.txt ,  Rparameters.txt , and sampleList.txt  are described in  Differential analysis\ninput  See  LIMMA differential expression\nviewer  for a description of the\nVIEWERS directory.  Other files in the LIMMA directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nlimmaMethods.txt is an intermediate file describing methods. Out.tar.gz\nis used for testing purposes. The sessionInfo.txt file describe the R\nsession working parameters and modules loaded. meanVariance.png is a\nplot for assessing quality of count data\n( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29 )   METHODS  The files here are described in  Methods Files .   SAMPLELIST  These files are used internally by the pipeline.   SIMPLE_DIFEX  mdsPlot.normCPM.png  and * simpleDEPlot. .png** are described in Initial analysis of results  * simpleDE. .txt** are describe in  Differential expression\nresults  simpleDE.R ,  countFile.txt ,  Rparameters_simple.txt , and sampleList.txt  are described in  Differential analysis\ninput  See  Simple differential expression\nviewer  for a description of the\nVIEWERS directory.  Other files in the SIMPLE_DIFEX directory include contrastFiles.txt\ncontrastsFile.txt, and limmaSampleList.txt which are used internally.\nsimpleDifEx.methods.txt is an intermediate file describing methods.\nOut.tar.gz is used for testing purposes. The sessionInfo.txt file\ndescribe the R session working parameters and modules loaded.",
            "title": "Auxiliary Files"
        },
        {
            "location": "/guides/tools/warden/#frequently-asked-questions",
            "text": "None yet! If you have any questions not covered here, feel free to reach\nout on  our contact\nform .",
            "title": "Frequently Asked Questions"
        },
        {
            "location": "/guides/tools/mutational-signatures/",
            "text": "Authors\n\n\nScott Newman, Michael Macias\n\n\n\n\n\n\nPublication\n\n\nMutational Signatures employs MutationalPatterns: \"\nMutationalPatterns: comprehensive genome-wide analysis of mutational processes.\n\"\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nMutational Signatures\n finds and quantifies COSMIC mutational signatures\nacross samples. This is done by finding the optimal non-negative linear\ncombination of mutation signatures to reconstruct a mutation matrix. It\nbuilds the initial mutation matrix from multiple single-sample VCFs and, by\ndefault, fits it to \nmutational signatures from COSMIC\n. Mutational\nSignatures employs \nMutationalPatterns\n (\nBlokzijl, et al. (2018)\n) to\nachieve this.\n\n\nMutational Signatures supports both hg19 (GRCh37) and hg38 (GRCh38).\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVCF(s)\n\n\nArray of files\n\n\nList of VCF inputs. Can be single-sample or multi-sample and uncompressed or gzipped.\n\n\n[\n*.vcf\n, \n*.vcf.gz\n]\n\n\n\n\n\n\nSample sheet\n\n\nFile\n\n\nTab-delimited file (no headers) with sample ID and tag pairs [optional]\n\n\n*.txt\n\n\n\n\n\n\nGenome build\n\n\nString\n\n\nGenome build used as reference. Can be either \"GRCh37\" or \"GRCh38\". [default: \"GRCh38\"]\n\n\nGRCh38\n\n\n\n\n\n\nMinimum mutation burden\n\n\nInteger\n\n\nMinimum number of somatic SNVs a sample must have to be considered for analysis [default: 9]\n\n\n15\n\n\n\n\n\n\nMinimum signature contribution\n\n\nInteger\n\n\nMinimum number of mutations attributable to a single signature [default: 9]\n\n\n100\n\n\n\n\n\n\nOutput prefix\n\n\nString\n\n\nPrefix to append to output filenames [optional]\n\n\nmtsg\n\n\n\n\n\n\nDisabled VCF column\n\n\nInteger\n\n\nVCF column (starting from sample names, zero-based) to ignore when reading VCFS [optional]\n\n\n1\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nRaw signatures\n\n\nFile\n\n\nTab-delimited file of the raw results with sample contributions for each signature\n\n\n\n\n\n\nSignatures visualization\n\n\nFile\n\n\nHTML file for interactive plotting\n\n\n\n\n\n\nSample sheet\n\n\nFile\n\n\nTab-delimited file (no headers) with sample ID and tag pairs\n\n\n\n\n\n\n\n\nProcess\n\n\n\nMutational Signatures runs four steps using subcommands of \nmtsg\n.\n\n\n\n\nSplit VCFs (single or multi-sample) to multiple single-sample VCFs.\n\n\nIf not given, generate a sample sheet from the directory of single-sample\n     VCFs.\n\n\nBuild a mutation matrix and reconstruct/fit it using COSMIC mutation\n     signatures.\n\n\nCreate a visualization file using the fitted signatures.\n\n\n\n\nGetting started\n\n\nAfter logging in, click the \"Start\" button on the \nMutational Signatures tool\npage\n. This creates a new DNAnexus project and imports the tool.\n\n\nWith subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.\n\n\nInput configuration\n\n\nMutational Signatures only requires VCFs as inputs. This can be a single\nmulti-sample VCF, multiple single-sample VCFs, or a combination of both. All\nother inputs are optional.\n\n\nInput files can be uploaded via the \ndata transfer application\n or \ncommand\nline\n.\n\n\nVCF(s)\n\n\n\nVCF(s)\n is a list of VCF inputs. The inputs can be single-sample or\nmulti-sample and uncompressed or gzipped. Sample names are taken from the VCF\nheader.\n\n\nWhen using multi-sample VCFs, empty cells/absent variant calls must be\ndenoted with \n.:.\n.\n\n\ngVCFs are not supported.\n\n\nSample sheet\n\n\n\nSample sheet\n is a tab-delimited file (no headers) with two columns: the\nsample ID and a tag. The tag is an arbitrary identifier used to group the\nsamples, typically a disease abbreviation or tissue of origin.\n\n\nIf not given, a sample sheet will be generated automatically.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSJACT001_D\n\n\nACT\n\n\n\n\n\n\nSJACT002_D\n\n\nACT\n\n\n\n\n\n\nSJBALL063_D\n\n\nBALL\n\n\n\n\n\n\nSJHGG017_D\n\n\nHGG\n\n\n\n\n\n\n\n\nOutput prefix\n\n\n\nOutput prefix\n is the prefix to append to the output filenames. By default,\nif a single input VCF is given, its basename is used as the output prefix. If\nmultiple input VCFs are given, a default \"mtsg\" prefix is used. This behavior\ncan be overridden by a user-defined prefix.\n\n\nExample\n\n\n\n\n\n\n\n\n\nVCF(s)\n\n\nPrefix\n\n\nOutput filename for raw signatures\n\n\n\n\n\n\n\n\n\n\n[\npcgp.b38.refseq.goodbad.vcf\n]\n\n\npcgp.b38.refseq.goodbad\n\n\npcgp.b38.refseq.goodbad.signatures.txt\n\n\n\n\n\n\n[\nSJOS013_D.vcf\n, \nSJRHB007_D.vcf\n]\n\n\nmtsg\n\n\nmtsg.signatures.txt\n\n\n\n\n\n\n\n\nDisabled VCF column\n\n\n\nDisabled VCF column\n is the column index to ignore when reading VCFs. This\nis useful when the inputs are tumor-normal VCFs, and one column should be\nignored. Otherwise, the results would likely be duplicated.\n\n\nThe argument is a zero-based index relative to the sample names in the header\nof the VCF. For example, in a VCF with samples \nSJEPD003_D\n and \nSJEPD003_G\n,\nthe germline sample (\nSJEPD003_G\n) can be discarded by setting the \ndisabled\nVCF column\n to \n1\n.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\n\n\n\n\n\n\n#CHROM\n\n\nPOS\n\n\nID\n\n\nREF\n\n\nALT\n\n\nQUAL\n\n\nFILTER\n\n\nINFO\n\n\nFORMAT\n\n\nSJEPD003_D\n\n\nSJEPD003_G\n\n\n\n\n\n\n\n\nUploading data\n\n\nMutational Signatures requires at least one VCF and an optional sample sheet\nto be uploaded. These files can be uploaded via the \ndata transfer\napplication\n or \ncommand line\n.\n\n\nAnalysis of results\n\n\nUpon a successful run of Mutational Signatures, three files are saved to the\nresults directory: raw signature contributions, a visualization file, and a\nsample sheet.\n\n\nInterpreting results\n\n\nRaw signatures\n\n\n\nRaw signatures\n is a tab-delimited file of the raw results with sample\ncontributions for each signature. Column 1 is the sample name, columns\n2-(N-1) are the COSMIC signatures contribution counts, and column N is the\ngroup tag, where N is the total number of columns. The number of columns is\nvariable since if the signature has no contributions for all samples, it is\ncompletely omitted.\n\n\nNote that the last column \ntissue\n is a misnomer. It aligns to the arbitrary\ntag given in the \nsample sheet\n.\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nSignature.1\n\n\nSignature.2\n\n\n\u2026\n\n\nSignature.30\n\n\ntissue\n\n\n\n\n\n\n\n\n\n\nSJACT001_D\n\n\n1.71758029\n\n\n131.033723\n\n\n\u2026\n\n\n18.6910151\n\n\nACT\n\n\n\n\n\n\nSJAMLM7005_D\n\n\n51.9627312\n\n\n7.10850351\n\n\n\u2026\n\n\n0\n\n\nAMLM7\n\n\n\n\n\n\n\n\nSignatures visualization\n\n\n\nSignatures visualization\n is an HTML file that can be used for interactive\nplotting.\n\n\nWhen opened in a web browser, a set of controls allows plotting various\nstacked bar charts: total contributions by signature, total contributions by\ntag, and total contributions by sample per tag. The total contributions can be\nstacked as absolute values or as a percentage of the total.\n\n\nSample sheet\n\n\n\nWhen no sample sheet is given as an input, one is generated automatically,\nbut it is not guaranteed the derived tags will be of any use. This generated\nsample sheet is given as an output in the case the tags need to be manually\nedited, and the job is resubmitted with it as an input.\n\n\nWhen a sample sheet is given as an input, the sample sheet output is a copy\nof the input.\n\n\nSee also the description for the input \nsample sheet\n.\n\n\nTroubleshooting\n\n\nTo troubleshoot a failed run of Mutational Signatures, check the job log for\ndetails.\n\n\nWrong genome build\n\n\n\nIf the \"Building mutation matrix\" step during \nrun\n fails, it is likely that\nthe selected genome build does not match the input VCF(s). Rerun the job with\na matching genome build.\n\n\nExample\n\n\n\nR\n:\n \nBuilding\n \nmutation\n \nmatrix\n \nfrom\n \n6\n \nVCFs\n\n\nR\n:\n \nError\n \nin\n \nmut_matrix\n(\nvcf_list\n \n=\n \nfiltered_vcfs\n,\n \nref_genome\n \n=\n \nref_genome\n)\n \n:\n\n\nR\n:\n   \nError\n \nin\n \n.\nCall2\n(\n\"solve_user_SEW\"\n,\n \nrefwidths\n,\n \nstart\n,\n \nend\n,\n \nwidth\n,\n \ntranslate\n.\nnegative\n.\ncoord\n,\n  \n:\n\n\nR\n:\n   \nsolving\n \nrow\n \n526\n:\n \n'allow.nonnarrowing'\n \nis\n \nFALSE\n \nand\n \nthe\n \nsupplied\n \nstart\n \n(\n79440206\n)\n \nis\n \n>\n \nrefwidth\n \n+\n \n1\n\n\n\n\n\n\nReferences\n\n\n\n\nBlokzijl F, Janssen R, van Boxtel R, Cuppen E (2018). \"MutationalPatterns:\n    comprehensive genome-wide analysis of mutational processes.\" \nGenome\n    Medicine\n. doi: \n10.1186/s13073-018-0539-0\n. PMID: \n29695279\n.",
            "title": "Mutational Signatures"
        },
        {
            "location": "/guides/tools/mutational-signatures/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/mutational-signatures/#getting-started",
            "text": "After logging in, click the \"Start\" button on the  Mutational Signatures tool\npage . This creates a new DNAnexus project and imports the tool.  With subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/mutational-signatures/#input-configuration",
            "text": "Mutational Signatures only requires VCFs as inputs. This can be a single\nmulti-sample VCF, multiple single-sample VCFs, or a combination of both. All\nother inputs are optional.  Input files can be uploaded via the  data transfer application  or  command\nline .",
            "title": "Input configuration"
        },
        {
            "location": "/guides/tools/mutational-signatures/#uploading-data",
            "text": "Mutational Signatures requires at least one VCF and an optional sample sheet\nto be uploaded. These files can be uploaded via the  data transfer\napplication  or  command line .",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/mutational-signatures/#analysis-of-results",
            "text": "Upon a successful run of Mutational Signatures, three files are saved to the\nresults directory: raw signature contributions, a visualization file, and a\nsample sheet.",
            "title": "Analysis of results"
        },
        {
            "location": "/guides/tools/mutational-signatures/#interpreting-results",
            "text": "",
            "title": "Interpreting results"
        },
        {
            "location": "/guides/tools/mutational-signatures/#troubleshooting",
            "text": "To troubleshoot a failed run of Mutational Signatures, check the job log for\ndetails.",
            "title": "Troubleshooting"
        },
        {
            "location": "/guides/tools/mutational-signatures/#references",
            "text": "Blokzijl F, Janssen R, van Boxtel R, Cuppen E (2018). \"MutationalPatterns:\n    comprehensive genome-wide analysis of mutational processes.\"  Genome\n    Medicine . doi:  10.1186/s13073-018-0539-0 . PMID:  29695279 .",
            "title": "References"
        },
        {
            "location": "/guides/tools/cis-x/",
            "text": "Warning\n\n\ncis-X is an upcoming St. Jude Cloud tool and is not yet publicly available.\nSee \ncis-X on St. Jude Research\n for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthors\n\n\nYu Liu, Chunliang Li, Shuhong Shen\n\n\n\n\n\n\nPublication\n\n\nN/A (not published)\n\n\n\n\n\n\nTechnical Support\n\n\nContact Us\n\n\n\n\n\n\n\n\nActivating regular variants usually cause the cis-activation of target genes.\nTo find cis-activated genes, allelic specific/imbalance expressions (ASE) and\noutlier high expression (OHE) signals are used. Variants in the same\ntopologically associated domains with the candidates can then be searched,\nincluding structural variants (SV), copy number aberrations (CNA), and single\nnucleotide variations (SNV) and insertion/deletions (indel).\n\n\nA transcription factor binding analysis is also done, using motifs from\n\nHOCOMOCO\n v10 models.\n\n\ncis-X currently only works with hg19 (GRCh37).\n\n\nOverview\n\n\nInputs\n\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nSample ID\n\n\nString\n\n\nThe ID of the input sample\n\n\nSJALL018373_D1\n\n\n\n\n\n\nDisease subtype\n\n\nString\n\n\nThe disease name under analysis. Must be either TALL or AML.\n\n\nTALL\n\n\n\n\n\n\nSingle nucleotide variants\n\n\nFile\n\n\nTab-delimited file containing raw sequence variants\n\n\n*.txt\n\n\n\n\n\n\nCNV/LOH regions\n\n\nFile\n\n\nTab-delimited file containing any aneuploidy region existing in the tumor genome under analysis\n\n\n*.txt\n\n\n\n\n\n\nRNA-seq BAM\n\n\nFile\n\n\nBAM file aligned to hg19 (GRCh37)\n\n\n*.bam\n\n\n\n\n\n\nRNA-seq BAM index\n\n\nFile\n\n\nBAM index for the given BAM\n\n\n*.bam.bai\n\n\n\n\n\n\nGene expression table\n\n\nFile\n\n\nTab-delimited file containing gene level expressions for the tumor under analysis in FPKM\n\n\n*.txt\n\n\n\n\n\n\nSomatic SNV/indels\n\n\nFile\n\n\nTab-delimited file containing somatic SNV/indels in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nSomatic SVs\n\n\nFile\n\n\nTab-delimited file containing somatic acquired structural variants in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nSomatic CNVs\n\n\nFile\n\n\nTab-delimited file containing copy number aberrations in the tumor genome\n\n\n*.txt\n\n\n\n\n\n\nCNV/LOH action\n\n\nString\n\n\nThe behavior when handling markers in CNV/LOH regions. Can be either \nkeep\n or \ndrop\n.\n\n\ndrop\n\n\n\n\n\n\nMinimum coverage for WGS\n\n\nInteger\n\n\nThe minimum coverage in WGS to be included in the analysis\n\n\n10\n\n\n\n\n\n\nMinimum coverage for RNA-seq\n\n\nInteger\n\n\nThe minimum coverage in RNA-seq to be included in the analysis\n\n\n5\n\n\n\n\n\n\nCandidate FPKM threshold\n\n\nFloat\n\n\nThe FPKM threshold for the nomination of a cis-activated candidate\n\n\n0.1\n\n\n\n\n\n\n\n\nOutputs\n\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncis-activated candidates\n\n\ncis-activated candidates in the tumor genome under analysis\n\n\n\n\n\n\nSV candidates\n\n\nStructural variant (SV) candidates predicted as the causal for the cis-activated genes in the regulatory territory\n\n\n\n\n\n\nCNA candidates\n\n\nCopy number aberrations (CNA) predicted as the causal for the cis-activated genes in the regulatory territory\n\n\n\n\n\n\nSNV/indel candidates\n\n\nSNV/indel candidates predicted as functional and predicted transcription factors\n\n\n\n\n\n\nOHE results\n\n\nRaw outlier high expression (OHE) results\n\n\n\n\n\n\nGene level ASE results\n\n\nRaw gene level allelic specific expression (ASE) results\n\n\n\n\n\n\nSingle marker ASE results\n\n\nRaw single marker allelic specific expression (ASE) results\n\n\n\n\n\n\n\n\nGetting started\n\n\nAfter logging in, click the \"Start\" button on the \ncis-X tool page\n. This\ncreates a new DNAnexus project and imports the tool.\n\n\nWith subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.\n\n\nInput file configuration\n\n\ncis-X requires six tab-delimited input files to be prepared in advance. These\nfiles can be uploaded via the \ndata transfer application\n or \ncommand line\n.\n\n\n\n\nNote\n\n\nEven though CNV/LOH regions, somatic SNV/indels, somatic SVs, and\nsomatic CNVs can be \"empty\", using such inputs will produce results with a\nmuch higher false positive rate.\n\n\n\n\nSingle nucleotide variants\n\n\n\nA list of single nucleotide markers is a tab-delimited file with the\nfollowing columns:\n\n\n\n\nChr\n: chromosome name for the marker\n\n\nPos\n: genomic start location for the marker\n\n\nChr_Allele\n: reference allele\n\n\nAlternative_Allele\n: alternative allele\n\n\nreference_tumor_count\n: reference allele count in the tumor genome\n\n\nalternative_tumor_count\n: alternative allele count in the tumor genome\n\n\nreference_normal_count\n: reference allele count in the matched normal genome\n\n\nalternative_normal_count\n: alternative count in the matched normal genome\n\n\n\n\nThis file can be generated with Bambino.\n\n\nExample\n\n\n\n\n\n\n\n\n\nChr\n\n\nPos\n\n\nChr_Allele\n\n\nAlternative_Allele\n\n\nreference_tumor_count\n\n\nalternative_tumor_count\n\n\nreference_normal_count\n\n\nalternative_normal_count\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n61396\n\n\nTT\n\n\n\n\n0\n\n\n3\n\n\n0\n\n\n10\n\n\n\n\n\n\nchr11\n\n\n72981\n\n\n\n\nT\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n\n\n\n\n\n\nCNV/LOH regions\n\n\n\nThe CNV/LOH regions are all the genomic regions carrying copy number\nvariations (CNV) or loss of heterozygosity (LOH), which will be filtered out\nduring analysis.\n\n\nThis is a tab-delimited file in the bed format. It must have at least the\nfollowing three columns:\n\n\n\n\nchrom\n: chromosome name\n\n\nloc.start\n: genomic start location\n\n\nloc.end\n: genomic end location\n\n\n\n\nIf no CNV/LOH are in the genome under analysis, a file with no rows (but\nincluding headers) can be provided.\n\n\nThis file can be generated with CONSERTING.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\nloc.start\n\n\nloc.end\n\n\nSample\n\n\nseg.mean\n\n\nLogRatio\n\n\nsource\n\n\n\n\n\n\n\n\n\n\nchr9\n\n\n10712\n\n\n37855747\n\n\nSJALL018373_D1\n\n\n0.471181417\n\n\n\n\nLOH\n\n\n\n\n\n\nchr9\n\n\n20276901\n\n\n20703900\n\n\nSJALL018373_D1\n\n\n-0.978\n\n\n-5.696\n\n\nCNV\n\n\n\n\n\n\n\n\nGene expression table\n\n\n\nThe gene expression table is a tab-delimited file containing gene level\nexpressions for the tumor under analysis. The expressions are in FPKM\n(fragments per kilobase of transcript per million mapped reads).\n\n\n\n\nGeneID\n: gene \nEnsembl\n ID\n\n\nGeneName\n: gene symbol\n\n\nType\n: \ntranscript type\n\n\nStatus\n: transcript status (must be \nKNOWN\n, \nNOVEL\n, or \nPUTATIVE\n)\n\n\nChr\n: chromosome name\n\n\nStart\n genomic start location\n\n\nEnd\n: genomic end location\n\n\n[SampleID...]: FPKM for the given sample\n\n\n\n\nThis file can be generated with the output of HTseq-count preprocessed\nthrough \nmergeData_geneName.pl\n (available with the distribution of cis-X).\nThe data must be able to match values in the given gene specific reference\nexpression matrices generated from a larger cohort.\n\n\nExample\n\n\n\n\n\n\n\n\n\nGeneID\n\n\nGeneName\n\n\nType\n\n\nStatus\n\n\nChr\n\n\nStart\n\n\nEnd\n\n\nSJALL018373_D1\n\n\n\n\n\n\n\n\n\n\nENSG00000261122.2\n\n\n5S_rRNA\n\n\nlincRNA\n\n\nNOVEL\n\n\nchr16\n\n\n34977639\n\n\n34990886\n\n\n0.0000\n\n\n\n\n\n\nENSG00000249352.3\n\n\n7SK\n\n\nlincRNA\n\n\nNOVEL\n\n\nchr5\n\n\n68266266\n\n\n68325992\n\n\n4.5937\n\n\n\n\n\n\n\n\nSomatic SNV/indels\n\n\n\nThis is a tab-delimited file containing somatic sequence mutations present in\nthe genome under analysis. It includes both single nucleotide variants (SNV)\nand small insertion/deletions (indel). The file must have the following\ncolumns:\n\n\n\n\nchr\n: chromosome name\n\n\npos\n: genomic start location\n\n\nref\n: reference nucleotide\n\n\nmutant\n: mutant nucleotide\n\n\ntype\n: mutation type (must be either \nsnv\n or \nindel\n)\n\n\n\n\nNote that the coordinate used for an indel is after the inserted sequence.\n\n\nIf no SNV/indels are in the sample under analysis, a file with no rows\n(but including headers) can be provided.\n\n\nThis file can can be created with Bambino and then preprocessed using the\nsteps taken in \"\nThe genetic basis of early T-cell precursor acute lymphoblastic leukaemia\n\".\n\n\nExample\n\n\n\n\n\n\n\n\n\nchr\n\n\npos\n\n\nref\n\n\nmut\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nchr1\n\n\n24782720\n\n\nG\n\n\nA\n\n\nsnv\n\n\n\n\n\n\nchr11\n\n\n82896176\n\n\nT\n\n\nC\n\n\nsnv\n\n\n\n\n\n\n\n\nSomatic SVs\n\n\n\nThis is a tab-delimited file containing somatic-acquired structural variants\n(SV) in the cancer genome. The file must have the following columns:\n\n\n\n\nchrA\n: chromosome name of the left breakpoint\n\n\nposA\n: genomic location of the left breakpoint\n\n\nortA\n: strand orientation of the left breakpoint\n\n\nchrB\n: chromosome name of the right breakpoint\n\n\nposB\n: genomic location of the right breakpoint\n\n\nortB\n: strand orientation of the right breakpoint\n\n\n\n\nStrand orientations are denoted with a \n+\n for a sense or coding strand and\n\n-\n for a antisense or non-coding strand.\n\n\nIf no somatic SVs are in the sample under analysis, a file with no rows (but\nincluding headers) can be provided.\n\n\nThis file can be generated by CREST.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrA\n\n\nposA\n\n\nortA\n\n\nchrB\n\n\nposB\n\n\nortB\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n33913169\n\n\n+\n\n\nchr7\n\n\n142494049\n\n\n-\n\n\nCTX\n\n\n\n\n\n\nchr11\n\n\n64219334\n\n\n+\n\n\nchr2\n\n\n205042527\n\n\n-\n\n\nCTX\n\n\n\n\n\n\n\n\nSomatic CNVs\n\n\n\nThis is a tab-delimited file containing the genomic\nregions with somatic-acquired copy number aberrations (CNA) in the cancer\ngenome.\n\n\n\n\nchr\n: chromosome name\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\nlogR\n: log2 ratio\n\n\n\n\nIf no somatic CNVs are in the sample under analysis, a file with no rows\n(but including headers) can be provided.\n\n\nThis file can be generating by CONSERTING.\n\n\nExample\n\n\n\n\n\n\n\n\n\nchr\n\n\nstart\n\n\nend\n\n\nlogR\n\n\n\n\n\n\n\n\n\n\nchr9\n\n\n20276901\n\n\n20703900\n\n\n-5.696\n\n\n\n\n\n\n\n\nUploading data\n\n\ncis-X requires a total of eight files to be uploaded, as described in \"\nInput\nfile configuration\n\". These files can be uploaded via the \ndata transfer\napplication\n or \ncommand line\n.\n\n\nRunning the tool\n\n\n\n\nTodo\n\n\n\n\nMonitoring run progress\n\n\n\n\nTodo\n\n\n\n\nAnalysis of results\n\n\nUpon a successful run of cis-X, seven tab-delimited files are saved to the\nresults directory. These raw results can be processed through external tools\nfor further analysis.\n\n\nInterpreting results\n\n\ncis-activated candidates\n\n\n\nThe main result file contains the cis-activated candidates in the tumor genome\nunder analysis.\n\n\n\n\ngene\n: gene accession number (\nRefSeq\n ID)\n\n\ngsym\n: gene symbol\n\n\nchrom\n: chromosome name\n\n\nstrand\n: strand orientation\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\ncdsStartStat\n: coding sequence (CDS) start status\n\n\ncdsEndStat\n: coding sequence (CDS) end status\n\n\nmarkers\n: number of heterozygous markers in this gene\n\n\nase_markers\n: number of heterozygous markers showing allelic specific expressions (ASE)\n\n\naverage_ai_all\n: average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers\n\n\naverage_ai_ase\n: average BAF difference between RNA and DNA for ASE markers\n\n\npval_all_markers\n: p-value for each marker in the ASE test\n\n\npval_ase_markers\n: p-value for ASE markers in the ASE test\n\n\nai_all_markers\n: BAF difference between RNA and DNA for all heterozygrous markers\n\n\nai_ase_markers\n: BAF difference between RNA and DNA for ASE markers\n\n\ncomb.pval\n: combined p-value for the ASE test\n\n\nmean.delta\n: average BAF difference between RNA and DNA for all markers\n\n\nrawp\n: raw p-value for the ASE test\n\n\nBonferroni\n: adjusted p-value for the ASE test (single-step Bonferroni)\n\n\nABH\n: adjusted p-value for the ASE test (Benjamini-Hochberg)\n\n\nFPKM\n: FPKM value\n\n\nloo.source\n: which reference expression matrix was used in the outlier high expression (OHE) test\n\n\nloo.cohort.size\n: number of cases in the reference expression matrix for this gene\n\n\nloo.pval\n: p-value of the OHE test\n\n\nloo.rank\n: rank of the case under analysis among the reference cases\n\n\nimprinting.status\n: imprinting status of the gene\n\n\ncandidate.group\n: status of the gene, combining both ASE and outlier tests\n\n\n\n\nStrand orientations are denoted with a \n+\n for a sense or coding strand and\n\n-\n for a antisense or non-coding strand.\n\n\nCoding sequence status is typically one of \"none\" (not specified), \"unk\"\n(unknown), \"incmpl\" (incomplete), or \"cmpl\" (complete).\n\n\nExample\n\n\n\n\n\n\n\n\n\ngene\n\n\ngsym\n\n\nchrom\n\n\nstrand\n\n\nstart\n\n\nend\n\n\ncdsStartStat\n\n\ncdsEndStat\n\n\nmarkers\n\n\nase_markers\n\n\naverage_ai_all\n\n\naverage_ai_ase\n\n\npval_all_markers\n\n\npval_ase_markers\n\n\nai_all_markers\n\n\nai_ase_markers\n\n\ncomb.pval\n\n\nmean.delta\n\n\nrawp\n\n\nBonferroni\n\n\nABH\n\n\nFPKM\n\n\nloo.source\n\n\nloo.cohort.size\n\n\nloo.pval\n\n\nloo.rank\n\n\nimprinting.status\n\n\ncandidate.group\n\n\n\n\n\n\n\n\n\n\nNM_145804\n\n\nABTB2\n\n\nchr11\n\n\n-\n\n\n34172533\n\n\n34379555\n\n\ncmpl\n\n\ncmpl\n\n\n5\n\n\n5\n\n\n0.5\n\n\n0.500\n\n\n0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625\n\n\n0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625\n\n\n0.5,0.5,0.5,0.5,0.5\n\n\n0.5,0.5,0.5,0.5,0.5\n\n\n0.000644290972057077\n\n\n0.5\n\n\n0.000644290972057077\n\n\n0.632049443587993\n\n\n0.0110866672927557\n\n\n7.6776\n\n\nbi_cohort\n\n\n40\n\n\n0.0367241086505276\n\n\n1\n\n\n\n\nase_outlier\n\n\n\n\n\n\nNM_003189\n\n\nTAL1\n\n\nchr1\n\n\n-\n\n\n47681961\n\n\n47698007\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n2\n\n\n0.482\n\n\n0.482\n\n\n6.66361745922277e-28,3.30872245021211e-24\n\n\n6.66361745922277e-28,3.30872245021211e-24\n\n\n0.464912280701754,0.5\n\n\n0.464912280701754,0.5\n\n\n4.69553625126628e-26\n\n\n0.482456140350877\n\n\n4.69553625126628e-26\n\n\n4.60632106249222e-23\n\n\n6.11761294450693e-24\n\n\n8.8168\n\n\nwhite_list\n\n\n167\n\n\n0.0139385771987089\n\n\n1\n\n\n\n\nase_outlier\n\n\n\n\n\n\n\n\nSV candidates\n\n\n\nStructural variant (SV) candidates include candidates predicted as the causal\nfor the cis-activated genes in the regulatory territory.\n\n\n\n\nleft.candidate.inTAD\n: cis-activated candidate near the left breakpoint\n\n\nright.candidate.inTAD\n: cis-activated candidate near the right breakpoint\n\n\nchrA\n: chromosome name of the left breakpoint\n\n\nposA\n: genomic location of the left breakpoint\n\n\nortA\n: strand orientation of the left breakpoint\n\n\nchrB\n: chromosome name of the right breakpoint\n\n\nposB\n: genomic location of the right breakpoint\n\n\nortB\n: strand orientation of the right breakpoint\n\n\ntype\n: type of translocation\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nleft.candidate.inTAD\n\n\nright.candidate.inTAD\n\n\nchrA\n\n\nposA\n\n\nortA\n\n\nchrB\n\n\nposB\n\n\nortB\n\n\ntype\n\n\n\n\n\n\n\n\n\n\nLMO2\n\n\n\n\nchr11\n\n\n33913169\n\n\n+\n\n\nchr7\n\n\n142494049\n\n\n-\n\n\nCTX\n\n\n\n\n\n\n\n\nCNA candidates\n\n\n\nCopy number aberration (CNA) candidates include candidates predicted as the\ncausal for the cis-activated genes in the regulatory territory.\n\n\n\n\ncandidate.inTAD\n: cis-activated candidate by the CNA\n\n\nchr\n: chromosome name\n\n\nstart\n: genomic start position\n\n\nend\n: genomic end location\n\n\nlogR\n: log ratio of the CNA\n\n\n\n\nSNV/indel candidates\n\n\n\nSNV/indel candidates include predicted candidates as functional and predicted\ntranscription factors. The mutations are also annotated for known regulatory\nelements reported by the \nNIH Roadmap Epigenomics Project\n by collecting 111\ncell lines.\n\n\n\n\nchrom\n: chromosome name\n\n\npos\n: genomic start position\n\n\nref\n: reference allele genotype\n\n\nmut\n: mutant allele genotype\n\n\ntype\n: mutation type (either \nsnv\n or \nindel\n)\n\n\ntarget\n: cis-activated candidate\n\n\ndist\n: distance between the mutation and transcription start sites of the target gene\n\n\ntf\n: transcription factors predicted to have the binding motif introduced by the mutation\n\n\nEpiRoadmap_enhancer\n: enhancer regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\nEpiRoadmap_promoter\n: promoter regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\nEpiRoadmap_dyadic\n: dyadic regions that overlap with the mutation (from the \nNIH Roadmap Epigenomics Project\n)\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\npos\n\n\nref\n\n\nmut\n\n\ntype\n\n\ntarget\n\n\ndist\n\n\ntf\n\n\nEpiRoadmap_enhancer\n\n\nEpiRoadmap_promoter\n\n\nEpiRoadmap_dyadic\n\n\n\n\n\n\n\n\n\n\nchr1\n\n\n47696311\n\n\nC\n\n\nT\n\n\nsnv\n\n\nTAL1\n\n\n1696\n\n\nBCL11A,CEBPG,PBX2,YY1,ZBTB4\n\n\n\n\nBrain,Digestive,ES-deriv,ESC,HSC & B-cell,Heart,Muscle,Other,Sm. Muscle,iPSC\n\n\n\n\n\n\n\n\n\n\nOHE results\n\n\n\nOHE results are the raw results for the outlier expression test.\n\n\n\n\nGene\n: gene symbol\n\n\nfpkm.raw\n: FPKM value\n\n\nsize.bi\n: number of cases in the bi-allelic reference cohort\n\n\np.bi\n: p-value in the outlier test using the bi-allelic reference cohort\n\n\nrank.bi\n: rank of the expression level in the case under analysis compared to the bi-allelic reference cohort\n\n\nsize.cohort\n: number of cases in the entire reference cohort\n\n\np.cohort\n: p-value in the outlier test using the entire reference cohort\n\n\nrank.cohort\n: rank of the expression level in the case under analysis compared to the entire reference cohort\n\n\nsize.white\n: number of cases in the whitelist reference cohort\n\n\np.white\n: p-value in the outlier test using the whitelist reference cohort\n\n\nrank.white\n: rank of the expression level in the case under analysis compared to the whitelist reference cohort\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nGene\n\n\nfpkm.raw\n\n\nsize.bi\n\n\np.bi\n\n\nrank.bi\n\n\nsize.cohort\n\n\np.cohort\n\n\nrank.cohort\n\n\nsize.white\n\n\np.white\n\n\nrank.white\n\n\n\n\n\n\n\n\n\n\n7SK\n\n\n4.5937\n\n\nna\n\n\nna\n\n\nna\n\n\n264\n\n\n0.716284011918374\n\n\n162\n\n\nna\n\n\nna\n\n\nna\n\n\n\n\n\n\nA1BG\n\n\n0.2312\n\n\n24\n\n\n0.900132642257996\n\n\n21\n\n\n264\n\n\n0.84055666600945\n\n\n222\n\n\nna\n\n\nna\n\n\nna\n\n\n\n\n\n\n\n\nGene level ASE results\n\n\n\nGene level ASE results are the raw results from the gene level ASE test.\n\n\n\n\ngene\n: gene accession number (\nRefSeq\n ID)\n\n\ngsym\n: gene symbol\n\n\nchrom\n: chromosome name\n\n\nstrand\n: strand orientation\n\n\nstart\n: genomic start location\n\n\nend\n: genomic end location\n\n\ncdsStartStat\n: coding sequence (CDS) start status\n\n\ncdsEndStat\n: coding sequence (CDS) end status\n\n\nmarkers\n: number of heterozygous markers in this gene\n\n\nase_markers\n: number of heterozygous markers showing allelic specific expressions (ASE)\n\n\naverage_ai_all\n: average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers\n\n\naverage_ai_ase\n: average BAF difference between RNA and DNA for ASE markers\n\n\npval_all_markers\n: p-value for each marker in the ASE test\n\n\npval_ase_markers\n: p-value for ASE markers in the ASE test\n\n\nai_all_markers\n: BAF difference between RNA and DNA for all heterozygrous markers\n\n\nai_ase_markers\n: BAF difference between RNA and DNA for ASE markers\n\n\ncomb.pval\n: combined p-value for the ASE test\n\n\nmean.delta\n: average BAF difference between RNA and DNA for all markers\n\n\nrawp\n: raw p-value for the ASE test\n\n\nBonferroni\n: adjusted p-value for the ASE test (single-step Bonferroni)\n\n\nABH\n: adjusted p-value for the ASE test (Benjamini-Hochberg)\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\ngene\n\n\ngsym\n\n\nchrom\n\n\nstrand\n\n\nstart\n\n\nend\n\n\ncdsStartStat\n\n\ncdsEndStat\n\n\nmarkers\n\n\nase_markers\n\n\naverage_ai_all\n\n\naverage_ai_ase\n\n\npval_all_markers\n\n\npval_ase_markers\n\n\nai_all_markers\n\n\nai_ase_markers\n\n\ncomb.pval\n\n\nmean.delta\n\n\nrawp\n\n\nBonferroni\n\n\nABH\n\n\n\n\n\n\n\n\n\n\nNM_024684\n\n\nAAMDC\n\n\nchr11\n\n\n+\n\n\n77532207\n\n\n77583398\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n0\n\n\n0.079\n\n\nna\n\n\n0.924775093657227,0.0331439677875056\n\n\nna\n\n\n0.00892857142857145,0.149122807017544\n\n\nna\n\n\n0.175073458624837\n\n\n0.0790256892230577\n\n\n0.175073458624837\n\n\n1\n\n\n0.480780882445856\n\n\n\n\n\n\nNM_015423\n\n\nAASDHPPT\n\n\nchr11\n\n\n+\n\n\n105948291\n\n\n105969419\n\n\ncmpl\n\n\ncmpl\n\n\n2\n\n\n0\n\n\n0.023\n\n\nna\n\n\n0.749258624760841,1\n\n\nna\n\n\n0.0384615384615384,0.00769230769230766\n\n\nna\n\n\n0.86559726476049\n\n\n0.023076923076923\n\n\n0.86559726476049\n\n\n1\n\n\n0.873257417545981\n\n\n\n\n\n\n\n\nSingle marker ASE results\n\n\n\nSingle marker ASE results are the raw results from the single marker ASE test.\n\n\n\n\nchrom\n: chromosome name\n\n\npos\n: genomic start position\n\n\nref\n: reference allele genotype\n\n\nmut\n: non-reference allele genotype\n\n\ncvg_wgs\n: coverage of the marker from the whole genome sequence (WGS)\n\n\nmut_freq_wgs\n: non-reference allele fraction in the WGS\n\n\ncvg_rna\n: coverage of the marker from the RNA-seq\n\n\nmut_freq_rna\n: non-reference allele fraction in the RNA-seq\n\n\nref.1\n: read count of the reference allele in the RNA-seq\n\n\nvar\n: read count of the non-reference allele in the RNA-seq\n\n\npvalue\n: p-value from the binomial test\n\n\ndelta.abs\n: absolute difference of the non-reference allele fraction between the WGS and RNA-seq\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\nchrom\n\n\npos\n\n\nref\n\n\nmut\n\n\ncvg_wgs\n\n\nmut_freq_wgs\n\n\ncvg_rna\n\n\nmut_freq_rna\n\n\nref.1\n\n\nvar\n\n\npvalue\n\n\ndelta.abs\n\n\n\n\n\n\n\n\n\n\nchr11\n\n\n204147\n\n\nG\n\n\nA\n\n\n36\n\n\n0.472\n\n\n85\n\n\n0.553\n\n\n38\n\n\n47\n\n\n0.385669420119278\n\n\n0.0529411764705883\n\n\n\n\n\n\nchr11\n\n\n205198\n\n\nC\n\n\nA\n\n\n23\n\n\n0.522\n\n\n83\n\n\n0.313\n\n\n57\n\n\n26\n\n\n0.000877551780002863\n\n\n0.186746987951807",
            "title": ""
        },
        {
            "location": "/guides/tools/cis-x/#overview",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/guides/tools/cis-x/#getting-started",
            "text": "After logging in, click the \"Start\" button on the  cis-X tool page . This\ncreates a new DNAnexus project and imports the tool.  With subsequent runs, the sidebar shows \"Launch Tool\", meaning the project with\nthe tool already exists. Click \"Launch Tool\" to start a new analysis.",
            "title": "Getting started"
        },
        {
            "location": "/guides/tools/cis-x/#input-file-configuration",
            "text": "cis-X requires six tab-delimited input files to be prepared in advance. These\nfiles can be uploaded via the  data transfer application  or  command line .   Note  Even though CNV/LOH regions, somatic SNV/indels, somatic SVs, and\nsomatic CNVs can be \"empty\", using such inputs will produce results with a\nmuch higher false positive rate.",
            "title": "Input file configuration"
        },
        {
            "location": "/guides/tools/cis-x/#uploading-data",
            "text": "cis-X requires a total of eight files to be uploaded, as described in \" Input\nfile configuration \". These files can be uploaded via the  data transfer\napplication  or  command line .",
            "title": "Uploading data"
        },
        {
            "location": "/guides/tools/cis-x/#running-the-tool",
            "text": "Todo",
            "title": "Running the tool"
        },
        {
            "location": "/guides/tools/cis-x/#monitoring-run-progress",
            "text": "Todo",
            "title": "Monitoring run progress"
        },
        {
            "location": "/guides/tools/cis-x/#analysis-of-results",
            "text": "Upon a successful run of cis-X, seven tab-delimited files are saved to the\nresults directory. These raw results can be processed through external tools\nfor further analysis.",
            "title": "Analysis of results"
        },
        {
            "location": "/guides/tools/cis-x/#interpreting-results",
            "text": "",
            "title": "Interpreting results"
        },
        {
            "location": "/guides/portals/pecan/",
            "text": "PeCan Data Portal\n\n\nPeCan provides interactive visualizations of pediatric cancer mutations across various projects at St. Jude Children's Research Hospital and its collaborating institutions.\n\n\nHomepage\n\n\nThe \nPeCan homepage\n contains two main visualizations that work with each other to give a high level overview of the data being presented (SJ Cloud's \nPCGP\n dataset along with curated datasets from other institutions such as \nTARGET\n, \ndkfz\n, and others).\n\n\nDonut Chart\n\n\nThe donut chart (shown below) gives an at-a-glance disease distribution and disease hierarchy. \n\n\n\n\nYou can hover over the various donut slices to glance at the number (and %) of samples being represented by that disease. The diseases are categorized in two three main root categories: 1) HM -Hematopoietic Malignancies, 2) BT -Brain Tumor, 3) ST -Solid Tumor.\n\n\nClick here\n for a full mapping of disease codes.\n\n\nBubble Chart\n\n\nAny slice (at any level) of the donut chart can be clicked on to select it, and reveal a bubble chart of related genes. \n\n\nNote that the dataset bar (shown below) on top of the bubble chart visualizes the distribution of selected data across the datasets used in this visualization. It will update dynamically as you interact with the donut chart and make different selections.\n\n\n\n\nAn example of the bubble chart is shown below.\n\n\n\n\nYou can see the selected disease shown at the top (1). The bubbles represent the most prevalent genes in the selected disease sample set. The size of the bubble corresponds to the number of mutations in the set with that gene.\n\n\nFor some disease sets (like the one shown above), we have identified the most important disease pathway for the gene and have categorized them as such. This information is represented here via the use of colors. The legend at the bottom allows you to view the pathway information being shown (including the number of genes that are attached to each pathway).\n\n\nHovering over a pathway in the legend will highlight all matching genes. Clicking a gene will open it's ProteinPaint.\n\n\nProteinPaint\n\n\nProteinPaint\n is a web application for simultaneously visualizing genetic lesions (including sequence mutations and gene fusions) and RNA expression in pediatric cancers. You can find the ProteinPaint \npaper here\n. \n\n\nOverview\n\n\nThe image below shows an example ProteinPaint of the gene \nTP53\n annotated with descriptions of the many interactive elements of a ProteinPaint visualization. As you can see, there is a lot to explore.\n\n\n\n\nGlossary of Classes\n\n\nThe list below summarizes all classes of mutations used by ProteinPaint.\n\n\n\n\n\n\n\n\nMutation Class\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMISSENSE\n\n\na substitution variant in the coding region resulting in altered protein coding\n\n\n\n\n\n\nFRAMESHIFT\n\n\nan insertion or deletion variant that alters the protein coding frame\n\n\n\n\n\n\nNONSENSE\n\n\na variant altering protein coding to produce a premature stopgain or stoploss.\n\n\n\n\n\n\nPROTEINDEL\n\n\na deletion resulting in a loss of one or more codons from the product, but not altering the protein coding frame\n\n\n\n\n\n\nPROTEININS\n\n\nan insertion introducing one or more codons into the product, but not altering the protein coding frame\n\n\n\n\n\n\nSPLICE\n\n\na variant near an exon edge that may affect splicing functionality\n\n\n\n\n\n\nSILENT\n\n\na substitution variant in the coding region that does not alter protein coding\n\n\n\n\n\n\nSPLICE_REGION\n\n\na variant in an intron within 10 nt of an exon boundary\n\n\n\n\n\n\nUTR_5\n\n\na variant in the 5' untranslated region\n\n\n\n\n\n\nUTR_3\n\n\na variant in the 3' untranslated region\n\n\n\n\n\n\nEXON\n\n\na variant in the exon of a non-coding RNA\n\n\n\n\n\n\nINTRON\n\n\nan intronic variant\n\n\n\n\n\n\n\n\nGlossary of Origins\n\n\nThe list below summarizes all origins of mutations used by ProteinPaint.\n\n\n\n\n\n\n\n\nMutation Origin\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGermline\n\n\na variant found in a normal sample of a cancer patient.\n\n\n\n\n\n\nSomatic\n\n\na variant found only in a tumor sample.\n\n\n\n\n\n\nRelapse\n\n\na variant that arose in recurrence tumor.\n\n\n\n\n\n\n\n\nAdvanced Customizations\n\n\nThere are several more advanced customizations you can leverage with ProteinPaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our \ndetailed tutorial\n. Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.\n\n\nPeCan Pie\n\n\nPeCan Pie documentation can be found \nhere\n.\n\n\nRequesting Raw Genomics through PeCan\n\n\n\n\nAdd samples to your cart by diagnosis.\n\n\n    \n\n\n\n\nAdd samples to your cart by gene mutation.\n\n\n    \n\n\n\n\nAdd samples to your cart by gene expression.\n\n\n    \n\n\n\n\n\n\nClicking \nSubmit to SJCloud\n from the PeCan checkout window will land you back in the Data Browser with your checked out data selected.",
            "title": "PeCan Data Portal"
        },
        {
            "location": "/guides/portals/pecan/#pecan-data-portal",
            "text": "PeCan provides interactive visualizations of pediatric cancer mutations across various projects at St. Jude Children's Research Hospital and its collaborating institutions.",
            "title": "PeCan Data Portal"
        },
        {
            "location": "/guides/portals/pecan/#homepage",
            "text": "The  PeCan homepage  contains two main visualizations that work with each other to give a high level overview of the data being presented (SJ Cloud's  PCGP  dataset along with curated datasets from other institutions such as  TARGET ,  dkfz , and others).",
            "title": "Homepage"
        },
        {
            "location": "/guides/portals/pecan/#donut-chart",
            "text": "The donut chart (shown below) gives an at-a-glance disease distribution and disease hierarchy.    You can hover over the various donut slices to glance at the number (and %) of samples being represented by that disease. The diseases are categorized in two three main root categories: 1) HM -Hematopoietic Malignancies, 2) BT -Brain Tumor, 3) ST -Solid Tumor.  Click here  for a full mapping of disease codes.",
            "title": "Donut Chart"
        },
        {
            "location": "/guides/portals/pecan/#bubble-chart",
            "text": "Any slice (at any level) of the donut chart can be clicked on to select it, and reveal a bubble chart of related genes.   Note that the dataset bar (shown below) on top of the bubble chart visualizes the distribution of selected data across the datasets used in this visualization. It will update dynamically as you interact with the donut chart and make different selections.   An example of the bubble chart is shown below.   You can see the selected disease shown at the top (1). The bubbles represent the most prevalent genes in the selected disease sample set. The size of the bubble corresponds to the number of mutations in the set with that gene.  For some disease sets (like the one shown above), we have identified the most important disease pathway for the gene and have categorized them as such. This information is represented here via the use of colors. The legend at the bottom allows you to view the pathway information being shown (including the number of genes that are attached to each pathway).  Hovering over a pathway in the legend will highlight all matching genes. Clicking a gene will open it's ProteinPaint.",
            "title": "Bubble Chart"
        },
        {
            "location": "/guides/portals/pecan/#proteinpaint",
            "text": "ProteinPaint  is a web application for simultaneously visualizing genetic lesions (including sequence mutations and gene fusions) and RNA expression in pediatric cancers. You can find the ProteinPaint  paper here .",
            "title": "ProteinPaint"
        },
        {
            "location": "/guides/portals/pecan/#overview",
            "text": "The image below shows an example ProteinPaint of the gene  TP53  annotated with descriptions of the many interactive elements of a ProteinPaint visualization. As you can see, there is a lot to explore.",
            "title": "Overview"
        },
        {
            "location": "/guides/portals/pecan/#glossary-of-classes",
            "text": "The list below summarizes all classes of mutations used by ProteinPaint.     Mutation Class  Description      MISSENSE  a substitution variant in the coding region resulting in altered protein coding    FRAMESHIFT  an insertion or deletion variant that alters the protein coding frame    NONSENSE  a variant altering protein coding to produce a premature stopgain or stoploss.    PROTEINDEL  a deletion resulting in a loss of one or more codons from the product, but not altering the protein coding frame    PROTEININS  an insertion introducing one or more codons into the product, but not altering the protein coding frame    SPLICE  a variant near an exon edge that may affect splicing functionality    SILENT  a substitution variant in the coding region that does not alter protein coding    SPLICE_REGION  a variant in an intron within 10 nt of an exon boundary    UTR_5  a variant in the 5' untranslated region    UTR_3  a variant in the 3' untranslated region    EXON  a variant in the exon of a non-coding RNA    INTRON  an intronic variant",
            "title": "Glossary of Classes"
        },
        {
            "location": "/guides/portals/pecan/#glossary-of-origins",
            "text": "The list below summarizes all origins of mutations used by ProteinPaint.     Mutation Origin  Description      Germline  a variant found in a normal sample of a cancer patient.    Somatic  a variant found only in a tumor sample.    Relapse  a variant that arose in recurrence tumor.",
            "title": "Glossary of Origins"
        },
        {
            "location": "/guides/portals/pecan/#advanced-customizations",
            "text": "There are several more advanced customizations you can leverage with ProteinPaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our  detailed tutorial . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.",
            "title": "Advanced Customizations"
        },
        {
            "location": "/guides/portals/pecan/#pecan-pie",
            "text": "PeCan Pie documentation can be found  here .",
            "title": "PeCan Pie"
        },
        {
            "location": "/guides/portals/pecan/#requesting-raw-genomics-through-pecan",
            "text": "Add samples to your cart by diagnosis. \n       Add samples to your cart by gene mutation. \n       Add samples to your cart by gene expression. \n        Clicking  Submit to SJCloud  from the PeCan checkout window will land you back in the Data Browser with your checked out data selected.",
            "title": "Requesting Raw Genomics through PeCan"
        },
        {
            "location": "/guides/portals/genome-paint/",
            "text": "Genome Paint\n\n\nGenomePaint\n is a visualization browser for simultaneously viewing genomic, transcriptomic, and epigenomic pediatric cancer mutation datasets across a multitude of disease cohorts. GenomePaint datasets include WGS, WES, RNA-Seq, SNP-chip, ChIP-Seq, and Hi-C data visualized over the hg19 reference genome. You can use GenomePaint to interpret the impact of somatic coding and noncoding alterations from ~3,800 pediatric tumors, make novel discoveries through visual exploration, and create publication ready figures!\n\n\nGetting Started\n\n\nThe GenomePaint browser homepage lands on a dense cohort view of the TAL1 gene region of chromosome 1. Each section of the display can be interacted with by clicking, dragging, or hovering. Filter the information displayed on the tracks by clicking buttons in the legend. Customize the legend display by hiding/showing classes. Click \nCONFIG\n to the right of tracks for additional display customization.\n\n\n\n\nTo navigate tracks,\n\n\n\n\nPan left or right by clicking on the middle part of the track and dragging\n\n\nZoom in by dragging the genomic coordinate ruler on top or zoom in 1 fold by clicking on the \nIN\n button\n\n\nZoom out by \nx\n fold by clicking on an \nOUT\n button.\n\n\n\n\n\n\nYou can even zoom in to display mutations at bp resolution. \n\n\n\n\nGenome Paint offers three different views: cohort view, sample view, and matrix view. The figure below summarizes how each view is connected.\n\n\n\n\nCohort View\n\n\nThe cohort view shows mutations from all samples over a genomic region, along with the gene expression ranks for each of the samples. By default the mutation track displays the cohort view in dense mode, a compact display showing density plots for SVs and SNV/indels. You can toggle the view to expanded mode by clicking the \nCONFIG\n button to the right of the mutation track and then clicking Expanded.\n\n\n\n\nIn Expanded mode (see below) all types of mutations are shown for each sample, one row per sample. Circles represent SV/fusion breakpoints, and x marks represent SNV/indels, each of which are displayed together with CNV/LOH. SNV/indels and breakpoints are always shown on top of CNV and LOH. Text labels can be shown for SV/fusion/SNV/indel, if available.\n\n\n\n\nSample View\n\n\nThe sample view shows mutations for one sample alone, along with any available genomic assay tracks. You can open a sample view from the expanded cohort view by clicking on any type of single mutation within the sample, and then selecting \nFocus\n. This brings up a new browser view showing data tracks from this sample in the region surrounding the mutation.\n\n\n\n\nOn the sample view you can explore expression rank, tumor mutations, structural variants, splice junctions, WES coverage, and RNA-Seq coverage. Customize the display by zooming in/out, hiding and/or rearranging tracks, or editing \nCONFIG\n options.\n\n\nMatrix View\n\n\nThe matrix view combines the mutation profiles of multiple genomic regions in one view, in the form of a sample-by-region matrix. Such a matrix can be generated for samples from one cancer type. To open a matrix view, select a disease cohort from the cohort view and then select Matrix view. This organizes the selected cohort tumors with mutations in the genomic region you are viewing into a single-column matrix.\n\n\n\n\nNext, go back to the cohort view and type another gene or region of interest into the genome coordinate box and press ENTER. The cohort view will now show data at the new genomic location. Click on the same disease cohort and then select Matrix view.\n\n\n\n\nThis will add the new genomic variant as a second column in the matrix. You can continue adding columns to this matrix in the same manner.\n\n\nAdvanced Customizations\n\n\nThere are several more advanced customizations you can leverage with GenomePaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our \ndetailed tutorial\n. Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.",
            "title": "Genome Paint"
        },
        {
            "location": "/guides/portals/genome-paint/#genome-paint",
            "text": "GenomePaint  is a visualization browser for simultaneously viewing genomic, transcriptomic, and epigenomic pediatric cancer mutation datasets across a multitude of disease cohorts. GenomePaint datasets include WGS, WES, RNA-Seq, SNP-chip, ChIP-Seq, and Hi-C data visualized over the hg19 reference genome. You can use GenomePaint to interpret the impact of somatic coding and noncoding alterations from ~3,800 pediatric tumors, make novel discoveries through visual exploration, and create publication ready figures!",
            "title": "Genome Paint"
        },
        {
            "location": "/guides/portals/genome-paint/#getting-started",
            "text": "The GenomePaint browser homepage lands on a dense cohort view of the TAL1 gene region of chromosome 1. Each section of the display can be interacted with by clicking, dragging, or hovering. Filter the information displayed on the tracks by clicking buttons in the legend. Customize the legend display by hiding/showing classes. Click  CONFIG  to the right of tracks for additional display customization.   To navigate tracks,   Pan left or right by clicking on the middle part of the track and dragging  Zoom in by dragging the genomic coordinate ruler on top or zoom in 1 fold by clicking on the  IN  button  Zoom out by  x  fold by clicking on an  OUT  button.    You can even zoom in to display mutations at bp resolution.    Genome Paint offers three different views: cohort view, sample view, and matrix view. The figure below summarizes how each view is connected.",
            "title": "Getting Started"
        },
        {
            "location": "/guides/portals/genome-paint/#cohort-view",
            "text": "The cohort view shows mutations from all samples over a genomic region, along with the gene expression ranks for each of the samples. By default the mutation track displays the cohort view in dense mode, a compact display showing density plots for SVs and SNV/indels. You can toggle the view to expanded mode by clicking the  CONFIG  button to the right of the mutation track and then clicking Expanded.   In Expanded mode (see below) all types of mutations are shown for each sample, one row per sample. Circles represent SV/fusion breakpoints, and x marks represent SNV/indels, each of which are displayed together with CNV/LOH. SNV/indels and breakpoints are always shown on top of CNV and LOH. Text labels can be shown for SV/fusion/SNV/indel, if available.",
            "title": "Cohort View"
        },
        {
            "location": "/guides/portals/genome-paint/#sample-view",
            "text": "The sample view shows mutations for one sample alone, along with any available genomic assay tracks. You can open a sample view from the expanded cohort view by clicking on any type of single mutation within the sample, and then selecting  Focus . This brings up a new browser view showing data tracks from this sample in the region surrounding the mutation.   On the sample view you can explore expression rank, tumor mutations, structural variants, splice junctions, WES coverage, and RNA-Seq coverage. Customize the display by zooming in/out, hiding and/or rearranging tracks, or editing  CONFIG  options.",
            "title": "Sample View"
        },
        {
            "location": "/guides/portals/genome-paint/#matrix-view",
            "text": "The matrix view combines the mutation profiles of multiple genomic regions in one view, in the form of a sample-by-region matrix. Such a matrix can be generated for samples from one cancer type. To open a matrix view, select a disease cohort from the cohort view and then select Matrix view. This organizes the selected cohort tumors with mutations in the genomic region you are viewing into a single-column matrix.   Next, go back to the cohort view and type another gene or region of interest into the genome coordinate box and press ENTER. The cohort view will now show data at the new genomic location. Click on the same disease cohort and then select Matrix view.   This will add the new genomic variant as a second column in the matrix. You can continue adding columns to this matrix in the same manner.",
            "title": "Matrix View"
        },
        {
            "location": "/guides/portals/genome-paint/#advanced-customizations",
            "text": "There are several more advanced customizations you can leverage with GenomePaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our  detailed tutorial . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.",
            "title": "Advanced Customizations"
        },
        {
            "location": "/guides/portals/sickle-cell/",
            "text": "The Sickle Cell Genomics Portal contains two viewers for the exploration of data from the  \nSickle Cell Genome Project (SGP)\n dataset.\n\n\nGenome Browser\n\n\nOverview\n\n\nUpon launching the browser, you will see an image similar to the one shown here.\n\n\n\nA description of the elements of the browser are as follows:\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNavigation tools and track selector. (\nSee Navigation Buttons section\n)\n\n\n\n\n\n\n2\n\n\nDNase hypersensitivity tracks.  By default, four epigenetic tracks are shown.  These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells.  Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below)\n\n\n\n\n\n\n3\n\n\nRefSeq genes.  Gene models from the RefSeq database are displayed in this tracks.\n\n\n\n\n\n\n4\n\n\n-log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease.  The analysis has only been performed around the  KIAA1109/Tenr/IL2/IL21 region.   Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)).  The Y-axis for the track represents the -log10 of the p-value.  The higher the value, the more statistically significant the association between the variant and pain rate is.  Clicking on a variant will open op a window that gives further details about the variant.  (See Figure 3).\n\n\n\n\n\n\n5\n\n\n-log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease.  See (4) above for more information on this type of track.\n\n\n\n\n\n\n6\n\n\nFilters: Filters allow variants within tracks to be filtered by numerous citeria.  \nSee Filter description\n\n\n\n\n\n\n\n\nNavigation buttons\n\n\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na\n\n\nLocation/Locus entry field.  One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID.\n\n\n\n\n\n\nb\n\n\nBrowser zoom in and out\n\n\n\n\n\n\nc\n\n\nTracks: Add or hide tracks (See section below on adding/hiding tracks)\n\n\n\n\n\n\nd\n\n\nMore:  Save svg image of browser, get DNA sequence or highlight regions of the browser.\n\n\n\n\n\n\n\n\nFilters\n\n\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na\n\n\nFilters for pain rate p-value track\n\n\n\n\n\n\nb\n\n\nFilters for age of first vaso-occlusive crisis (VOC) p-value\n\n\n\n\n\n\nc\n\n\nThe highlighted filter shows which value is  used for the Y-axis on the browser track.  The value can be changed.\n\n\n\n\n\n\nd\n\n\nA highlighted value within a filter shows which filter value is set.  The number next to the filter represents the number of individuals that meet the filter criteria.\n\n\n\n\n\n\n\n\nGetting Started\n\n\nFinding a variant of interest\n\n\nA user can navigate to a gene or to a variant ID.\nEnter in the variant ID rs13140464 into the search text field at the top of the browser. (See below)\n\n\n\nPressing enter will center the browser of the selected variant.  (see below)\n\n\n\nZooming in and out\n\n\nOne can use the buttons next to the search field to zoom in and out along the genome.  Press the x50 button to zoom out 50 fold\n\n\n\nThis will show a larger region of the chromosome.\n\n\n\nOne can now see three DNase peaks (1) around the rs13140464 variant(2).  In addition there is another variant (3) seen near one of the DNase peaks.\n\n\n\nObtaining additional variant information\n\n\nLeft clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.\n\n\n\nAdding and removing tracks\n\n\nSelect the tracks button from the top of the genome browser.\n\n\nA window displaying selected tracks and tracks available for selection will pop up.\n\n\nOne can scroll down to see additional tracks.  Try selecting and unselecting various tracks and observe the updated tracks on the browser.\n\n\nGetting DNA sequence\n\n\nSelect the 'More' button at the top of the browser.\n\n\nSeveral options will be available.  Select the DNA sequence button.\n\n\nYou will be shown the DNA sequence for the region.\n\n\n\nVariants and Phenotype Viewer\n\n\nOverview\n\n\nWhen the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization.\n\n\nThe different elements of the view are as follows.\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nSettings and sort buttons.  In addition a link to this help document.\n\n\n\n\n\n\n2\n\n\nLegend for different tracks in the viewer\n\n\n\n\n\n\n3\n\n\nPhenotypic data displayed with an individual represented in each column\n\n\n\n\n\n\n4\n\n\nGenotypic data displayed with an individual represented in each column\n\n\n\n\n\n\n\n\nLabels\n\n\nSee glossary for further details\n\n\n\n\n\n\n\n\n#\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHb\n\n\nHemoglobin\n\n\n\n\n\n\nHbF\n\n\nFetal Hemoglobin\n\n\n\n\n\n\nHbA2\n\n\nVariant of hemoglobin that contains two alpha subunits and two delta subunits\n\n\n\n\n\n\nMCV\n\n\nMean corpuscular  volume. This is the average size of red blood cells\n\n\n\n\n\n\nPainRate\n\n\nNumber of hospitalizations per year over a two year period.\n\n\n\n\n\n\nSickle cell genotype\n\n\n(SS_Genotype in legend.  Whether patient is SS or SB0\n\n\n\n\n\n\nAlpha deletion\n\n\nWhether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles)\n\n\n\n\n\n\nrs######\n\n\nSeveral variants that we have found to be associated with pain in Sickle Cell Disease\n\n\n\n\n\n\n\n\nGetting Started\n\n\nSorting\n\n\nHover your mouse over the MCV label in the graph.  A box will pop up with several icons.  Select the triangle that is pointed to the left to sort individuals by MCV.\n\n\n\nThe following graph shows individuals sorted by MCV.  Blank columns represent no data available.  Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.\n\n\n\nView values for all patients\n\n\nHovering over one column will enable the viewing of all phenotypic values for that patient.\n\n\nUndo\n\n\nWhile exploring the data, one may inadvertently sort or remove data.  One can undo the changes by selecting the undo button at the top of the viewer.  The redo button will revert the undo.\n\n\n\nGlossary\n\n\n\n\n\n\n\n\nFetal hemoglobin (HbF)\n\nFetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subunits of beta-globin and two units of alpha-globin. \n\n\n\n\n\n\nHeriditary persistence of fetal hemoglobin (HPFH)\n\nIndividuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease.\n\n\n\n\n\n\nPrincipal Component Analysis (PCA)\n\nA \nmethod\n for reducing high dimensional data into low-dimensional representations.\n\n\n\n\n\n\nSC\n\nAn individual with one copy of the sickle cell allele \nrs334\n and one copy of \nhemoglobin C\n.\n\n\n\n\n\n\nS\u03b2\n+\n\nAn individual with \nbeta-thalassemia\n who has one copy of the sickle cell allele \nrs334\n and one copy of a beta-globin gene that has reduced expression.\n\n\n\n\n\n\nS\u03b2\n0\n\nAn individual with \nbeta-thalassemia\n who has one copy of the sickle cell allele \nrs334\n and one copy of a beta-globin gene that is not expressed or is deleted.\n\n\n\n\n\n\n\nSS\n \nAn individual with \nsickle cell disease\n who is homozygous for the sickle cell allele \nrs334\n.\n\n\n\n\n\n\nSCCRIP\n\nThe \nSickle Cell Research and Intervention Program\n.",
            "title": "Sickle Cell Genomics Portal"
        },
        {
            "location": "/guides/portals/sickle-cell/#genome-browser",
            "text": "",
            "title": "Genome Browser"
        },
        {
            "location": "/guides/portals/sickle-cell/#overview",
            "text": "Upon launching the browser, you will see an image similar to the one shown here.  A description of the elements of the browser are as follows:     #  Description      1  Navigation tools and track selector. ( See Navigation Buttons section )    2  DNase hypersensitivity tracks.  By default, four epigenetic tracks are shown.  These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells.  Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below)    3  RefSeq genes.  Gene models from the RefSeq database are displayed in this tracks.    4  -log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease.  The analysis has only been performed around the  KIAA1109/Tenr/IL2/IL21 region.   Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)).  The Y-axis for the track represents the -log10 of the p-value.  The higher the value, the more statistically significant the association between the variant and pain rate is.  Clicking on a variant will open op a window that gives further details about the variant.  (See Figure 3).    5  -log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease.  See (4) above for more information on this type of track.    6  Filters: Filters allow variants within tracks to be filtered by numerous citeria.   See Filter description",
            "title": "Overview"
        },
        {
            "location": "/guides/portals/sickle-cell/#navigation-buttons",
            "text": "#  Description      a  Location/Locus entry field.  One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID.    b  Browser zoom in and out    c  Tracks: Add or hide tracks (See section below on adding/hiding tracks)    d  More:  Save svg image of browser, get DNA sequence or highlight regions of the browser.",
            "title": "Navigation buttons"
        },
        {
            "location": "/guides/portals/sickle-cell/#filters",
            "text": "#  Description      a  Filters for pain rate p-value track    b  Filters for age of first vaso-occlusive crisis (VOC) p-value    c  The highlighted filter shows which value is  used for the Y-axis on the browser track.  The value can be changed.    d  A highlighted value within a filter shows which filter value is set.  The number next to the filter represents the number of individuals that meet the filter criteria.",
            "title": "Filters"
        },
        {
            "location": "/guides/portals/sickle-cell/#getting-started",
            "text": "",
            "title": "Getting Started"
        },
        {
            "location": "/guides/portals/sickle-cell/#finding-a-variant-of-interest",
            "text": "A user can navigate to a gene or to a variant ID.\nEnter in the variant ID rs13140464 into the search text field at the top of the browser. (See below)  Pressing enter will center the browser of the selected variant.  (see below)",
            "title": "Finding a variant of interest"
        },
        {
            "location": "/guides/portals/sickle-cell/#zooming-in-and-out",
            "text": "One can use the buttons next to the search field to zoom in and out along the genome.  Press the x50 button to zoom out 50 fold  This will show a larger region of the chromosome.  One can now see three DNase peaks (1) around the rs13140464 variant(2).  In addition there is another variant (3) seen near one of the DNase peaks.",
            "title": "Zooming in and out"
        },
        {
            "location": "/guides/portals/sickle-cell/#obtaining-additional-variant-information",
            "text": "Left clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.",
            "title": "Obtaining additional variant information"
        },
        {
            "location": "/guides/portals/sickle-cell/#adding-and-removing-tracks",
            "text": "Select the tracks button from the top of the genome browser. \nA window displaying selected tracks and tracks available for selection will pop up. \nOne can scroll down to see additional tracks.  Try selecting and unselecting various tracks and observe the updated tracks on the browser.",
            "title": "Adding and removing tracks"
        },
        {
            "location": "/guides/portals/sickle-cell/#getting-dna-sequence",
            "text": "Select the 'More' button at the top of the browser. \nSeveral options will be available.  Select the DNA sequence button. \nYou will be shown the DNA sequence for the region.",
            "title": "Getting DNA sequence"
        },
        {
            "location": "/guides/portals/sickle-cell/#variants-and-phenotype-viewer",
            "text": "",
            "title": "Variants and Phenotype Viewer"
        },
        {
            "location": "/guides/portals/sickle-cell/#overview_1",
            "text": "When the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization. \nThe different elements of the view are as follows.     #  Description      1  Settings and sort buttons.  In addition a link to this help document.    2  Legend for different tracks in the viewer    3  Phenotypic data displayed with an individual represented in each column    4  Genotypic data displayed with an individual represented in each column",
            "title": "Overview"
        },
        {
            "location": "/guides/portals/sickle-cell/#labels",
            "text": "See glossary for further details     #  Description      Hb  Hemoglobin    HbF  Fetal Hemoglobin    HbA2  Variant of hemoglobin that contains two alpha subunits and two delta subunits    MCV  Mean corpuscular  volume. This is the average size of red blood cells    PainRate  Number of hospitalizations per year over a two year period.    Sickle cell genotype  (SS_Genotype in legend.  Whether patient is SS or SB0    Alpha deletion  Whether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles)    rs######  Several variants that we have found to be associated with pain in Sickle Cell Disease",
            "title": "Labels"
        },
        {
            "location": "/guides/portals/sickle-cell/#getting-started_1",
            "text": "",
            "title": "Getting Started"
        },
        {
            "location": "/guides/portals/sickle-cell/#sorting",
            "text": "Hover your mouse over the MCV label in the graph.  A box will pop up with several icons.  Select the triangle that is pointed to the left to sort individuals by MCV.  The following graph shows individuals sorted by MCV.  Blank columns represent no data available.  Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.",
            "title": "Sorting"
        },
        {
            "location": "/guides/portals/sickle-cell/#view-values-for-all-patients",
            "text": "Hovering over one column will enable the viewing of all phenotypic values for that patient.",
            "title": "View values for all patients"
        },
        {
            "location": "/guides/portals/sickle-cell/#undo",
            "text": "While exploring the data, one may inadvertently sort or remove data.  One can undo the changes by selecting the undo button at the top of the viewer.  The redo button will revert the undo.",
            "title": "Undo"
        },
        {
            "location": "/guides/portals/sickle-cell/#glossary",
            "text": "Fetal hemoglobin (HbF) \nFetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subunits of beta-globin and two units of alpha-globin.     Heriditary persistence of fetal hemoglobin (HPFH) \nIndividuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease.    Principal Component Analysis (PCA) \nA  method  for reducing high dimensional data into low-dimensional representations.    SC \nAn individual with one copy of the sickle cell allele  rs334  and one copy of  hemoglobin C .    S\u03b2 + \nAn individual with  beta-thalassemia  who has one copy of the sickle cell allele  rs334  and one copy of a beta-globin gene that has reduced expression.    S\u03b2 0 \nAn individual with  beta-thalassemia  who has one copy of the sickle cell allele  rs334  and one copy of a beta-globin gene that is not expressed or is deleted.    SS  \nAn individual with  sickle cell disease  who is homozygous for the sickle cell allele  rs334 .    SCCRIP \nThe  Sickle Cell Research and Intervention Program .",
            "title": "Glossary"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/",
            "text": "Filling Out The Data Access Agreement\n\n\nThe \nData Access Agreement (DAA)\n is a legal document used by St. Jude Cloud to verify the identity and intent of those requesting to access St. Jude Children\u2019s Research Hospital\u2019s genomics data. The document binds you and your institution in agreement to protect, use and share the data appropriately. \n\n\nUpon selection of your desired data, you will be prompted to complete a Data Access Agreement if you have not already been approved for access to the selected datasets. In order to simplify the data access request process, an \nelectronic data access agreement\n is available for US residents only.\n\n\nIf you reside outside of the US, you must fill out the Data Access Agreement manually. You may click \nhere\n to download the latest version of the DAA. Please read the first 6 pages carefully, which consist of terms and conditions that you and your institution must agree to in order to access any data on St. Jude Cloud. Then, follow the directions starting at \nData Access Units\n to ensure that you have correctly filled out the DAA. Please note that there are \ntwo additional required sections\n if you intend to download data to your local infrastructure.\n\n\nThe Data Access Agreement\n\n\nDownloading Data\n\n\nIf and only if\n you wish to download the genomic data locally, you must have the \nPrincipal Investigator\n initial on page 7 and have the \nInformation Security Officer\n sign on page 13. If filling out the Data Access Agreement electronically, you must select the option to download within the setup wizard and input the contact information of your institution\u2019s Information Security Officer. \n\n\nData Access Units\n\n\nA St. Jude Cloud \nData Access Unit (DAU)\n is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. To learn more, see our section on \nData Access Units\n.\n\n\nOn page 7 of the DAA, you must mark all Data Access Units for which you are applying. The DAU(s) associated to the data you requested are listed in the Controlled Access Data section, directly above the Download Data Access Agreement button. This can be found on the Request Data webpage which immediately follows your selection of data from the data browser. \n\n\n\n\nIf you mark the incorrect datasets, you will be required to resubmit your agreement with the correct datasets marked. When completing the DAA electronically using the setup wizard, the DAU options will be automatically preselected for you. \n\n\nContemplated Use\n\n\nOn page 8 of the DAA, you must submit a description of your research project. Please specifically describe the intended role of St. Jude\u2019s data in your research project. Your contemplated use can be anywhere from a paragraph to a few pages long, although a typical contemplated use is 1-2 paragraphs. Each \nData Access Committee\n will evaluate your contemplated use case and decide whether to approve or deny your application for data access based on their own set of protocols. Please \ncontact us\n if you have any questions regarding the protocols of the approval process.\n\n\n\n\nContemplated Use Example\n\n\nThe below is a simulated example of a contemplated use.\n\n\n\n\n\"We propose to apply our own variant calling method to detect structural variants in both pediatric tumor and germline samples.  The detected variants will then be compared to our own genomic data and to publicly available data sources of normal and disease samples to assess the novelty of those variants and their association with disease.  Variants will be categorized by their expected effect on genes known to be relevant to cancer, DNA repair, or epigenetics.   One goal of the research is to compare our mutation detection software to existing methods on samples with known mutations and structural variants such as those in the PCGP data set.   Another goal is the identification of novel variants with potential clinical relevance.  Promising tractable variants will be introduced into cell lines to observe their effect on cell growth and tumor progression.   Lastly, we propose to use the results of our analyses to further develop and refine our variant detection methods.\"\n\n\n\n\nPrincipal Investigator\n\n\nOn page 9 of the DAA, the Principal Investigator of the research project must input their information and sign the agreement. Typically the PI signee is the faculty-level supervisor on the project, but it is not a requirement.\n\n\nWho may qualify as a Principal Investigator (PI)?\n\nThe PI is designated by the grantee organization to direct the project or activity being supported by the grant. The PI is responsible and accountable to the grantee for the proper conduct of the project or activity. The role of the PI within the eRA Commons is to complete the grant process, either by completing the required forms via the eRA Commons or by delegating this responsibility to another individual. A PI can access information for any grant for which they are designated the PI. See eRA Commons Roles & Privileges Matrix. \n  \n\n\nAdditional Applicants\n\n\nPages 10 and 11 of the DAA must be signed by any additional person(s) who will have access to the data. Additional applicants may include those working on your project, those working in your lab, or those who have access to where the data will be stored. These individuals will be legally bound to protecting and handling the data properly. Pages 10 and 11 may be duplicated and added to the agreement to accommodate for more than 8 additional applicants. When filling out the DAA electronically, you may only include up to 8 additional applicants on your agreement.\n\n\n\nInstitutional Authority\n\n\nPage 12 of the agreement must be filled out and signed by your Institutional or Administrative Authority. The institutional authority is an individual who has the authority to sign for a grant application. This individual cannot be the same as the Principal Investigator that signed on page 9, as this additional signature provides a second-party authority of the institution to ensure that the institution will uphold the terms of this agreement.\n    \n\n\nInformation Security Officer\n\n\nOn page 13 of the DAA, your institution\u2019s Information Security Officer's signature is required \nif and only if\n you intend to \ndownload\n a local copy of the data (note: you must also initial in the line below the DAU selection). This individual may go by varying job titles, such as Information Director or Chief Information Security Officer, but is the individual responsible for information security at your institution. This signature verifies that the data, once downloaded, will remain protected by your institution's data security protocols.\n    \n\n\nUploading A Revised DAA\n\n\nIf your DAA is incomplete (for example you missed a required signature or neglected to check the box next to a dataset for which you requested data), you will be contacted by a member of the St. Jude Cloud team. Once you have made the required edits, you can reupload a revised DAA through the \nManage Data page\n by clicking on the \nAdd a Form\n button.\n\n\n\n\nThe Electronic Data Access Agreement Process\n\n\nUsers who live in the United States will be given the option to complete the Data Access Agreement electronically or manually. If you elect to complete it electronically, the setup wizard will request information about you, your institution and your associates. You will also be asked to provide a detailed description of the research project in which the data you have requested will be used. \n\n\nAfter submitting the required information, the Data Access Agreement will be sent via email to each individual entered through the setup wizard. Each individual must follow the link in their email to sign the appropriate page of the agreement via DocuSign. If any individual rejects or declines to sign the agreement, you will need to start a new data request from the data browser. \n\n\nOnce all signatures have been collected, your data access request will be submitted to St. Jude Cloud. A St. Jude Cloud administrator will forward your Data Access Agreement and intended use to the appropriate \nData Access Committee(s)\n for approval. Upon approval from the DAC(s), you will receive an email with a link to the approved data, which will be hosted through DNAnexus.  \n\n\nLearn how to \ncheck your Request Status\n  \n\n\nSimilar Topics\n\n\nStudies\n\n\nMaking a Data Request\n\n\nRenewing Your Data Access\n\n\nDownloading Data",
            "title": "Filling out a Data Access Agreement"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#filling-out-the-data-access-agreement",
            "text": "The  Data Access Agreement (DAA)  is a legal document used by St. Jude Cloud to verify the identity and intent of those requesting to access St. Jude Children\u2019s Research Hospital\u2019s genomics data. The document binds you and your institution in agreement to protect, use and share the data appropriately.   Upon selection of your desired data, you will be prompted to complete a Data Access Agreement if you have not already been approved for access to the selected datasets. In order to simplify the data access request process, an  electronic data access agreement  is available for US residents only.  If you reside outside of the US, you must fill out the Data Access Agreement manually. You may click  here  to download the latest version of the DAA. Please read the first 6 pages carefully, which consist of terms and conditions that you and your institution must agree to in order to access any data on St. Jude Cloud. Then, follow the directions starting at  Data Access Units  to ensure that you have correctly filled out the DAA. Please note that there are  two additional required sections  if you intend to download data to your local infrastructure.",
            "title": "Filling Out The Data Access Agreement"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#the-data-access-agreement",
            "text": "",
            "title": "The Data Access Agreement"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#downloading-data",
            "text": "If and only if  you wish to download the genomic data locally, you must have the  Principal Investigator  initial on page 7 and have the  Information Security Officer  sign on page 13. If filling out the Data Access Agreement electronically, you must select the option to download within the setup wizard and input the contact information of your institution\u2019s Information Security Officer.",
            "title": "Downloading Data"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#data-access-units",
            "text": "A St. Jude Cloud  Data Access Unit (DAU)  is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. To learn more, see our section on  Data Access Units .  On page 7 of the DAA, you must mark all Data Access Units for which you are applying. The DAU(s) associated to the data you requested are listed in the Controlled Access Data section, directly above the Download Data Access Agreement button. This can be found on the Request Data webpage which immediately follows your selection of data from the data browser.    If you mark the incorrect datasets, you will be required to resubmit your agreement with the correct datasets marked. When completing the DAA electronically using the setup wizard, the DAU options will be automatically preselected for you.",
            "title": "Data Access Units"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#contemplated-use",
            "text": "On page 8 of the DAA, you must submit a description of your research project. Please specifically describe the intended role of St. Jude\u2019s data in your research project. Your contemplated use can be anywhere from a paragraph to a few pages long, although a typical contemplated use is 1-2 paragraphs. Each  Data Access Committee  will evaluate your contemplated use case and decide whether to approve or deny your application for data access based on their own set of protocols. Please  contact us  if you have any questions regarding the protocols of the approval process.",
            "title": "Contemplated Use"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#contemplated-use-example",
            "text": "The below is a simulated example of a contemplated use.   \"We propose to apply our own variant calling method to detect structural variants in both pediatric tumor and germline samples.  The detected variants will then be compared to our own genomic data and to publicly available data sources of normal and disease samples to assess the novelty of those variants and their association with disease.  Variants will be categorized by their expected effect on genes known to be relevant to cancer, DNA repair, or epigenetics.   One goal of the research is to compare our mutation detection software to existing methods on samples with known mutations and structural variants such as those in the PCGP data set.   Another goal is the identification of novel variants with potential clinical relevance.  Promising tractable variants will be introduced into cell lines to observe their effect on cell growth and tumor progression.   Lastly, we propose to use the results of our analyses to further develop and refine our variant detection methods.\"",
            "title": "Contemplated Use Example"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#principal-investigator",
            "text": "On page 9 of the DAA, the Principal Investigator of the research project must input their information and sign the agreement. Typically the PI signee is the faculty-level supervisor on the project, but it is not a requirement.  Who may qualify as a Principal Investigator (PI)? \nThe PI is designated by the grantee organization to direct the project or activity being supported by the grant. The PI is responsible and accountable to the grantee for the proper conduct of the project or activity. The role of the PI within the eRA Commons is to complete the grant process, either by completing the required forms via the eRA Commons or by delegating this responsibility to another individual. A PI can access information for any grant for which they are designated the PI. See eRA Commons Roles & Privileges Matrix.",
            "title": "Principal Investigator"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#additional-applicants",
            "text": "Pages 10 and 11 of the DAA must be signed by any additional person(s) who will have access to the data. Additional applicants may include those working on your project, those working in your lab, or those who have access to where the data will be stored. These individuals will be legally bound to protecting and handling the data properly. Pages 10 and 11 may be duplicated and added to the agreement to accommodate for more than 8 additional applicants. When filling out the DAA electronically, you may only include up to 8 additional applicants on your agreement.",
            "title": "Additional Applicants"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#institutional-authority",
            "text": "Page 12 of the agreement must be filled out and signed by your Institutional or Administrative Authority. The institutional authority is an individual who has the authority to sign for a grant application. This individual cannot be the same as the Principal Investigator that signed on page 9, as this additional signature provides a second-party authority of the institution to ensure that the institution will uphold the terms of this agreement.",
            "title": "Institutional Authority"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#information-security-officer",
            "text": "On page 13 of the DAA, your institution\u2019s Information Security Officer's signature is required  if and only if  you intend to  download  a local copy of the data (note: you must also initial in the line below the DAU selection). This individual may go by varying job titles, such as Information Director or Chief Information Security Officer, but is the individual responsible for information security at your institution. This signature verifies that the data, once downloaded, will remain protected by your institution's data security protocols.",
            "title": "Information Security Officer"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#uploading-a-revised-daa",
            "text": "If your DAA is incomplete (for example you missed a required signature or neglected to check the box next to a dataset for which you requested data), you will be contacted by a member of the St. Jude Cloud team. Once you have made the required edits, you can reupload a revised DAA through the  Manage Data page  by clicking on the  Add a Form  button.",
            "title": "Uploading A Revised DAA"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#the-electronic-data-access-agreement-process",
            "text": "Users who live in the United States will be given the option to complete the Data Access Agreement electronically or manually. If you elect to complete it electronically, the setup wizard will request information about you, your institution and your associates. You will also be asked to provide a detailed description of the research project in which the data you have requested will be used.   After submitting the required information, the Data Access Agreement will be sent via email to each individual entered through the setup wizard. Each individual must follow the link in their email to sign the appropriate page of the agreement via DocuSign. If any individual rejects or declines to sign the agreement, you will need to start a new data request from the data browser.   Once all signatures have been collected, your data access request will be submitted to St. Jude Cloud. A St. Jude Cloud administrator will forward your Data Access Agreement and intended use to the appropriate  Data Access Committee(s)  for approval. Upon approval from the DAC(s), you will receive an email with a link to the approved data, which will be hosted through DNAnexus.    Learn how to  check your Request Status",
            "title": "The Electronic Data Access Agreement Process"
        },
        {
            "location": "/guides/forms/how-to-fill-out-DAA/#similar-topics",
            "text": "Studies  Making a Data Request  Renewing Your Data Access  Downloading Data",
            "title": "Similar Topics"
        },
        {
            "location": "/guides/forms/how-to-fill-out-Extension/",
            "text": "Filling Out The Extension Addendum\n\n\nThe St. Jude Cloud \nData Access Agreement (DAA)\n is only valid for one year after the date it was approved. When your DAA is about to expire, you will get an automated email from notifications@stjude.cloud with the name of the DAA that is expiring and a link to the St. Jude Cloud Extension Addendum.\n\n\nIn order to extend your DAA, you must fill out an Extension Addendum. The Extension Addendum will extend the previous agreement for an additional year. Please note that if you do not fill out an Extension Addendum, you will be expected to delete all copies of the data subject to the expiring agreement.\n\n\nFollow the steps below to ensure that you have accurately filled out all sections of the Extension Addendum.\n\n\n\n\n\n\nPage 1\n\n\n\n\nIn the top section, enter the current date on which that the agreement is being filled out, your current institution, and the date on which you signed the expiring DAA or extension.\n\n\n\n\n\n\n\n\nIn the bottom section, enter a date that is one year after the date on which you signed the expiring DAA or extension. This can be found on the page linked from the email notifying you of agreement expiration.\n\n\n\n\n\n\n\n\n\n\nPage 2\n\n\n\n\n\n\nCheck exactly the datasets that you were granted access to by the terms of the original DAA. \nThe datasets checked on the original DAA and any extensions must match.\n If you would like to apply for access to additional datasets, please \nmake a new data request\n for the additional dataset(s).\n\n\n\n\n\n\nThe Principal Investigator (PI) or faculty level supervisor on the project must sign and date the extension. \nThe PI who signed the original DAA must match the PI signing any extensions.\n\n\n\n\n\n\nAll additional applicants (excluding the administrative authority and information security officer) that were included in the original DAA must sign and date the extension.\n\n\n\n\n\n\n\n\n\n\nPage 3\n\n\n\n\n\n\nEnter the name of your current institution. This must match the institution entered on page 1.\n\n\n\n\n\n\nThe Administrative Authority must sign and date the extension. This is usually the same administrative authority as the one on the original DAA.\n\n\n\n\n\n\nThe Information Security Officer must sign and date the extension.\n\n\n\n\n\n\nThe bottom section of page 3 is for St. Jude to sign and date. Do not fill out this section.\n     \n\n\n\n\n\n\n\n\n\n\nPage 4\n\n\n\n\nIf your research question(s) or contemplated use has changed since the original DAA, use this space to provide your updated project description. You may also use this space to explain why you need to extend your agreement. Finally, if the Administrative Authority or Information Security Officer has changed from the original DAA to this extension, please use this space to explain why.\n\n\n\n\n\n\n\n\nOnce you have finished filling out the Extension Addendum, you may upload the completed form on the extension addendum page linked in the notification of expiration email from notifications@stjude.cloud.",
            "title": "Filling out an Extension Addendum"
        },
        {
            "location": "/guides/forms/how-to-fill-out-Extension/#filling-out-the-extension-addendum",
            "text": "The St. Jude Cloud  Data Access Agreement (DAA)  is only valid for one year after the date it was approved. When your DAA is about to expire, you will get an automated email from notifications@stjude.cloud with the name of the DAA that is expiring and a link to the St. Jude Cloud Extension Addendum.  In order to extend your DAA, you must fill out an Extension Addendum. The Extension Addendum will extend the previous agreement for an additional year. Please note that if you do not fill out an Extension Addendum, you will be expected to delete all copies of the data subject to the expiring agreement.  Follow the steps below to ensure that you have accurately filled out all sections of the Extension Addendum.    Page 1   In the top section, enter the current date on which that the agreement is being filled out, your current institution, and the date on which you signed the expiring DAA or extension.     In the bottom section, enter a date that is one year after the date on which you signed the expiring DAA or extension. This can be found on the page linked from the email notifying you of agreement expiration.      Page 2    Check exactly the datasets that you were granted access to by the terms of the original DAA.  The datasets checked on the original DAA and any extensions must match.  If you would like to apply for access to additional datasets, please  make a new data request  for the additional dataset(s).    The Principal Investigator (PI) or faculty level supervisor on the project must sign and date the extension.  The PI who signed the original DAA must match the PI signing any extensions.    All additional applicants (excluding the administrative authority and information security officer) that were included in the original DAA must sign and date the extension.      Page 3    Enter the name of your current institution. This must match the institution entered on page 1.    The Administrative Authority must sign and date the extension. This is usually the same administrative authority as the one on the original DAA.    The Information Security Officer must sign and date the extension.    The bottom section of page 3 is for St. Jude to sign and date. Do not fill out this section.\n           Page 4   If your research question(s) or contemplated use has changed since the original DAA, use this space to provide your updated project description. You may also use this space to explain why you need to extend your agreement. Finally, if the Administrative Authority or Information Security Officer has changed from the original DAA to this extension, please use this space to explain why.     Once you have finished filling out the Extension Addendum, you may upload the completed form on the extension addendum page linked in the notification of expiration email from notifications@stjude.cloud.",
            "title": "Filling Out The Extension Addendum"
        },
        {
            "location": "/citing-stjude-cloud/",
            "text": "Citing St. Jude Cloud\n\n\nWe are currently in progress of preparing the St. Jude Cloud manuscript. Until further notice, please ensure that when St. Jude Cloud or data accessed therein is used in your work, please:\n\n\n\n\n\n\nCite the relevant paper for each of the datasets and/or resources that you used in your study.\n\n\n\n\n\n\nState in the \nResults\n and/or \nMethods\n section that the relevant data and/or resource was obtained from St. Jude Cloud. Example statement:\n\n\n\n\n\"Whole genome sequencing data for relapse tumor samples from 345 pediatric patients were obtained from St. Jude Cloud.\"\n\n\n\n\n\n\n\n\nState in the \nData availability\n section of the manuscript that data and/or resource can be accessed via St. Jude Cloud. Example statement:\n\n\n\n\n\"Whole genome sequencing data for pediatric relapse tumor samples used for analysis in this study were obtained from St. Jude Cloud (https://www.stjude.cloud) \u2013 a publicly accessible pediatric genomic data resource requiring approval for controlled data access.\"\n\n\n\n\n\n\n\n\nDataset Reference Table\n\n\nPlease download the Schedule 1(s) (linked in table below) to find dataset specific wording of acknowledgement(s).\n\n\n\n\n\n\n\n\nSt. Jude Cloud Dataset\n\n\nReference\n\n\n\n\n\n\n\n\n\n\nPediatric Cancer Genome Project (PCGP) dataset\n\n\nPCGP perspectives paper\n and the \nrelevant tumor type paper(s)\n; \nfile_download\n PCGP Schedule 1\n\n\n\n\n\n\nSt. Jude Lifetime (SJLIFE) dataset\n\n\nSJLIFE paper\n; \nfile_download\n SJLIFE Schedule 1\n\n\n\n\n\n\nClinical Genomics (Clinical Pilot, Genomes for Kids, Real-Time Clinical Genomics) dataset\n\n\nClinical Pilot paper\n; \nfile_download\n Clinical Genomics Schedule 1\n\n\n\n\n\n\nSickle Cell Genome Project (SGP) dataset\n\n\npaper in progress; \nfile_download\n SGP Schedule 1\n\n\n\n\n\n\nChildhood Cancer Survivor Study (CCSS) dataset\n\n\nCCSS study design paper\n; \nfile_download\n CCSS Schedule 1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are unsure what dataset(s) the data that you have been vended belongs to, you can find this information in the sj_datasets column of the \nSAMPLE_INFO.txt\n file.\n\n\n\n\n\n\nWarning\n\n\nPublishing using any of the data files \nbefore\n the \nembargo date\n has passed is strictly prohibited as outlined in the \nData Access Agreement (DAA)\n.\n\n\n\n\nResource Reference Table\n\n\n\n\n\n\n\n\nSt. Jude Cloud Resource\n\n\nReference\n\n\n\n\n\n\n\n\n\n\nProteinPaint\n\n\nProteinPaint paper\n\n\n\n\n\n\nGenomePaint\n\n\npaper in progress\n\n\n\n\n\n\nPeCan Pie\n\n\nPeCan Pie paper\n\n\n\n\n\n\nNeoepitopePred\n\n\nNeoepitopePred paper\n\n\n\n\n\n\nChIP-Seq Peak Calling\n\n\nunpublished\n\n\n\n\n\n\nRapid RNA-Seq Fusion Detection\n\n\npaper in progress\n\n\n\n\n\n\nWARDEN\n\n\nunpublished\n\n\n\n\n\n\nMutational Signatures\n\n\nMutational Patterns paper\n\n\n\n\n\n\ncis-x\n\n\npaper in progress\n\n\n\n\n\n\nXenoCP\n\n\npaper in progress\n\n\n\n\n\n\n\n\nContact Us\n\n\nAny questions, comments, or concerns can be directed to our \n\"Contact Us\"\n form or you can email us directly at support@stjude.cloud.",
            "title": "Citing St. Jude Cloud"
        },
        {
            "location": "/citing-stjude-cloud/#citing-st-jude-cloud",
            "text": "We are currently in progress of preparing the St. Jude Cloud manuscript. Until further notice, please ensure that when St. Jude Cloud or data accessed therein is used in your work, please:    Cite the relevant paper for each of the datasets and/or resources that you used in your study.    State in the  Results  and/or  Methods  section that the relevant data and/or resource was obtained from St. Jude Cloud. Example statement:   \"Whole genome sequencing data for relapse tumor samples from 345 pediatric patients were obtained from St. Jude Cloud.\"     State in the  Data availability  section of the manuscript that data and/or resource can be accessed via St. Jude Cloud. Example statement:   \"Whole genome sequencing data for pediatric relapse tumor samples used for analysis in this study were obtained from St. Jude Cloud (https://www.stjude.cloud) \u2013 a publicly accessible pediatric genomic data resource requiring approval for controlled data access.\"",
            "title": "Citing St. Jude Cloud"
        },
        {
            "location": "/citing-stjude-cloud/#dataset-reference-table",
            "text": "Please download the Schedule 1(s) (linked in table below) to find dataset specific wording of acknowledgement(s).     St. Jude Cloud Dataset  Reference      Pediatric Cancer Genome Project (PCGP) dataset  PCGP perspectives paper  and the  relevant tumor type paper(s) ;  file_download  PCGP Schedule 1    St. Jude Lifetime (SJLIFE) dataset  SJLIFE paper ;  file_download  SJLIFE Schedule 1    Clinical Genomics (Clinical Pilot, Genomes for Kids, Real-Time Clinical Genomics) dataset  Clinical Pilot paper ;  file_download  Clinical Genomics Schedule 1    Sickle Cell Genome Project (SGP) dataset  paper in progress;  file_download  SGP Schedule 1    Childhood Cancer Survivor Study (CCSS) dataset  CCSS study design paper ;  file_download  CCSS Schedule 1      Note  If you are unsure what dataset(s) the data that you have been vended belongs to, you can find this information in the sj_datasets column of the  SAMPLE_INFO.txt  file.    Warning  Publishing using any of the data files  before  the  embargo date  has passed is strictly prohibited as outlined in the  Data Access Agreement (DAA) .",
            "title": "Dataset Reference Table"
        },
        {
            "location": "/citing-stjude-cloud/#resource-reference-table",
            "text": "St. Jude Cloud Resource  Reference      ProteinPaint  ProteinPaint paper    GenomePaint  paper in progress    PeCan Pie  PeCan Pie paper    NeoepitopePred  NeoepitopePred paper    ChIP-Seq Peak Calling  unpublished    Rapid RNA-Seq Fusion Detection  paper in progress    WARDEN  unpublished    Mutational Signatures  Mutational Patterns paper    cis-x  paper in progress    XenoCP  paper in progress",
            "title": "Resource Reference Table"
        },
        {
            "location": "/citing-stjude-cloud/#contact-us",
            "text": "Any questions, comments, or concerns can be directed to our  \"Contact Us\"  form or you can email us directly at support@stjude.cloud.",
            "title": "Contact Us"
        },
        {
            "location": "/faq/",
            "text": "Frequently Asked Questions\n\n\nAccount Questions\n\n\nWhere can I find the Terms of Service or the Privacy Policy?\n\n\nHow can I sign up for updates when new data or features are added to the cloud?\n\n\nHow can I delete my account?\n  \n\n\nBilling Questions\n\n\nWill I be charged for using St. Jude Cloud?\n \n\n\nHow can I set up billing for my lab?\n\n\nData Request Questions\n\n\nWhy do I need to sign the Data Access Agreement (DAA)?\n\n\nCan I make edits/revisions to the DAA?\n\n\nCan I get a Microsoft Word version of the DAA\n\n\nWhere can I find the latest version of the Data Access Agreement (DAA)?\n\n\nWhere do I submit the Data Access Agreement (DAA)?\n\n\nWhat if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data?\n\n\nWhat clinical information is available about samples in St. Jude Cloud?\n\n\nCan I get a copy of IRB consent forms?\n\n\nCan I request FASTQ files on St. Jude Cloud?\n  \n\n\nTechnical Questions\n\n\nHow can I explore and manipulate data files stored on the cloud without downloading the files to my local machine?\n\n\nHow can I run an analysis workflow on multiple sample files at the same time?\n\n\nHow can I connect to DNAnexus API via SSH on a Windows machine?\n\n\nWhy am having trouble connecting to DNAnexus API via SSH?\n  \n\n\nPublication Questions\n\n\nHow do I cite St. Jude Cloud?\n\n\nWhen can I publish my findings using St. Jude Cloud data?\n\n\nWhere can I find the embargo date?\n\n\nMiscellaneous\n \n\n\nWill St. Jude Cloud host my institution's data in the data browser or on PeCan?\n  \n\n\nWhere can I find the Terms of Service or the Privacy Policy?\n\n\nYou can find the Terms of Service\n\nhere\n and the Privacy Policy\n\nhere\n.\n\n\nHow can I sign up for updates when new data or features are added to the cloud?\n\n\nWe are always adding data to St. Jude Cloud, and if you would like to sign up for updates, \nsubscribe here\n.\n\n\nHow can I delete my account?\n\n\nIf you'd like to delete your account, please email DNAnexus support at\n\nsupport@dnanexus.com\n with the following email.\n\n\nHi\n \nDNAnexus\n,\n\n\n  \nWould\n \nyou\n \nplease\n \nassist\n \nme\n \nin\n \ndeleting\n \nmy\n \nSt\n.\n \nJude\n \nCloud\n \naccount\n?\n \nMy\n \nusername\n \nis\n \n_____\n.\n\n\n\nThank\n \nyou\n!\n\n\n\n\n\n\nWill I be charged for using St. Jude Cloud?\n\n\nAny copy of the St. Jude data you receive is considered \"sponsored\",\n  so you do not have to pay a fee to store this data in St. Jude\n  Cloud. Although you may be prompted to enter billing information, you will not\nbe charged for anything with the exception of the following actions:\n\n\n\n\nYou will be charged for any \nderivative\n files stored on the St. Jude Cloud, such as results files obtained through running analyses workflows.\n\n\nThere is a small monthly storage fee associated for any of your own data you upload to the cloud.\n\n\nIf you run any of our analysis workflows (such as Rapid RNA-Seq, WARDEN, etc.) or your own workflows that you have uploaded and packaged into the cloud, you will be charged for the\n  compute resources used in producing the results. Soon we hope to be able to sponsor all compute costs associated with running our St. Jude Cloud workflows.\n\n\n\n\nYou can find DNAnexus's specific resource-based pricing table by navigating to the Billing Account tab of your profile page on DNAnexus and then clicking the green 'Add Billing Info' button next to your name.\n\n\nHow can I set up billing for my lab?\n\n\nBilling setup is different based on whether you are an internal user (you work at St. Jude) or an external user. External users refer to the \nCreate an Account page\n for instructions. Internal users search 'Bioinformatics self-service on St. Jude Cloud' from the \nintranet home page\n for instructions. \n\n\nWhy do I need to sign the Data Access Agreement (DAA)?\n\n\nAlthough the \nDAA\n serves many purposes, the terms included in the data access\nagreement are ultimately in place to protect our patients. We take\npatient security very seriously, and we require that requesters are\ncommitted to protecting that privacy to the fullest extent.\n\n\nCan I make edits/revisions to the DAA?\n\n\nAs a rule, we do not negotiate the terms of the data access agreement unless terms are found to be in conflict with the institution's state law. The terms included in the data access agreement are ultimately in place to protect our patients, a matter which we take very seriously.\n\n\nCan I get a Microsoft Word version of the DAA?\n\n\nNo. If we provide an editable format, we cannot ensure that the legal document has not been changed. Since we do not accept different versions of this agreement, we unfortunately cannot provide the document in Word format.\n\n\nWhere can I find the latest version of the Data Access Agreement (DAA)?\n\n\nWe keep \nour site\n up to date with the latest version on the Data Access Agreement for you to download, or you can download a copy\n\nhere\n.\n\n\nWhere do I submit the Data Access Agreement (DAA)?\n\n\nYou can submit your Data Access Agreement in the drag and drop box on the \nlast step of the data request process\n.\n\n\nWhat if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data?\n\n\nSince this would be a change in terms from the original agreement, you would need to fill out a new DAA (including the \nData Download Permission section\n for any datasets you want to download.\n\n\nWhat clinical information is available about samples in St. Jude Cloud?\n\n\nCurrently the only \nclinical information\n we provide is age at diagnosis, diagnosis, ethnicity, sex, race, and oncotree disease code. Unfortunately, even if we do collect other information, such as other supportive oncology data, it is not available on the cloud at this time. We are working towards being able to provide additional clinical annotations in the future.\n\n\nCan I get a copy of IRB consent forms?\n\n\nUnfortunately, we will not be able to share blank consent forms at the current time. We have chosen to remain consistent with the requirements of the other major genomic data repositories in that (1) there is an internal vetting process by the St. Jude IRB to ensure samples may be shared with the research community, but (2) we do not share the informed consents with data requesters. It is important to remember that St. Jude Cloud is the platform upon which all St. Jude data is shared. This means that there are more than 100 consent forms + revisions for the various studies across St. Jude. Thus, there is an additional logistical barrier in that we simply don't have the bandwidth to pull together a packet containing all of this information for each requestor.\n\n\nCan I request FASTQ files on St. Jude Cloud?\n\n\nWe do not share FASTQ formats, but several tools exist that you can leverage to revert BAM to FASTQ files. (We recommend using Picard SamToFastq to revert BAM files.) You can efficiently revert BAMs to FASTQs in the cloud by wrapping the conversion tool of your choice into a \nCloud App\n.\n\n\nHow can I explore and manipulate data files stored on the cloud without downloading the files to my local machine?\n\n\nYou can quickly and easily interact with data files using the DNAnexus cloud workstation app. See \nthis guide\n to help you setup and run the app. Note that if you are doing any type of large-scale, multi-sample analysis, especially if you plan to repeat the analysis or want to run in parallel, you will want to \nwrite your own cloud app\n rather than use the cloud workstation app.\n\n\nHow can I run an analysis workflow on multiple sample files at the same time?\n\n\nThe DNAnexus interface does have a batch tool available; however, it is in early testing, so we recommend using dx-toolkit on the command line as the most reliable and user friendly approach to batch and submit jobs. You can find our documentation on how to install and get started with dx-toolkit \nhere\n. You may also refer to the sample script below that loops through all the BAM files in the \ndata\n folder and submits a job using the BAM and matching index file.   \n\n\nfor bam in $(dx ls '/data/*.bam'); do  \n\n\n  dx run \\  \n\n\n    --yes \\  \n\n\n    --input \"0.BAM=/data/$bam\" \\\n\n\n    --input \"0.BAM_INDEX=/data/$bam.bai\" \\\n\n\n    \"$PROJECT_ID:/Rapid RNA-Seq (BAM)\"\n\n\ndone\n\n\n\n\n\n\nNote that this sample script assumes that the BAM and index files are in the \ndata\n folder and the Rapid RNA-Seq analysis workflow is in the project. \n$PROJECT_ID\n can be set to your project dxid, and \nRapid RNA-Seq (BAM)\n can be changed to the workflow you want to run.\n\n\nHow can I connect to DNAnexus API via SSH on a Windows machine?\n\n\nTo connect via SSH on a Windows machine we recommend using Windows Subsystem for Linux (WSL) or a Linux virtual machine.\n\nThe WSL method:\n\n1. Open PowerShell as an administrator and run \n\n\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n\n2. Restart computer.\n\n3. Install Ubuntu from the \nMicrosoft Store\n.\n\n4. Open Ubuntu. This prompts you to create a user and password.\n\n5. Install dx toolkit:\n\n\n$ sudo apt update\n\n\n$ sudo apt install python-minimal python-pip\n  \n\n\n$ pip install dxpy\n  \n\n6. Close Ubuntu window. Open Ubuntu again.\n\n7. Log in to DNAnexus, where \n$TOKEN\n is an \nAPI Token\n.\n\n\n$ dx login --token \n$TOKEN\n \n\n\nYou may also find \nthese instructions\n on the DNAnexus documentation helpful.\n\n\nWhy am I getting a connectivity error when connecting to DNAnexus API via SSH?\n\n\nIf you are trying to run something like  \n\n\n$ dx run --ssh <executable>\n \n\nand are getting a connectivity error, it may be that your firewall is too restrictive. Are you able to perform the command from an unrestricted network (like a home network)? If yes, you can resolve this issue by asking your network administrator to whitelist connections to Azure US West. All subnets (Region Name=\"uswest\") are provided \nhere\n.\n\n\nHow do I cite St. Jude Cloud?\n\n\nWe are currently in progress of preparing a paper for St. Jude Cloud. In the meantime, please refer to the \ncitation guide\n.\n\n\nWhen can I publish my findings using St. Jude Cloud data?\n\n\nOnce the \nembargo date\n for the St. Jude datasets that you've used in your research has passed, you are legally permitted to publish results based on that data.\n\n\nWhere can I find the embargo date?\n\n\nAll of our samples are marked with an \nembargo date\n.\nYou can find this by looking at the tags for each file or in the\n\nSAMPLE_INFO.txt\n file that is included with each data request.\nSelect a sample and click info to see more.\n\n\n\n\n\n\nWill St. Jude Cloud host my institution's data in the data browser or on PeCan?\n\n\nIf you are interested in submitting data to St. Jude Cloud, please contact us at support@stjude.cloud.",
            "title": "Frequently Asked Questions"
        },
        {
            "location": "/faq/#frequently-asked-questions",
            "text": "Account Questions  Where can I find the Terms of Service or the Privacy Policy?  How can I sign up for updates when new data or features are added to the cloud?  How can I delete my account?     Billing Questions  Will I be charged for using St. Jude Cloud?    How can I set up billing for my lab?  Data Request Questions  Why do I need to sign the Data Access Agreement (DAA)?  Can I make edits/revisions to the DAA?  Can I get a Microsoft Word version of the DAA  Where can I find the latest version of the Data Access Agreement (DAA)?  Where do I submit the Data Access Agreement (DAA)?  What if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data?  What clinical information is available about samples in St. Jude Cloud?  Can I get a copy of IRB consent forms?  Can I request FASTQ files on St. Jude Cloud?     Technical Questions  How can I explore and manipulate data files stored on the cloud without downloading the files to my local machine?  How can I run an analysis workflow on multiple sample files at the same time?  How can I connect to DNAnexus API via SSH on a Windows machine?  Why am having trouble connecting to DNAnexus API via SSH?     Publication Questions  How do I cite St. Jude Cloud?  When can I publish my findings using St. Jude Cloud data?  Where can I find the embargo date?  Miscellaneous    Will St. Jude Cloud host my institution's data in the data browser or on PeCan?",
            "title": "Frequently Asked Questions"
        },
        {
            "location": "/faq/#where-can-i-find-the-terms-of-service-or-the-privacy-policy",
            "text": "You can find the Terms of Service here  and the Privacy Policy here .",
            "title": "Where can I find the Terms of Service or the Privacy Policy?"
        },
        {
            "location": "/faq/#how-can-i-sign-up-for-updates-when-new-data-or-features-are-added-to-the-cloud",
            "text": "We are always adding data to St. Jude Cloud, and if you would like to sign up for updates,  subscribe here .",
            "title": "How can I sign up for updates when new data or features are added to the cloud?"
        },
        {
            "location": "/faq/#how-can-i-delete-my-account",
            "text": "If you'd like to delete your account, please email DNAnexus support at support@dnanexus.com  with the following email.  Hi   DNAnexus , \n\n   Would   you   please   assist   me   in   deleting   my   St .   Jude   Cloud   account ?   My   username   is   _____ .  Thank   you !",
            "title": "How can I delete my account?"
        },
        {
            "location": "/faq/#will-i-be-charged-for-using-st-jude-cloud",
            "text": "Any copy of the St. Jude data you receive is considered \"sponsored\",\n  so you do not have to pay a fee to store this data in St. Jude\n  Cloud. Although you may be prompted to enter billing information, you will not\nbe charged for anything with the exception of the following actions:   You will be charged for any  derivative  files stored on the St. Jude Cloud, such as results files obtained through running analyses workflows.  There is a small monthly storage fee associated for any of your own data you upload to the cloud.  If you run any of our analysis workflows (such as Rapid RNA-Seq, WARDEN, etc.) or your own workflows that you have uploaded and packaged into the cloud, you will be charged for the\n  compute resources used in producing the results. Soon we hope to be able to sponsor all compute costs associated with running our St. Jude Cloud workflows.   You can find DNAnexus's specific resource-based pricing table by navigating to the Billing Account tab of your profile page on DNAnexus and then clicking the green 'Add Billing Info' button next to your name.",
            "title": "Will I be charged for using St. Jude Cloud?"
        },
        {
            "location": "/faq/#how-can-i-set-up-billing-for-my-lab",
            "text": "Billing setup is different based on whether you are an internal user (you work at St. Jude) or an external user. External users refer to the  Create an Account page  for instructions. Internal users search 'Bioinformatics self-service on St. Jude Cloud' from the  intranet home page  for instructions.",
            "title": "How can I set up billing for my lab?"
        },
        {
            "location": "/faq/#why-do-i-need-to-sign-the-data-access-agreement-daa",
            "text": "Although the  DAA  serves many purposes, the terms included in the data access\nagreement are ultimately in place to protect our patients. We take\npatient security very seriously, and we require that requesters are\ncommitted to protecting that privacy to the fullest extent.",
            "title": "Why do I need to sign the Data Access Agreement (DAA)?"
        },
        {
            "location": "/faq/#can-i-make-editsrevisions-to-the-daa",
            "text": "As a rule, we do not negotiate the terms of the data access agreement unless terms are found to be in conflict with the institution's state law. The terms included in the data access agreement are ultimately in place to protect our patients, a matter which we take very seriously.",
            "title": "Can I make edits/revisions to the DAA?"
        },
        {
            "location": "/faq/#can-i-get-a-microsoft-word-version-of-the-daa",
            "text": "No. If we provide an editable format, we cannot ensure that the legal document has not been changed. Since we do not accept different versions of this agreement, we unfortunately cannot provide the document in Word format.",
            "title": "Can I get a Microsoft Word version of the DAA?"
        },
        {
            "location": "/faq/#where-can-i-find-the-latest-version-of-the-data-access-agreement-daa",
            "text": "We keep  our site  up to date with the latest version on the Data Access Agreement for you to download, or you can download a copy here .",
            "title": "Where can I find the latest version of the Data Access Agreement (DAA)?"
        },
        {
            "location": "/faq/#where-do-i-submit-the-data-access-agreement-daa",
            "text": "You can submit your Data Access Agreement in the drag and drop box on the  last step of the data request process .",
            "title": "Where do I submit the Data Access Agreement (DAA)?"
        },
        {
            "location": "/faq/#what-if-i-did-not-fill-out-the-data-download-permission-section-of-the-original-daa-but-now-i-want-to-download-data",
            "text": "Since this would be a change in terms from the original agreement, you would need to fill out a new DAA (including the  Data Download Permission section  for any datasets you want to download.",
            "title": "What if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data?"
        },
        {
            "location": "/faq/#what-clinical-information-is-available-about-samples-in-st-jude-cloud",
            "text": "Currently the only  clinical information  we provide is age at diagnosis, diagnosis, ethnicity, sex, race, and oncotree disease code. Unfortunately, even if we do collect other information, such as other supportive oncology data, it is not available on the cloud at this time. We are working towards being able to provide additional clinical annotations in the future.",
            "title": "What clinical information is available about samples in St. Jude Cloud?"
        },
        {
            "location": "/faq/#can-i-get-a-copy-of-irb-consent-forms",
            "text": "Unfortunately, we will not be able to share blank consent forms at the current time. We have chosen to remain consistent with the requirements of the other major genomic data repositories in that (1) there is an internal vetting process by the St. Jude IRB to ensure samples may be shared with the research community, but (2) we do not share the informed consents with data requesters. It is important to remember that St. Jude Cloud is the platform upon which all St. Jude data is shared. This means that there are more than 100 consent forms + revisions for the various studies across St. Jude. Thus, there is an additional logistical barrier in that we simply don't have the bandwidth to pull together a packet containing all of this information for each requestor.",
            "title": "Can I get a copy of IRB consent forms?"
        },
        {
            "location": "/faq/#can-i-request-fastq-files-on-st-jude-cloud",
            "text": "We do not share FASTQ formats, but several tools exist that you can leverage to revert BAM to FASTQ files. (We recommend using Picard SamToFastq to revert BAM files.) You can efficiently revert BAMs to FASTQs in the cloud by wrapping the conversion tool of your choice into a  Cloud App .",
            "title": "Can I request FASTQ files on St. Jude Cloud?"
        },
        {
            "location": "/faq/#how-can-i-explore-and-manipulate-data-files-stored-on-the-cloud-without-downloading-the-files-to-my-local-machine",
            "text": "You can quickly and easily interact with data files using the DNAnexus cloud workstation app. See  this guide  to help you setup and run the app. Note that if you are doing any type of large-scale, multi-sample analysis, especially if you plan to repeat the analysis or want to run in parallel, you will want to  write your own cloud app  rather than use the cloud workstation app.",
            "title": "How can I explore and manipulate data files stored on the cloud without downloading the files to my local machine?"
        },
        {
            "location": "/faq/#how-can-i-run-an-analysis-workflow-on-multiple-sample-files-at-the-same-time",
            "text": "The DNAnexus interface does have a batch tool available; however, it is in early testing, so we recommend using dx-toolkit on the command line as the most reliable and user friendly approach to batch and submit jobs. You can find our documentation on how to install and get started with dx-toolkit  here . You may also refer to the sample script below that loops through all the BAM files in the  data  folder and submits a job using the BAM and matching index file.     for bam in $(dx ls '/data/*.bam'); do      dx run \\        --yes \\        --input \"0.BAM=/data/$bam\" \\      --input \"0.BAM_INDEX=/data/$bam.bai\" \\      \"$PROJECT_ID:/Rapid RNA-Seq (BAM)\"  done   Note that this sample script assumes that the BAM and index files are in the  data  folder and the Rapid RNA-Seq analysis workflow is in the project.  $PROJECT_ID  can be set to your project dxid, and  Rapid RNA-Seq (BAM)  can be changed to the workflow you want to run.",
            "title": "How can I run an analysis workflow on multiple sample files at the same time?"
        },
        {
            "location": "/faq/#how-can-i-connect-to-dnanexus-api-via-ssh-on-a-windows-machine",
            "text": "To connect via SSH on a Windows machine we recommend using Windows Subsystem for Linux (WSL) or a Linux virtual machine. \nThe WSL method: \n1. Open PowerShell as an administrator and run   Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux \n2. Restart computer. \n3. Install Ubuntu from the  Microsoft Store . \n4. Open Ubuntu. This prompts you to create a user and password. \n5. Install dx toolkit:  $ sudo apt update  $ sudo apt install python-minimal python-pip     $ pip install dxpy    \n6. Close Ubuntu window. Open Ubuntu again. \n7. Log in to DNAnexus, where  $TOKEN  is an  API Token .  $ dx login --token  $TOKEN    You may also find  these instructions  on the DNAnexus documentation helpful.",
            "title": "How can I connect to DNAnexus API via SSH on a Windows machine?"
        },
        {
            "location": "/faq/#why-am-i-getting-a-connectivity-error-when-connecting-to-dnanexus-api-via-ssh",
            "text": "If you are trying to run something like    $ dx run --ssh <executable>   \nand are getting a connectivity error, it may be that your firewall is too restrictive. Are you able to perform the command from an unrestricted network (like a home network)? If yes, you can resolve this issue by asking your network administrator to whitelist connections to Azure US West. All subnets (Region Name=\"uswest\") are provided  here .",
            "title": "Why am I getting a connectivity error when connecting to DNAnexus API via SSH?"
        },
        {
            "location": "/faq/#how-do-i-cite-st-jude-cloud",
            "text": "We are currently in progress of preparing a paper for St. Jude Cloud. In the meantime, please refer to the  citation guide .",
            "title": "How do I cite St. Jude Cloud?"
        },
        {
            "location": "/faq/#when-can-i-publish-my-findings-using-st-jude-cloud-data",
            "text": "Once the  embargo date  for the St. Jude datasets that you've used in your research has passed, you are legally permitted to publish results based on that data.",
            "title": "When can I publish my findings using St. Jude Cloud data?"
        },
        {
            "location": "/faq/#where-can-i-find-the-embargo-date",
            "text": "All of our samples are marked with an  embargo date .\nYou can find this by looking at the tags for each file or in the SAMPLE_INFO.txt  file that is included with each data request.\nSelect a sample and click info to see more.",
            "title": "Where can I find the embargo date?"
        },
        {
            "location": "/faq/#will-st-jude-cloud-host-my-institutions-data-in-the-data-browser-or-on-pecan",
            "text": "If you are interested in submitting data to St. Jude Cloud, please contact us at support@stjude.cloud.",
            "title": "Will St. Jude Cloud host my institution's data in the data browser or on PeCan?"
        },
        {
            "location": "/glossary/",
            "text": "Data Access Agreement\n\n\nA St. Jude Cloud \nData Access Agreement (DAA)\n is a legally binding document outlining a number of terms and conditions to which anyone working with St. Jude Cloud data must agree.\nWe do not negotiate the terms of this document unless terms are found to be in conflict with the institution's state law. Filling out the Data Access Agreement carefully and completely is crucial to having your request approved promptly. \nClick Here\n to download a copy of the DAA. \nClick Here\n for a step by step guide on how to fill out the DAA.\n\n\nIf you have incompletely or incorrectly filled out your DAA and would like to upload a revised form, \nClick Here\n for instructions.\n\n\nOnce you have submitted a correctly filled out DAA and have been granted access to one or more \nData Access Units (DAUs)\n, you can continue checking out files from those DAUs until your access expires. Access is generally granted for 1 year, at which point you must submit an Extension Addendum to continue using the data. \nClick Here\n for a step-by-step guide on how to fill out the Extension Addendum.\n\n\nData Access Committee\n\n\nA St. Jude Cloud \nData Access Committee (DAC)\n is group of St. Jude researchers who oversee access to a particular \nData Access Unit (DAU)\n and evaluate incoming data requests.\n\n\nThe first time you request access to files in \na DAU, it is required that you fill out a \nData Access Agreement (DAA)\n. Access is granted at the DAU level based on the decision of each DAC upon reviewing the DAA.\n\n\n\n\nExample\n\n\nFor example, if you make a request asking for all of St. Jude's Acute \nLymphoblastic Leukemia sequencing data, you might be asking for data from \nmultiple different projects (DAUs) here at St. Jude. For the sake of the example,\nlet's say the data you want is spread across three different DAUs. Once\nyou place a request, your application will be routed to the corresponding\nthree data access committees for approval. Since each DAC is made up of\ndifferent individuals using different criteria for evaluation, you may or\nmay not be approved for access to all of the files. \n\n\n\n\nData Access Unit\n\n\nA St. Jude Cloud \nData Access Unit (DAU)\n is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. Each DAU has its own governing body of researchers, the \nData Access Committee\n, who preside over the data and who may grant or deny access. We currently have 5 DAUs: Pediatric Cancer Genome Project (PCGP), St. Jude Lifetime Cohort Study (SJLIFE), Genomes for Kids (G4K) and Clinical Genomics, Sickle Cell Genome Project (SGP), and Childhood Cancer Survivor Study (CCSS). For a brief description of each DAU see the \nStudies page\n. For a more detailed description please see the respective \nSchedule 1(s)\n.\n\n\nEmbargo Date\n\n\nThe \nEmbargo Date\n specifies the date that a publishing embargo on the file in question has been lifted. Publishing using any of the files \nbefore\n the embargo date has passed is strictly prohibited as outlined in the \nData Access Agreement (DAA)\n. Typically, samples from the same \nData Access Unit (DAU)\n all have the same embargo date, as they would have been released on St. Jude Cloud at the same time.\n\n\nCurrent Embargo Dates\n\n\n\n\n\n\n\n\nData Access Unit\n\n\nEmbargo Date\n\n\n\n\n\n\n\n\n\n\nPediatric Cancer Genome Project\n\n\nJuly 23, 2018\n\n\n\n\n\n\nSt. Jude LIFE\n\n\nJanuary 15, 2019\n\n\n\n\n\n\nClinical Genomics\n\n\nJanuary 15, 2019\n\n\n\n\n\n\nSickle Cell Genome Project\n\n\nSeptember 1, 2019\n\n\n\n\n\n\nChildhood Cancer Survivor Study\n\n\nNovember 1, 2019",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#data-access-agreement",
            "text": "A St. Jude Cloud  Data Access Agreement (DAA)  is a legally binding document outlining a number of terms and conditions to which anyone working with St. Jude Cloud data must agree.\nWe do not negotiate the terms of this document unless terms are found to be in conflict with the institution's state law. Filling out the Data Access Agreement carefully and completely is crucial to having your request approved promptly.  Click Here  to download a copy of the DAA.  Click Here  for a step by step guide on how to fill out the DAA.  If you have incompletely or incorrectly filled out your DAA and would like to upload a revised form,  Click Here  for instructions.  Once you have submitted a correctly filled out DAA and have been granted access to one or more  Data Access Units (DAUs) , you can continue checking out files from those DAUs until your access expires. Access is generally granted for 1 year, at which point you must submit an Extension Addendum to continue using the data.  Click Here  for a step-by-step guide on how to fill out the Extension Addendum.",
            "title": "Data Access Agreement"
        },
        {
            "location": "/glossary/#data-access-committee",
            "text": "A St. Jude Cloud  Data Access Committee (DAC)  is group of St. Jude researchers who oversee access to a particular  Data Access Unit (DAU)  and evaluate incoming data requests.  The first time you request access to files in \na DAU, it is required that you fill out a  Data Access Agreement (DAA) . Access is granted at the DAU level based on the decision of each DAC upon reviewing the DAA.   Example  For example, if you make a request asking for all of St. Jude's Acute \nLymphoblastic Leukemia sequencing data, you might be asking for data from \nmultiple different projects (DAUs) here at St. Jude. For the sake of the example,\nlet's say the data you want is spread across three different DAUs. Once\nyou place a request, your application will be routed to the corresponding\nthree data access committees for approval. Since each DAC is made up of\ndifferent individuals using different criteria for evaluation, you may or\nmay not be approved for access to all of the files.",
            "title": "Data Access Committee"
        },
        {
            "location": "/glossary/#data-access-unit",
            "text": "A St. Jude Cloud  Data Access Unit (DAU)  is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. Each DAU has its own governing body of researchers, the  Data Access Committee , who preside over the data and who may grant or deny access. We currently have 5 DAUs: Pediatric Cancer Genome Project (PCGP), St. Jude Lifetime Cohort Study (SJLIFE), Genomes for Kids (G4K) and Clinical Genomics, Sickle Cell Genome Project (SGP), and Childhood Cancer Survivor Study (CCSS). For a brief description of each DAU see the  Studies page . For a more detailed description please see the respective  Schedule 1(s) .",
            "title": "Data Access Unit"
        },
        {
            "location": "/glossary/#embargo-date",
            "text": "The  Embargo Date  specifies the date that a publishing embargo on the file in question has been lifted. Publishing using any of the files  before  the embargo date has passed is strictly prohibited as outlined in the  Data Access Agreement (DAA) . Typically, samples from the same  Data Access Unit (DAU)  all have the same embargo date, as they would have been released on St. Jude Cloud at the same time.  Current Embargo Dates     Data Access Unit  Embargo Date      Pediatric Cancer Genome Project  July 23, 2018    St. Jude LIFE  January 15, 2019    Clinical Genomics  January 15, 2019    Sickle Cell Genome Project  September 1, 2019    Childhood Cancer Survivor Study  November 1, 2019",
            "title": "Embargo Date"
        }
    ]
}
{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to St. Jude Cloud Documentation! COVID-19 Information See how St. Jude Cloud is responding to the coronavirus pandemic on our COVID-19 Quickstart page. Overview \u00b6 The St. Jude Cloud documentation includes authoritative guides for accessing St. Jude Cloud data, creating and running analysis workflows on the cloud, and exploring curated data from numerous published studies by St. Jude and our collaborating institutions.To learn more about our ecosystem, read this guide . Sign up here to receive email notifications when we add new datasets, analysis pipelines, or other exciting features. Features \u00b6 You can apply many different capabilities of St. Jude Cloud to your research, such as: Explore the raw genomics data we currently offer. You can browse by diagnosis, publication, or curated dataset while applying a number of different filters. For more information, see our data request guide . Run your tools on our data by requesting data and packaging your tools in a secure cloud environment. See this guide for an example. Run our in house analysis workflows on your data by moving your data to the cloud and selecting a workflow to run. See this guide for an example. Explore St. Jude datasets through interactive visualizations that we have packaged for the community. For example, visit PeCan to visually investigate pediatric cancer mutation data. Create manuscript quality figures with your data to use in publications or to host on your website with ProteinPaint or GenomePaint . See the ProteinPaint documentation and GenomePaint documentation for help. Note Please note that while it is free to receive and store our data in St. Jude Cloud, there are compute and storage fees associated with working in the cloud, as well as egress fees for downloading data . Apps \u00b6 St. Jude Cloud provides a number of applications which you can use for various purposes. Click on the links below to checkout one or more of our applications. Genomics Platform Browse our publicly available genomics datasets, make a data request, and run analyses in a secure cloud environment. View the documentation for this app here . Pediatric Cancer Portal (PeCan) Interactively explore mutational recurrence and pathogenicity assessment of variants in pediatric cancer using a wide variety of St. Jude + publicly available data. View the documentation for this app here . Visualization Community Explore, create, and share interactive visualizations using tools such as ProteinPaint and GenomePaint. View the documentation for this app here . Studies \u00b6 The following projects currently distribute data through St. Jude Cloud. Click here for a brief description of each project listed below. Or click on a project in the bulleted list below to go straight to the corresponding Study page. Pediatric Cancer Genome Project (PCGP) St. Jude Lifetime (SJLIFE) Clinical Genomics (Clinical Pilot and G4K) Sickle Cell Genome Project (SGP) Childhood Cancer Survivor Study (CCSS) Analysis Workflows \u00b6 St. Jude shares a number of end-to-end analysis workflows as part of the Genomics Platform application. Click on the links below to learn more about the workflow. NeoepitopePred ChIP-Seq Peak Calling Rapid RNA-Seq Fusion Detection WARDEN Differential Expression Analysis Mutational Signatures SequencErr M2A (coming soon) Contact Us \u00b6 Any questions, comments, or concerns can be directed to our \"Contact Us\" form or you can email us directly at support@stjude.cloud .","title":"Overview"},{"location":"#overview","text":"The St. Jude Cloud documentation includes authoritative guides for accessing St. Jude Cloud data, creating and running analysis workflows on the cloud, and exploring curated data from numerous published studies by St. Jude and our collaborating institutions.To learn more about our ecosystem, read this guide . Sign up here to receive email notifications when we add new datasets, analysis pipelines, or other exciting features.","title":"Overview"},{"location":"#features","text":"You can apply many different capabilities of St. Jude Cloud to your research, such as: Explore the raw genomics data we currently offer. You can browse by diagnosis, publication, or curated dataset while applying a number of different filters. For more information, see our data request guide . Run your tools on our data by requesting data and packaging your tools in a secure cloud environment. See this guide for an example. Run our in house analysis workflows on your data by moving your data to the cloud and selecting a workflow to run. See this guide for an example. Explore St. Jude datasets through interactive visualizations that we have packaged for the community. For example, visit PeCan to visually investigate pediatric cancer mutation data. Create manuscript quality figures with your data to use in publications or to host on your website with ProteinPaint or GenomePaint . See the ProteinPaint documentation and GenomePaint documentation for help. Note Please note that while it is free to receive and store our data in St. Jude Cloud, there are compute and storage fees associated with working in the cloud, as well as egress fees for downloading data .","title":"Features"},{"location":"#apps","text":"St. Jude Cloud provides a number of applications which you can use for various purposes. Click on the links below to checkout one or more of our applications. Genomics Platform Browse our publicly available genomics datasets, make a data request, and run analyses in a secure cloud environment. View the documentation for this app here . Pediatric Cancer Portal (PeCan) Interactively explore mutational recurrence and pathogenicity assessment of variants in pediatric cancer using a wide variety of St. Jude + publicly available data. View the documentation for this app here . Visualization Community Explore, create, and share interactive visualizations using tools such as ProteinPaint and GenomePaint. View the documentation for this app here .","title":"Apps"},{"location":"#studies","text":"The following projects currently distribute data through St. Jude Cloud. Click here for a brief description of each project listed below. Or click on a project in the bulleted list below to go straight to the corresponding Study page. Pediatric Cancer Genome Project (PCGP) St. Jude Lifetime (SJLIFE) Clinical Genomics (Clinical Pilot and G4K) Sickle Cell Genome Project (SGP) Childhood Cancer Survivor Study (CCSS)","title":"Studies"},{"location":"#analysis-workflows","text":"St. Jude shares a number of end-to-end analysis workflows as part of the Genomics Platform application. Click on the links below to learn more about the workflow. NeoepitopePred ChIP-Seq Peak Calling Rapid RNA-Seq Fusion Detection WARDEN Differential Expression Analysis Mutational Signatures SequencErr M2A (coming soon)","title":"Analysis Workflows"},{"location":"#contact-us","text":"Any questions, comments, or concerns can be directed to our \"Contact Us\" form or you can email us directly at support@stjude.cloud .","title":"Contact Us"},{"location":"citing-stjude-cloud/","text":"Citing St. Jude Cloud We are currently in progress of preparing the St. Jude Cloud manuscript. Until further notice, please ensure that when St. Jude Cloud or data accessed therein is used in your work, please: Cite the relevant paper for each of the datasets and/or resources that you used in your study. State in the Results and/or Methods section that the relevant data and/or resource was obtained from St. Jude Cloud. Example statement: \"Whole genome sequencing data for relapse tumor samples from 345 pediatric patients were obtained from St. Jude Cloud.\" State in the Data availability section of the manuscript that data and/or resource can be accessed via St. Jude Cloud. Example statement: \"Whole genome sequencing data for pediatric relapse tumor samples used for analysis in this study were obtained from St. Jude Cloud ( https://www.stjude.cloud ) \u2013 a publicly accessible pediatric genomic data resource requiring approval for controlled data access.\" Dataset Reference Table \u00b6 Please download the Schedule 1(s) (linked in table below) to find dataset specific wording of acknowledgement(s). St. Jude Cloud Dataset Reference Pediatric Cancer Genome Project (PCGP) dataset PCGP perspectives paper and the relevant tumor type paper(s) ; file_download PCGP Schedule 1 St. Jude Lifetime (SJLIFE) dataset SJLIFE paper ; file_download SJLIFE Schedule 1 Clinical Genomics (Clinical Pilot, Genomes for Kids, Real-Time Clinical Genomics) dataset Clinical Pilot paper ; file_download Clinical Genomics Schedule 1 Sickle Cell Genome Project (SGP) dataset paper in progress; file_download SGP Schedule 1 Childhood Cancer Survivor Study (CCSS) dataset CCSS study design paper ; file_download CCSS Schedule 1 Note If you are unsure what dataset(s) the data that you have been vended belongs to, you can find this information in the sj_datasets column of the SAMPLE_INFO.txt file. Warning Publishing using any of the data files before the embargo date has passed is strictly prohibited as outlined in the Data Access Agreement (DAA) . Resource Reference Table \u00b6 St. Jude Cloud Resource Reference ProteinPaint ProteinPaint paper GenomePaint paper in progress PeCan Pie PeCan Pie paper NeoepitopePred NeoepitopePred paper ChIP-Seq Peak Calling unpublished Rapid RNA-Seq Fusion Detection paper in progress WARDEN unpublished Mutational Signatures Mutational Patterns paper cis-x paper in progress XenoCP paper in progress Contact Us \u00b6 Any questions, comments, or concerns can be directed to our \"Contact Us\" form or you can email us directly at support@stjude.cloud .","title":"Citing St. Jude Cloud"},{"location":"citing-stjude-cloud/#dataset-reference-table","text":"Please download the Schedule 1(s) (linked in table below) to find dataset specific wording of acknowledgement(s). St. Jude Cloud Dataset Reference Pediatric Cancer Genome Project (PCGP) dataset PCGP perspectives paper and the relevant tumor type paper(s) ; file_download PCGP Schedule 1 St. Jude Lifetime (SJLIFE) dataset SJLIFE paper ; file_download SJLIFE Schedule 1 Clinical Genomics (Clinical Pilot, Genomes for Kids, Real-Time Clinical Genomics) dataset Clinical Pilot paper ; file_download Clinical Genomics Schedule 1 Sickle Cell Genome Project (SGP) dataset paper in progress; file_download SGP Schedule 1 Childhood Cancer Survivor Study (CCSS) dataset CCSS study design paper ; file_download CCSS Schedule 1 Note If you are unsure what dataset(s) the data that you have been vended belongs to, you can find this information in the sj_datasets column of the SAMPLE_INFO.txt file. Warning Publishing using any of the data files before the embargo date has passed is strictly prohibited as outlined in the Data Access Agreement (DAA) .","title":"Dataset Reference Table"},{"location":"citing-stjude-cloud/#resource-reference-table","text":"St. Jude Cloud Resource Reference ProteinPaint ProteinPaint paper GenomePaint paper in progress PeCan Pie PeCan Pie paper NeoepitopePred NeoepitopePred paper ChIP-Seq Peak Calling unpublished Rapid RNA-Seq Fusion Detection paper in progress WARDEN unpublished Mutational Signatures Mutational Patterns paper cis-x paper in progress XenoCP paper in progress","title":"Resource Reference Table"},{"location":"citing-stjude-cloud/#contact-us","text":"Any questions, comments, or concerns can be directed to our \"Contact Us\" form or you can email us directly at support@stjude.cloud .","title":"Contact Us"},{"location":"ecosystem/","text":"About our Ecosystem Overview \u00b6 The St. Jude Cloud ecosystem has been designed with three main entry points, called Research Domains, to guide users along a path of content that will be most relevant to their research interests. Cards for these top-level Research Domains (Pediatric Cancer, Cancer Survivorship, and Non-Cancerous Diseases) are the first selections you will be presented with on our homepage . The diagram below shows an abstraction of our ecosystem. Research Domains are presented as vertical panels which cascade down through the sets of Apps and Studies, the building blocks of content along the Research Domains. Apps are interactive and facilitate data sharing and discovery. Studies are static pages of content that discuss how St. Jude has generated and used a particular dataset. Apps and Studies are disjoint ecosystem elements, yet they are distributed non-uniquely among the Research Domains in order to facilitate a focused sharing of content. For a complete list of Applications and Studies please visit the Welcome to St. Jude documentation page. Migration to new structure \u00b6 St. Jude Cloud's original architecture of Data, Tools, and Visualizations has been retired. You can learn more about what changed in the announcement blog post . We hope this redesigned ecosystem will better facilitate data sharing, discovery, and community engagement.","title":"About our Ecosystem"},{"location":"ecosystem/#overview","text":"The St. Jude Cloud ecosystem has been designed with three main entry points, called Research Domains, to guide users along a path of content that will be most relevant to their research interests. Cards for these top-level Research Domains (Pediatric Cancer, Cancer Survivorship, and Non-Cancerous Diseases) are the first selections you will be presented with on our homepage . The diagram below shows an abstraction of our ecosystem. Research Domains are presented as vertical panels which cascade down through the sets of Apps and Studies, the building blocks of content along the Research Domains. Apps are interactive and facilitate data sharing and discovery. Studies are static pages of content that discuss how St. Jude has generated and used a particular dataset. Apps and Studies are disjoint ecosystem elements, yet they are distributed non-uniquely among the Research Domains in order to facilitate a focused sharing of content. For a complete list of Applications and Studies please visit the Welcome to St. Jude documentation page.","title":"Overview"},{"location":"ecosystem/#migration-to-new-structure","text":"St. Jude Cloud's original architecture of Data, Tools, and Visualizations has been retired. You can learn more about what changed in the announcement blog post . We hope this redesigned ecosystem will better facilitate data sharing, discovery, and community engagement.","title":"Migration to new structure"},{"location":"faq/","text":"Frequently Asked Questions Info This page contains information about site-wide St. Jude Cloud frequently asked questions. For more detailed information, we highly recommend that you also visit the dedicated frequently asked questions page for the application you are using. St. Jude Cloud Genomics Platform FAQ St. Jude Cloud PeCan FAQ St. Jude Cloud Visualization Community FAQ General Questions Where can I find the Terms of Service or the Privacy Policy? How can I sign up for updates? Billing Questions Will I be charged for using St. Jude Cloud? Publication Questions How do I cite St. Jude Cloud? When can I publish my findings using St. Jude Cloud data? General Questions \u00b6 Where can I find the Terms of Service or the Privacy Policy? \u00b6 You can find the Terms of Service here and the Privacy Policy here . They are updated regularly, so as we note out in the text there, please check back regularly! How can I sign up for updates? \u00b6 To receive updates on St. Jude Cloud, such as new data releases and new applications released, please subscribe to our email list . Billing Questions \u00b6 Will I be charged for using St. Jude Cloud? \u00b6 Please see the following questions in each application's dedicated frequently asked questions page. St. Jude Cloud Genomics Platform St. Jude Cloud PeCan St. Jude Cloud Visualization Community Publication Questions \u00b6 How do I cite St. Jude Cloud? \u00b6 The manuscript for St. Jude Cloud is currently in preparation. In the meantime, please refer to the citation guide . When can I publish my findings using St. Jude Cloud data? \u00b6 Within St. Jude Cloud Genomics Platform, all of the samples are marked with an embargo date . Once the embargo date has passed for all of the data sets you've used in your research, you are permitted to publish results based on that data. You can find this by looking at the tags for each file or in the SAMPLE_INFO.txt file that is included with each data request. To learn more, please visit the dedicated section in our guide . Any data in St. Jude Cloud Visualization Community or St. Jude Cloud PeCan is considered published. For any data not covered here, or if you have any questions, please contact us .","title":"Frequently Asked Questions"},{"location":"faq/#general-questions","text":"","title":"General Questions"},{"location":"faq/#where-can-i-find-the-terms-of-service-or-the-privacy-policy","text":"You can find the Terms of Service here and the Privacy Policy here . They are updated regularly, so as we note out in the text there, please check back regularly!","title":"Where can I find the Terms of Service or the Privacy Policy?"},{"location":"faq/#how-can-i-sign-up-for-updates","text":"To receive updates on St. Jude Cloud, such as new data releases and new applications released, please subscribe to our email list .","title":"How can I sign up for updates?"},{"location":"faq/#billing-questions","text":"","title":"Billing Questions"},{"location":"faq/#will-i-be-charged-for-using-st-jude-cloud","text":"Please see the following questions in each application's dedicated frequently asked questions page. St. Jude Cloud Genomics Platform St. Jude Cloud PeCan St. Jude Cloud Visualization Community","title":"Will I be charged for using St. Jude Cloud?"},{"location":"faq/#publication-questions","text":"","title":"Publication Questions"},{"location":"faq/#how-do-i-cite-st-jude-cloud","text":"The manuscript for St. Jude Cloud is currently in preparation. In the meantime, please refer to the citation guide .","title":"How do I cite St. Jude Cloud?"},{"location":"faq/#when-can-i-publish-my-findings-using-st-jude-cloud-data","text":"Within St. Jude Cloud Genomics Platform, all of the samples are marked with an embargo date . Once the embargo date has passed for all of the data sets you've used in your research, you are permitted to publish results based on that data. You can find this by looking at the tags for each file or in the SAMPLE_INFO.txt file that is included with each data request. To learn more, please visit the dedicated section in our guide . Any data in St. Jude Cloud Visualization Community or St. Jude Cloud PeCan is considered published. For any data not covered here, or if you have any questions, please contact us .","title":"When can I publish my findings using St. Jude Cloud data?"},{"location":"guides/covid-19/","text":"Info This section of the documentation is geared towards St. Jude Children's Research Hospital research staff affected by COVID-19, not to other users of St. Jude Cloud. Regardless, anyone may use these pages as a path to follow should they wish to try out St. Jude Cloud. Overview \u00b6 The sudden, fully remote operation of St. Jude research staff due to the COVID-19 pandemic has caused higher than normal demand on VPN and HPC cluster resources. In response to these events, the St. Jude Cloud team has worked with Information Services to accelerate our plans for exposing a cloud-based computational environment that can support both production-grade pipelines and interactive, ad-hoc analysis. This page serves as a guide for quickly trying out St. Jude Cloud and exploring experimental offerings we have prototyped to ease your transition. Read below about the different levels of engagement we are offering during this time and decide which you are interested in. If you're interested in jumping in, you can skip to the guides in the walkthrough section . Tip Currently, this is guide is focused on omics-based data analysis. The DNAnexus ecosystem can be used for many other types of data analysis, and can serve as a secure cloud environment to easily do other types of research. If you are interested, please contact us and let us know \u2014 we'd be happy to help. Levels of Engagement \u00b6 If you're interested in trying our platform, you'll need to pick one of the two levels of engagement we offer: General Use. Use the guides and resources provided to self-navigate using St. Jude Cloud. Anyone can independently leverage St. Jude Cloud to move computational workloads from the HPC without involvement from our team. If you have a St. Jude SSO account, then you already have a St. Jude Cloud account! You can navigate to the walkthrough section to get started. Discovery Sponsorship Program. We are developing several experimental features, including a new, cloud-native BAM viewer and interactive nodes in the cloud, to help ease the transition. As such, we're looking for a small number of labs to collaborate much more closely with to develop out these features and ensure a successful transition to the cloud. If you are interested in helping us shape the St. Jude Cloud experience for all users and getting sponsored compute + storage, we recommend you apply for one of the limited discovery program slots . All other users can navigate to the walkthrough section to get started. You can see a more complete list of the differences between these two offerings in the figure below. Walkthrough \u00b6 Please read the guides on the left in order to quickly get up to speed on how you can transition to using St. Jude Cloud. If you have any questions, please contact us .","title":"Introduction"},{"location":"guides/covid-19/#overview","text":"The sudden, fully remote operation of St. Jude research staff due to the COVID-19 pandemic has caused higher than normal demand on VPN and HPC cluster resources. In response to these events, the St. Jude Cloud team has worked with Information Services to accelerate our plans for exposing a cloud-based computational environment that can support both production-grade pipelines and interactive, ad-hoc analysis. This page serves as a guide for quickly trying out St. Jude Cloud and exploring experimental offerings we have prototyped to ease your transition. Read below about the different levels of engagement we are offering during this time and decide which you are interested in. If you're interested in jumping in, you can skip to the guides in the walkthrough section . Tip Currently, this is guide is focused on omics-based data analysis. The DNAnexus ecosystem can be used for many other types of data analysis, and can serve as a secure cloud environment to easily do other types of research. If you are interested, please contact us and let us know \u2014 we'd be happy to help.","title":"Overview"},{"location":"guides/covid-19/#levels-of-engagement","text":"If you're interested in trying our platform, you'll need to pick one of the two levels of engagement we offer: General Use. Use the guides and resources provided to self-navigate using St. Jude Cloud. Anyone can independently leverage St. Jude Cloud to move computational workloads from the HPC without involvement from our team. If you have a St. Jude SSO account, then you already have a St. Jude Cloud account! You can navigate to the walkthrough section to get started. Discovery Sponsorship Program. We are developing several experimental features, including a new, cloud-native BAM viewer and interactive nodes in the cloud, to help ease the transition. As such, we're looking for a small number of labs to collaborate much more closely with to develop out these features and ensure a successful transition to the cloud. If you are interested in helping us shape the St. Jude Cloud experience for all users and getting sponsored compute + storage, we recommend you apply for one of the limited discovery program slots . All other users can navigate to the walkthrough section to get started. You can see a more complete list of the differences between these two offerings in the figure below.","title":"Levels of Engagement"},{"location":"guides/covid-19/#walkthrough","text":"Please read the guides on the left in order to quickly get up to speed on how you can transition to using St. Jude Cloud. If you have any questions, please contact us .","title":"Walkthrough"},{"location":"guides/covid-19/creating-a-cloud-app-covid/","text":"Creating A Cloud App We already have a full guide on creating a cloud application in DNAnexus located in our Data Guides section. Please visit Creating A Cloud App and return back to this page to continue on the remote work quickstart guide.","title":"Creating a cloud app"},{"location":"guides/covid-19/getting-started/","text":"Please complete the following check-list to get started with using St. Jude Cloud. If applicable, apply for Discovery Sponsorship Program ( link ). Join St. Jude Cloud helpdesk channel on Slack ( link ). Log in to your St. Jude Cloud account ( link ). Enable Microsoft Azure ( link ). Create your first project ( link ). Set up your billing account ( link ). Apply for Discovery Sponsorship Program \u00b6 Please see the Discovery Sponsorship Program (DSP) section in the figure below for an overview of program benefits. Applying for the program is easy - just fill out the application form and submit it to discoveryprogram@stjude.org . The last day to apply is April 9 th . Joining Slack \u00b6 The St. Jude Cloud team uses Slack to communicate on a day-to-day basis. When standing up a help desk, we decided to offer that functionality on Slack to allow our community to answer questions alongside our team. If you are an employee at SJCRH, you already have a Slack account (at no cost to you) \u2014 you might even like it so much, you decide to use it during this work from home period! If you have any issues with the instructions below, you can email helpdesk@stjude.org or support@stjude.cloud . Steps Navigate to https://stjude.slack.com , click \"Sign in with your St. Jude Account\", and enter your St. Jude credentials. Congrats! You're now on Slack. Download the desktop app by visiting https://slack.com/download . The instructions differ depending on whether you are on a Windows/Mac/Linux machine. Once you install and start up the app, the sign-in process should look the same as step 1. To join the #stjudecloud-helpdesk channel, you can click the word \"Channels\" in the left sidebar and search for #stjudecloud-helpdesk . If you have issues, please see the official Slack guide on joining a channel. You should now see a screen similar to the one included below. You can type your questions into the chat box at the bottom. See you there! Login to St. Jude Cloud \u00b6 Signing in to St. Jude Cloud is similarly easy \u2014 if you have a SJCRH account, you already have a St. Jude Cloud account. Steps Visit https://cloud.stjude.org , log in with your St. Jude credentials, and fill in the basic profile page to your satisfaction if prompted. Congrats! You're now logged in to St. Jude Cloud. This should be sufficient for the purposes of this guide. For more detailed information, see our standard guide on accounts and billing . Tip As you navigate around St. Jude Cloud, you can click the DNAnexus logo in the top left to go back to the home screen at any time. Enable Microsoft Azure \u00b6 Steps Change your preferences to pull cloud resources from Microsoft Azure instead of Amazon AWS by default. To do this, (i) click on your profile in the top right corner, (ii) select \"My Profile\", and (iii) ensure \"Azure US (West)\" is checked. See the pictures below for more detail. Create your First Project \u00b6 Steps Select \"New Project\" in the upper left part of the screen, fill in the form as instructed in the image below, and click \"Create Project\". You should be redirected to your first DNAnexus project. We highly recommend you read through DNAnexus's dedicated guide to learn about projects: how you can navigate them, how you can share them with collaborators, and how they are billed. Set Up your Billing Account \u00b6 Billing is handled in St. Jude Cloud by creating and managing a DNAnexus billing organization (or \"org\"). Each project in DNAnexus is associated with a single org (you had to specify one when you created a project in the last step), and all compute and storage costs are billed to that org. By default, each new user on St. Jude Cloud gets a billing org called user-[username] with $50 of trial credit. You can view the billing orgs available to your account here . Info We intend to prioritize funds for Discovery Sponsorship Program participants. However, there is the potential for left-over funds to be available. If you are a lab that would like to use the cloud but do not have funds available, please let us know at support@stjude.cloud . Steps If your lab has been accepted into the Discovery Sponsorship Program, we will create an organization for you to bill your project against. If we have not already, we will be in contact with you shortly concerning this information. Otherwise, your lab will need to set up a billing org for itself. Please see our Account and Billing sections below. Billing for St. Jude Employees Billing for Non-St. Jude Employees","title":"Getting started"},{"location":"guides/covid-19/getting-started/#apply-for-discovery-sponsorship-program","text":"Please see the Discovery Sponsorship Program (DSP) section in the figure below for an overview of program benefits. Applying for the program is easy - just fill out the application form and submit it to discoveryprogram@stjude.org . The last day to apply is April 9 th .","title":"Apply for Discovery Sponsorship Program"},{"location":"guides/covid-19/getting-started/#joining-slack","text":"The St. Jude Cloud team uses Slack to communicate on a day-to-day basis. When standing up a help desk, we decided to offer that functionality on Slack to allow our community to answer questions alongside our team. If you are an employee at SJCRH, you already have a Slack account (at no cost to you) \u2014 you might even like it so much, you decide to use it during this work from home period! If you have any issues with the instructions below, you can email helpdesk@stjude.org or support@stjude.cloud . Steps Navigate to https://stjude.slack.com , click \"Sign in with your St. Jude Account\", and enter your St. Jude credentials. Congrats! You're now on Slack. Download the desktop app by visiting https://slack.com/download . The instructions differ depending on whether you are on a Windows/Mac/Linux machine. Once you install and start up the app, the sign-in process should look the same as step 1. To join the #stjudecloud-helpdesk channel, you can click the word \"Channels\" in the left sidebar and search for #stjudecloud-helpdesk . If you have issues, please see the official Slack guide on joining a channel. You should now see a screen similar to the one included below. You can type your questions into the chat box at the bottom. See you there!","title":"Joining Slack"},{"location":"guides/covid-19/getting-started/#login-to-st-jude-cloud","text":"Signing in to St. Jude Cloud is similarly easy \u2014 if you have a SJCRH account, you already have a St. Jude Cloud account. Steps Visit https://cloud.stjude.org , log in with your St. Jude credentials, and fill in the basic profile page to your satisfaction if prompted. Congrats! You're now logged in to St. Jude Cloud. This should be sufficient for the purposes of this guide. For more detailed information, see our standard guide on accounts and billing . Tip As you navigate around St. Jude Cloud, you can click the DNAnexus logo in the top left to go back to the home screen at any time.","title":"Login to St. Jude Cloud"},{"location":"guides/covid-19/getting-started/#enable-microsoft-azure","text":"Steps Change your preferences to pull cloud resources from Microsoft Azure instead of Amazon AWS by default. To do this, (i) click on your profile in the top right corner, (ii) select \"My Profile\", and (iii) ensure \"Azure US (West)\" is checked. See the pictures below for more detail.","title":"Enable Microsoft Azure"},{"location":"guides/covid-19/getting-started/#create-your-first-project","text":"Steps Select \"New Project\" in the upper left part of the screen, fill in the form as instructed in the image below, and click \"Create Project\". You should be redirected to your first DNAnexus project. We highly recommend you read through DNAnexus's dedicated guide to learn about projects: how you can navigate them, how you can share them with collaborators, and how they are billed.","title":"Create your First Project"},{"location":"guides/covid-19/getting-started/#set-up-your-billing-account","text":"Billing is handled in St. Jude Cloud by creating and managing a DNAnexus billing organization (or \"org\"). Each project in DNAnexus is associated with a single org (you had to specify one when you created a project in the last step), and all compute and storage costs are billed to that org. By default, each new user on St. Jude Cloud gets a billing org called user-[username] with $50 of trial credit. You can view the billing orgs available to your account here . Info We intend to prioritize funds for Discovery Sponsorship Program participants. However, there is the potential for left-over funds to be available. If you are a lab that would like to use the cloud but do not have funds available, please let us know at support@stjude.cloud . Steps If your lab has been accepted into the Discovery Sponsorship Program, we will create an organization for you to bill your project against. If we have not already, we will be in contact with you shortly concerning this information. Otherwise, your lab will need to set up a billing org for itself. Please see our Account and Billing sections below. Billing for St. Jude Employees Billing for Non-St. Jude Employees","title":"Set Up your Billing Account"},{"location":"guides/covid-19/interactive-node/","text":"Interactive Nodes in the Cloud Apart from creating and running cloud apps , you can request an interactive node in the cloud to use for iterative development. This can be particularly useful if you want to run some quick analyses on the cloud, want to run tools without creating an app, or want to submit jobs similar to bsub on the local St. Jude HPC. This guide assumes that you have a DNAnexus account and have dxpy installed on your machine (view the local data upload guide for instructions on how to install dxpy ). Warning All instructions in this guide should be run from your local machine, not the HPC cluster. Overview \u00b6 There are two different experiences for doing interactive or ad-hoc analysis in the cloud: Cloud Workstations . Cloud workstations are a mature offering in the DNAnexus ecosystem, and you can use them in your production work. DNAnexus has a full guide on how to use cloud workstations. Unfortunately, they do not fully replicate the experience of an interactive node on the cluster: each time you ssh into a new cloud workstation, you get a blank machine with no dependencies or data installed. Thus, you need to configure your environment, download data to the node using dx , and upload results back to DNAnexus using dx . Interactive Nodes . Interactive nodes were created very recently between a partnership with St. Jude and DNAnexus. They offer a more complete alternative to interactive nodes in the HPC cluster, but the experience is currently in alpha (meaning that there are likely to be bugs and it is not ready for production use). We are actively looking for labs to partner with us to develop out this experience, so apply for the Discovery Sponsorship Program if you are interested! In this guide, we will briefly mention how to use Cloud Workstations and then spend the rest of the guide explaining how to use our new Interactive Node experience. Cloud Workstations \u00b6 A Cloud Workstation is a fresh node on the cloud that can be used to run any command with access to data in your projects. You will be given root access to the node so you can download and install any tool you require. Since this is an interactive session, you will be charged for the duration of the session so it is important to terminate the session after use. Configuring SSH \u00b6 Run dx ssh_config to configure your account to allow use of SSH connections to the node. Connecting to the workstation \u00b6 To start an interactive workstation, first select the project you would like to start it in. dx select \"project-alpha\" or just dx select to select from a list of your projects interactively. Next, run dx run app-cloud_workstation --ssh . You can set the maximum session length for this session or continue with the default options. Tip The default node size for the cloud workstations are mem1_ssd1_x2. If you want to request a larger node size, you can specify it by adding the --instance-type option. Check the advanced options section at the bottom for more information. Setting up workspace \u00b6 Once you are connected to the node, you will have access to download and install tools to the node and use them. The node is a clean linux environment with the dx command line tool already installed. In order to upload and download files from your DNAnexus project, you must first run the following commands. unset DX_WORKSPACE_ID dx cd \"project-alpha:/\" Downloading files from your DNAnexus project \u00b6 The node has access to the data in your projects. To download a file, test.bam , from your parent project, just run dx download test.bam . To download a file from any project you have access to, just specify the project and path to the file in your download command like dx download project-name:/path/to/test.bam . Once you have all the tools and data you require, you can use the workstation as a general-purpose workstation to run analyses. Note that in the Cloud Workstation, all files you want to use have to fit onto the local hard disk. Uploading files back to your project \u00b6 Since the node is transient and will be deleted after the session is terminated, it is important to upload any required files to your project. You can do that by running dx upload output.bam or dx upload --path \"$project-alpha:\" output.bam if you selected another project in the workstation. Terminating the session \u00b6 By default, the session will end after the max session time set when the workstation was first started. You can terminate the session when you are done working by exit ing out of the terminal. You will be asked whether you want to terminate the job, enter 'y' to terminate. You can also terminate the job by going to the Monitor tab of your project in DNAnexus and terminate the running job from the website. For more information about cloud workstations, please refer to the DNAnexus documentation . Interactive Nodes \u00b6 Danger The interactive node in the cloud experience was created specifically in response to the fully remote working situation. The experience is currently an alpha release and is not suitable for production use. Additionally, this guide will be updated each time we improve the experience, so please come back regularly to see how we are changing things. Info We are looking for labs across St. Jude to partner with us and tell us about their experience using the interactive node. If you are interested, please apply for the discovery sponsorship program . Cloud workstations are good for interactive work, but they require you to upload/download data from your projects. They also do not save your working environment so any tools you installed or changes you made to the machine will be lost when the session is terminated. The Interactive Node experience, sometimes referred to by its codename \"CWIC\" (cloud workstations in containers), solves these issues by saving your environment and letting you work with your data on the cloud without manually downloading it to the node. Setting up your Docker Hub account \u00b6 The workstation uses Docker images pushed to a Docker Hub repository to save your environment. To get started, go to Docker Hub and sign in or create an account. Every Docker Hub account is given one free private repository. It is highly recommended to use a private repository as this will be your working environment. Once you have a Docker Hub account, go to your \"Account Settings\", then \"Security\" and create a new access token. You can give it a descriptive name and copy the token. The access token will be needed for the credentials file below. Creating a credentials file \u00b6 Create a file with the template below and fill in your Docker Hub token and Docker Hub username in the appropriate places. { \"docker_registry\" : { \"token\" : \"<YOUR_DOCKERHUB_TOKEN>\" , \"organization\" : \"<YOUR_DOCKERHUB_USERNAME>\" , \"username\" : \"<YOUR_DOCKERHUB_USERNAME>\" , \"registry\" : \"docker.io\" } } Info If you would rather use a quay.io repository, you can use your quay credentials in the credentials file instead. Once you have made your credentials file on your computer, make a new DNAnexus project to save your credentials using dx new project . Upload the credentials file to your project by running dx upload creds.txt . It is recommended to save your credentials in a separate, private DNAnexus project to ensure that others do not have access to it. Starting an interactive terminal session \u00b6 The following command will run the app using the credentials you provided and will log you into the node after it boots up. dx run app-cwic -icredentials=mycredentials:creds.txt --ssh -y or replace mycredentials with the name of the DNAnexus project with your credentials file. If you have SSH issues while trying to connect to the job, make sure your SSH keys are configured properly . Working on the CWIC node \u00b6 Once the node starts, you will be taken to the home directory of the CWIC node. This node is an ubuntu environment and you can install or run any commands you want. For example, you can install samtools by running sudo apt install samtools . There are two main directories to work with data: /scratch/ - This is the directory local to the node. You can use this directory to save any intermediate or temporary results. You can run tools here but all the data in this directory will be deleted once the node is terminated. /project/ - This directory contains your DNAnexus project and the data in it. If you copy or move files to this directory, it saves to your DNAnexus project, which is a persistent storage. You can go to /project/<YOUR_DX_PROJECT_NAME> and see the files in your DNAnexus project. Upload some data to your project from a local machine for testing in the interactive node \u2014 here, we assume a BAM file uploaded from a laptop called sample.bam . Once data is uploaded to your DNAnexus project, you can access it on your CWIC node at /project/<YOUR_DX_PROJECT_NAME>/test.bam . For instance, when running samtools index /project/<YOUR_DX_PROJECT_NAME>/test.bam , you will find the index file samtools creates is saved to your cloud project. Adding bioinformatics tools to your environment \u00b6 We recommend installing Anaconda to manage any Python or R packages in your CWIC environment. To install miniconda (a minimal installation of anaconda), run curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc Follow the instructions and select 'yes' to install conda and initialize it. After installing conda, we recommend adding the bioconda channel, which provides many bioinformatics packages. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge To install a package such as bwa , simply run conda install bwa -y You can also create a new environment with conda called bio and install available packages like so: conda create -n bio bwa bowtie star -y conda activate bio bwa Saving your environment \u00b6 If you installed samtools , or any other tool to the node and want to save your environment, you can run dx-save-cwic . This will save the environment to your Docker Hub repository. Unfortunately, this is a manual step at the moment: in a future iteration, we plan to have this save your environment automatically. The next time you launch a CWIC node in this project, it will put you in an node with your saved environment. Therefore you will not need to reinstall samtools or any other tool you had in your environment. Running batch jobs \u00b6 We can dispatch non-interactive jobs from the node to parallelize analyses similar to a bsub experience on the HPC. First, you need to login to DNAnexus on the node. dx login --noprojects --token <dnanexus-user-token-from-ui> You can use samtools to split the bam by chromosome like below by specifying a command with the CWIC app. This will run the specified command with the saved environment and you can save the outputs to the /project directory which will save it in your DNAnexus project. root@cwic:~# chromosomes =( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ) root@cwic:~# for chr in ${ chromosomes [@] } ; do echo $chr ; dx run app-cwic \\ -icredentials = <DX_PROJECT_WITH_CREDS>:creds.txt \\ -icmd = \"samtools view -b /project/<YOUR_DX_PROJECT_NAME>/test.bam ${ chr } -o /project/<YOUR_DX_PROJECT_NAME>/bam_ ${ chr } .bam;\" \\ -y ; done After your jobs have finished running. You can run dx-reload-project to refresh the /project directory and see the newly added chromosome slices. Reloading project directory \u00b6 You may not see the updated files in your /project/<YOUR_PROJECT_NAME> directory immediately after they are added. In order to reload the project directory on the CWIC node with the latest files from your DNAnexus project, run dx-reload-project and you will see any new files. Unfortunately, this is a manual step at the moment: in a future iteration, we plan to have this update your files automatically. If you get a message such as umount: /project: target is busy. , cd into a directory other than /project and try reloading again. Saving any project updates \u00b6 Updates to any files in the /project directory only occur every 5 minutes. In order to propagate any recent updates, run dx-save-project to save the files to the DNAnexus project. In the future, we plan to have file syncing happen automatically whenever you update a file. Terminating the CWIC node \u00b6 Since the CWIC node is an interactive job, it gets billed for the duration of the job. Therefore it is important to terminate the node once you are done working. Save your work and environment, if needed, by running dx-save-project and dx-save-cwic respectively. To quit the node, type exit twice to get into the app execution environment. Press Ctrl+c to quit the CWIC app and type exit twice to get out of the terminal completely. You will be prompted to terminate the job, type 'y' to terminate the job. You can check if the node is still running by checking the Monitor tab in your project on the DNAnexus website. Alternatively, you can terminate the job from the Monitor tab. Advanced options \u00b6 Changing instance type \u00b6 If you require more or less runtime requirements for your nodes, you can change the instance type by specifying the flag --instance-type with a valid instance type from this list . dx run app-cloud_workstation --instance-type azure:mem1_ssd1_x16 --ssh or dx run app-cwic -icredentials = <DX_PROJECT_NAME_WITH_CREDS>:creds.txt --instance-type mem1_ssd1_x4 --ssh -y This is useful when you want to run some non-interactive jobs that have different memory or storage requirements. If you have any questions or suggestions on how we can improve this guide, please file an issue , contact us at https://stjude.cloud/contact , or email us at support@stjude.cloud . Making a Docker Hub repository private \u00b6 By default, the workstation creates a new public repository in Docker Hub. It is best practice to use a private repository so that your work environment is not publicly visible on Docker Hub. Follow the steps below to update an existing public repository to a private one. This should be done after you have already run dx-save-cwic once in the interactive session. First, go to your repositories page and click on the repository you want to make private. Next, go to the 'Settings' tab and click on the 'Make private' button. Type the name of the repository and click on the 'Make private' button. Finally, you can see the repository is now set to private and you can continue using interactive sessions as normal.","title":"Interactive nodes"},{"location":"guides/covid-19/interactive-node/#overview","text":"There are two different experiences for doing interactive or ad-hoc analysis in the cloud: Cloud Workstations . Cloud workstations are a mature offering in the DNAnexus ecosystem, and you can use them in your production work. DNAnexus has a full guide on how to use cloud workstations. Unfortunately, they do not fully replicate the experience of an interactive node on the cluster: each time you ssh into a new cloud workstation, you get a blank machine with no dependencies or data installed. Thus, you need to configure your environment, download data to the node using dx , and upload results back to DNAnexus using dx . Interactive Nodes . Interactive nodes were created very recently between a partnership with St. Jude and DNAnexus. They offer a more complete alternative to interactive nodes in the HPC cluster, but the experience is currently in alpha (meaning that there are likely to be bugs and it is not ready for production use). We are actively looking for labs to partner with us to develop out this experience, so apply for the Discovery Sponsorship Program if you are interested! In this guide, we will briefly mention how to use Cloud Workstations and then spend the rest of the guide explaining how to use our new Interactive Node experience.","title":"Overview"},{"location":"guides/covid-19/interactive-node/#cloud-workstations","text":"A Cloud Workstation is a fresh node on the cloud that can be used to run any command with access to data in your projects. You will be given root access to the node so you can download and install any tool you require. Since this is an interactive session, you will be charged for the duration of the session so it is important to terminate the session after use.","title":"Cloud Workstations"},{"location":"guides/covid-19/interactive-node/#configuring-ssh","text":"Run dx ssh_config to configure your account to allow use of SSH connections to the node.","title":"Configuring SSH"},{"location":"guides/covid-19/interactive-node/#connecting-to-the-workstation","text":"To start an interactive workstation, first select the project you would like to start it in. dx select \"project-alpha\" or just dx select to select from a list of your projects interactively. Next, run dx run app-cloud_workstation --ssh . You can set the maximum session length for this session or continue with the default options. Tip The default node size for the cloud workstations are mem1_ssd1_x2. If you want to request a larger node size, you can specify it by adding the --instance-type option. Check the advanced options section at the bottom for more information.","title":"Connecting to the workstation"},{"location":"guides/covid-19/interactive-node/#setting-up-workspace","text":"Once you are connected to the node, you will have access to download and install tools to the node and use them. The node is a clean linux environment with the dx command line tool already installed. In order to upload and download files from your DNAnexus project, you must first run the following commands. unset DX_WORKSPACE_ID dx cd \"project-alpha:/\"","title":"Setting up workspace"},{"location":"guides/covid-19/interactive-node/#downloading-files-from-your-dnanexus-project","text":"The node has access to the data in your projects. To download a file, test.bam , from your parent project, just run dx download test.bam . To download a file from any project you have access to, just specify the project and path to the file in your download command like dx download project-name:/path/to/test.bam . Once you have all the tools and data you require, you can use the workstation as a general-purpose workstation to run analyses. Note that in the Cloud Workstation, all files you want to use have to fit onto the local hard disk.","title":"Downloading files from your DNAnexus project"},{"location":"guides/covid-19/interactive-node/#uploading-files-back-to-your-project","text":"Since the node is transient and will be deleted after the session is terminated, it is important to upload any required files to your project. You can do that by running dx upload output.bam or dx upload --path \"$project-alpha:\" output.bam if you selected another project in the workstation.","title":"Uploading files back to your project"},{"location":"guides/covid-19/interactive-node/#terminating-the-session","text":"By default, the session will end after the max session time set when the workstation was first started. You can terminate the session when you are done working by exit ing out of the terminal. You will be asked whether you want to terminate the job, enter 'y' to terminate. You can also terminate the job by going to the Monitor tab of your project in DNAnexus and terminate the running job from the website. For more information about cloud workstations, please refer to the DNAnexus documentation .","title":"Terminating the session"},{"location":"guides/covid-19/interactive-node/#interactive-nodes","text":"Danger The interactive node in the cloud experience was created specifically in response to the fully remote working situation. The experience is currently an alpha release and is not suitable for production use. Additionally, this guide will be updated each time we improve the experience, so please come back regularly to see how we are changing things. Info We are looking for labs across St. Jude to partner with us and tell us about their experience using the interactive node. If you are interested, please apply for the discovery sponsorship program . Cloud workstations are good for interactive work, but they require you to upload/download data from your projects. They also do not save your working environment so any tools you installed or changes you made to the machine will be lost when the session is terminated. The Interactive Node experience, sometimes referred to by its codename \"CWIC\" (cloud workstations in containers), solves these issues by saving your environment and letting you work with your data on the cloud without manually downloading it to the node.","title":"Interactive Nodes"},{"location":"guides/covid-19/interactive-node/#setting-up-your-docker-hub-account","text":"The workstation uses Docker images pushed to a Docker Hub repository to save your environment. To get started, go to Docker Hub and sign in or create an account. Every Docker Hub account is given one free private repository. It is highly recommended to use a private repository as this will be your working environment. Once you have a Docker Hub account, go to your \"Account Settings\", then \"Security\" and create a new access token. You can give it a descriptive name and copy the token. The access token will be needed for the credentials file below.","title":"Setting up your Docker Hub account"},{"location":"guides/covid-19/interactive-node/#creating-a-credentials-file","text":"Create a file with the template below and fill in your Docker Hub token and Docker Hub username in the appropriate places. { \"docker_registry\" : { \"token\" : \"<YOUR_DOCKERHUB_TOKEN>\" , \"organization\" : \"<YOUR_DOCKERHUB_USERNAME>\" , \"username\" : \"<YOUR_DOCKERHUB_USERNAME>\" , \"registry\" : \"docker.io\" } } Info If you would rather use a quay.io repository, you can use your quay credentials in the credentials file instead. Once you have made your credentials file on your computer, make a new DNAnexus project to save your credentials using dx new project . Upload the credentials file to your project by running dx upload creds.txt . It is recommended to save your credentials in a separate, private DNAnexus project to ensure that others do not have access to it.","title":"Creating a credentials file"},{"location":"guides/covid-19/interactive-node/#starting-an-interactive-terminal-session","text":"The following command will run the app using the credentials you provided and will log you into the node after it boots up. dx run app-cwic -icredentials=mycredentials:creds.txt --ssh -y or replace mycredentials with the name of the DNAnexus project with your credentials file. If you have SSH issues while trying to connect to the job, make sure your SSH keys are configured properly .","title":"Starting an interactive terminal session"},{"location":"guides/covid-19/interactive-node/#working-on-the-cwic-node","text":"Once the node starts, you will be taken to the home directory of the CWIC node. This node is an ubuntu environment and you can install or run any commands you want. For example, you can install samtools by running sudo apt install samtools . There are two main directories to work with data: /scratch/ - This is the directory local to the node. You can use this directory to save any intermediate or temporary results. You can run tools here but all the data in this directory will be deleted once the node is terminated. /project/ - This directory contains your DNAnexus project and the data in it. If you copy or move files to this directory, it saves to your DNAnexus project, which is a persistent storage. You can go to /project/<YOUR_DX_PROJECT_NAME> and see the files in your DNAnexus project. Upload some data to your project from a local machine for testing in the interactive node \u2014 here, we assume a BAM file uploaded from a laptop called sample.bam . Once data is uploaded to your DNAnexus project, you can access it on your CWIC node at /project/<YOUR_DX_PROJECT_NAME>/test.bam . For instance, when running samtools index /project/<YOUR_DX_PROJECT_NAME>/test.bam , you will find the index file samtools creates is saved to your cloud project.","title":"Working on the CWIC node"},{"location":"guides/covid-19/interactive-node/#adding-bioinformatics-tools-to-your-environment","text":"We recommend installing Anaconda to manage any Python or R packages in your CWIC environment. To install miniconda (a minimal installation of anaconda), run curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc Follow the instructions and select 'yes' to install conda and initialize it. After installing conda, we recommend adding the bioconda channel, which provides many bioinformatics packages. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge To install a package such as bwa , simply run conda install bwa -y You can also create a new environment with conda called bio and install available packages like so: conda create -n bio bwa bowtie star -y conda activate bio bwa","title":"Adding bioinformatics tools to your environment"},{"location":"guides/covid-19/interactive-node/#saving-your-environment","text":"If you installed samtools , or any other tool to the node and want to save your environment, you can run dx-save-cwic . This will save the environment to your Docker Hub repository. Unfortunately, this is a manual step at the moment: in a future iteration, we plan to have this save your environment automatically. The next time you launch a CWIC node in this project, it will put you in an node with your saved environment. Therefore you will not need to reinstall samtools or any other tool you had in your environment.","title":"Saving your environment"},{"location":"guides/covid-19/interactive-node/#running-batch-jobs","text":"We can dispatch non-interactive jobs from the node to parallelize analyses similar to a bsub experience on the HPC. First, you need to login to DNAnexus on the node. dx login --noprojects --token <dnanexus-user-token-from-ui> You can use samtools to split the bam by chromosome like below by specifying a command with the CWIC app. This will run the specified command with the saved environment and you can save the outputs to the /project directory which will save it in your DNAnexus project. root@cwic:~# chromosomes =( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ) root@cwic:~# for chr in ${ chromosomes [@] } ; do echo $chr ; dx run app-cwic \\ -icredentials = <DX_PROJECT_WITH_CREDS>:creds.txt \\ -icmd = \"samtools view -b /project/<YOUR_DX_PROJECT_NAME>/test.bam ${ chr } -o /project/<YOUR_DX_PROJECT_NAME>/bam_ ${ chr } .bam;\" \\ -y ; done After your jobs have finished running. You can run dx-reload-project to refresh the /project directory and see the newly added chromosome slices.","title":"Running batch jobs"},{"location":"guides/covid-19/interactive-node/#reloading-project-directory","text":"You may not see the updated files in your /project/<YOUR_PROJECT_NAME> directory immediately after they are added. In order to reload the project directory on the CWIC node with the latest files from your DNAnexus project, run dx-reload-project and you will see any new files. Unfortunately, this is a manual step at the moment: in a future iteration, we plan to have this update your files automatically. If you get a message such as umount: /project: target is busy. , cd into a directory other than /project and try reloading again.","title":"Reloading project directory"},{"location":"guides/covid-19/interactive-node/#saving-any-project-updates","text":"Updates to any files in the /project directory only occur every 5 minutes. In order to propagate any recent updates, run dx-save-project to save the files to the DNAnexus project. In the future, we plan to have file syncing happen automatically whenever you update a file.","title":"Saving any project updates"},{"location":"guides/covid-19/interactive-node/#terminating-the-cwic-node","text":"Since the CWIC node is an interactive job, it gets billed for the duration of the job. Therefore it is important to terminate the node once you are done working. Save your work and environment, if needed, by running dx-save-project and dx-save-cwic respectively. To quit the node, type exit twice to get into the app execution environment. Press Ctrl+c to quit the CWIC app and type exit twice to get out of the terminal completely. You will be prompted to terminate the job, type 'y' to terminate the job. You can check if the node is still running by checking the Monitor tab in your project on the DNAnexus website. Alternatively, you can terminate the job from the Monitor tab.","title":"Terminating the CWIC node"},{"location":"guides/covid-19/interactive-node/#advanced-options","text":"","title":"Advanced options"},{"location":"guides/covid-19/interactive-node/#changing-instance-type","text":"If you require more or less runtime requirements for your nodes, you can change the instance type by specifying the flag --instance-type with a valid instance type from this list . dx run app-cloud_workstation --instance-type azure:mem1_ssd1_x16 --ssh or dx run app-cwic -icredentials = <DX_PROJECT_NAME_WITH_CREDS>:creds.txt --instance-type mem1_ssd1_x4 --ssh -y This is useful when you want to run some non-interactive jobs that have different memory or storage requirements. If you have any questions or suggestions on how we can improve this guide, please file an issue , contact us at https://stjude.cloud/contact , or email us at support@stjude.cloud .","title":"Changing instance type"},{"location":"guides/covid-19/interactive-node/#making-a-docker-hub-repository-private","text":"By default, the workstation creates a new public repository in Docker Hub. It is best practice to use a private repository so that your work environment is not publicly visible on Docker Hub. Follow the steps below to update an existing public repository to a private one. This should be done after you have already run dx-save-cwic once in the interactive session. First, go to your repositories page and click on the repository you want to make private. Next, go to the 'Settings' tab and click on the 'Make private' button. Type the name of the repository and click on the 'Make private' button. Finally, you can see the repository is now set to private and you can continue using interactive sessions as normal.","title":"Making a Docker Hub repository private"},{"location":"guides/covid-19/upload-cluster/","text":"Uploading Data from St. Jude HPC to DNAnexus This guide describes how to upload data from St. Jude's research computing cluster to DNAnexus. It covers logging in to the HPC, creating an interactive session, loading the DNAnexus upload agent, and uploading files to DNAnexus. The research cluster is restricted to St. Jude employees, as it is only accessible on St. Jude's intranet. If you are reading this page and work at another institution, please work with your HPC staff on translating the steps to your architecture. Logging in to the HPC \u00b6 The SSH (Secure Shell) protocol is used to log in to hpc , the hostname of the entry point into St. Jude's research cluster. SSH provides a secure method to log in to a remote computer. Regardless of platform, logging in requires a St. Jude username and will prompt you for your password. They are the same username and password used to log in to all St. Jude services. Windows \u00b6 Windows does not have an SSH client preinstalled. As of Windows 10 1809, OpenSSH is included as a feature that can be installed. Open the Settings app (search \"Settings\" in the Start menu). Under Apps > Apps & features > Apps & features, click on Optional features . Under the OpenSSH Client entry, click Install . Open PowerShell and run ssh <username>@hpc Alternatively (or on older versions of Windows), install and use the terminal emulator PuTTY . macOS \u00b6 macOS includes OpenSSH by default. Open Terminal and run $ ssh <username>@hpc Linux \u00b6 Most Linux distributions have OpenSSH installed by default. Open a terminal and run $ ssh <username>@hpc Creating an interactive session \u00b6 When logging in to hpc , you are placed on a head node , a controlled gateway configured to allow external access to the cluster. This node is not meant for computation, which should be done on a cluster node instead. To move to a cluster node, start an interactive session. The cluster's workload is managed by IBM Spectrum LSF , and even though LSF commands can be used to create an interactive session, the High-Performance Computer Facility (HPCF) provides a convenience script named hpcf_interactive for a simpler invocation. $ hpcf_interactive This moves you from the head node to a node in the research cluster, and tasks that require CPU time, memory, high bandwidth network access, etc., can be executed. When an interactive session starts, the prompt will look similar to the following: Job <99871669> is submitted to queue <interactive>. <<Waiting for dispatch ...>> <<Starting on nodecn013>> [username@nodecn013 ~]$ DNAnexus Upload Agent \u00b6 DNAnexus provides multiple methods to upload files to their platform. In this section, Upload Agent (UA) is used. It is simple to use and supports resuming interrupted transfers. Setup \u00b6 You can load the DNAnexus upload agent command line tool, ua , by loading the dx-ua module. module load dx-ua/1.5.30 Next, you'll need to configure the ua command line tool with access to your St. Jude Cloud account. Rather than exposing your username and password, best practice is to generate an authentication token that lives for a short period of time instead. You can do so by following this guide on how to generate a DNAnexus authentication token . Replace <auth-token> with your own token in the example below. $ export DX_SECURITY_CONTEXT = '{\"auth_token_type\": \"bearer\", \"auth_token\": \"<auth-token>\"}' This must be set once for ever new interactive session started. Uploading files \u00b6 Basic usage \u00b6 With DNAnexus Upload Agent loaded, files can be uploaded by running ua . To get acquainted with the command, you can view the help message for ua . ua --help The simplest usage of ua is providing the DNAnexus project to upload to and a source file. $ ua --do-not-compress --project <project-name-or-id> <src> For example, with a DNAnexus project named flagstat and a file named sample.1.bam , the command would be $ ua --do-not-compress --project flagstat sample.1.bam Why is --do-not-compress always set? Upload Agent uses an unfortunate default where uncompressed files are automatically gzipped. For example, uploading the text file samplesheet.txt results in the file samplesheet.txt.gz on DNAnexus. This is confusing and unexpected, and file management is more straightforward with the option disabled. Batched jobs \u00b6 When working in an interactive session, if the session is closed (e.g., a network disconnection or you quit the terminal), all running commands are killed. To avoid this from happening, a job is submitted in its place, which continues to run even after the session is closed. To submit a job, use the LSF command bsub , where -P is an arbitrary project name for the job submission. When uploading a large batch of files, HPCF requests the /stjudecloud/uploads job group be used to rate limit upload jobs. This can be done using the -g option when submitting a job. $ bsub \\ -P dx-upload \\ -g /stjudecloud/uploads \\ -R \"rusage[mem=2882]\" \\ ua \\ --do-not-compress \\ --project <project-name-or-id> \\ <src> Where does the 2882 MiB resource requirement come from? There is a note in the source code of Upload Agent that gives an estimate of how much memory is used for a transfer: <read-threads> + 2 * (<compress-threads> + <upload-threads>) * <chunk-size> Thus, using the default values (see ua --help ) and adding 20% as a buffer: (2 + 2 * (8 + 8) * 75 MiB) * 1.2 = ~2882 MiB Further examples \u00b6 The following examples are common usages of Upload Agent. If you use any of these options, please be sure submit them to the HPC job group as shown in batched jobs to ensure the number of uploads is throttled. Upload multiple files \u00b6 ua takes multiple source arguments. $ ua --do-not-compress --project <project-name-or-id> <src...> For example, for two files sample.1.bam and sample.1.bam.bai , the command is $ ua --do-not-compress --project flagstat sample.1.bam sample.1.bam.bai Upload a folder in DNAnexus \u00b6 By default, all files uploaded via ua get placed at the root of the project, i.e., / . To upload to a particular directory, use the --folder option. $ ua --do-not-compress --project <project-name-or-id> --folder /data sample.1.bam The resulting uploaded file will be located at /data/sample.1.bam . The directory does not have a previously exist in the DNAnexus project and will be created automatically, along with its parents (similar to mkdir -p ). Upload a local directory \u00b6 When uploading a directory, ua will only use the files at the top level (similar to find data -type f -maxdepth 1 ). For example, take the following directory structure and ua command. $ tree data data \u251c\u2500\u2500 extra \u2502 \u251c\u2500\u2500 sample.2.bam \u2502 \u2514\u2500\u2500 sample.2.bam.bai \u251c\u2500\u2500 sample.1.bam \u2514\u2500\u2500 sample.1.bam.bai 1 directory, 4 files $ ua --do-not-compress --project <project-name-or-id> data The resulting files will be /sample.1.bam and /sample.1.bam.bai . Use the --folder option and name it the same as the local source directory to include the directory on DNAnexus. $ ua --do-not-compress --project <project-name-or-id> --folder /data data The resulting files will be /data/sample.1.bam and /data/sample1.bam.bai . To include the subdirectories, use the --recursive option. $ ua --do-not-compress --project <project-name-or-id> --folder /data --recursive data Thus, using both --folder and --recursive uploads an entire directory: /data/sample.1.bam , /data/sample.1.bam.bai , /data/extras/sample.2.bam , and /data/extras/sample.2.bam.bai .","title":"Data upload (HPC)"},{"location":"guides/covid-19/upload-cluster/#logging-in-to-the-hpc","text":"The SSH (Secure Shell) protocol is used to log in to hpc , the hostname of the entry point into St. Jude's research cluster. SSH provides a secure method to log in to a remote computer. Regardless of platform, logging in requires a St. Jude username and will prompt you for your password. They are the same username and password used to log in to all St. Jude services.","title":"Logging in to the HPC"},{"location":"guides/covid-19/upload-cluster/#windows","text":"Windows does not have an SSH client preinstalled. As of Windows 10 1809, OpenSSH is included as a feature that can be installed. Open the Settings app (search \"Settings\" in the Start menu). Under Apps > Apps & features > Apps & features, click on Optional features . Under the OpenSSH Client entry, click Install . Open PowerShell and run ssh <username>@hpc Alternatively (or on older versions of Windows), install and use the terminal emulator PuTTY .","title":"Windows"},{"location":"guides/covid-19/upload-cluster/#macos","text":"macOS includes OpenSSH by default. Open Terminal and run $ ssh <username>@hpc","title":"macOS"},{"location":"guides/covid-19/upload-cluster/#linux","text":"Most Linux distributions have OpenSSH installed by default. Open a terminal and run $ ssh <username>@hpc","title":"Linux"},{"location":"guides/covid-19/upload-cluster/#creating-an-interactive-session","text":"When logging in to hpc , you are placed on a head node , a controlled gateway configured to allow external access to the cluster. This node is not meant for computation, which should be done on a cluster node instead. To move to a cluster node, start an interactive session. The cluster's workload is managed by IBM Spectrum LSF , and even though LSF commands can be used to create an interactive session, the High-Performance Computer Facility (HPCF) provides a convenience script named hpcf_interactive for a simpler invocation. $ hpcf_interactive This moves you from the head node to a node in the research cluster, and tasks that require CPU time, memory, high bandwidth network access, etc., can be executed. When an interactive session starts, the prompt will look similar to the following: Job <99871669> is submitted to queue <interactive>. <<Waiting for dispatch ...>> <<Starting on nodecn013>> [username@nodecn013 ~]$","title":"Creating an interactive session"},{"location":"guides/covid-19/upload-cluster/#dnanexus-upload-agent","text":"DNAnexus provides multiple methods to upload files to their platform. In this section, Upload Agent (UA) is used. It is simple to use and supports resuming interrupted transfers.","title":"DNAnexus Upload Agent"},{"location":"guides/covid-19/upload-cluster/#setup","text":"You can load the DNAnexus upload agent command line tool, ua , by loading the dx-ua module. module load dx-ua/1.5.30 Next, you'll need to configure the ua command line tool with access to your St. Jude Cloud account. Rather than exposing your username and password, best practice is to generate an authentication token that lives for a short period of time instead. You can do so by following this guide on how to generate a DNAnexus authentication token . Replace <auth-token> with your own token in the example below. $ export DX_SECURITY_CONTEXT = '{\"auth_token_type\": \"bearer\", \"auth_token\": \"<auth-token>\"}' This must be set once for ever new interactive session started.","title":"Setup"},{"location":"guides/covid-19/upload-cluster/#uploading-files","text":"","title":"Uploading files"},{"location":"guides/covid-19/upload-cluster/#basic-usage","text":"With DNAnexus Upload Agent loaded, files can be uploaded by running ua . To get acquainted with the command, you can view the help message for ua . ua --help The simplest usage of ua is providing the DNAnexus project to upload to and a source file. $ ua --do-not-compress --project <project-name-or-id> <src> For example, with a DNAnexus project named flagstat and a file named sample.1.bam , the command would be $ ua --do-not-compress --project flagstat sample.1.bam Why is --do-not-compress always set? Upload Agent uses an unfortunate default where uncompressed files are automatically gzipped. For example, uploading the text file samplesheet.txt results in the file samplesheet.txt.gz on DNAnexus. This is confusing and unexpected, and file management is more straightforward with the option disabled.","title":"Basic usage"},{"location":"guides/covid-19/upload-cluster/#batched-jobs","text":"When working in an interactive session, if the session is closed (e.g., a network disconnection or you quit the terminal), all running commands are killed. To avoid this from happening, a job is submitted in its place, which continues to run even after the session is closed. To submit a job, use the LSF command bsub , where -P is an arbitrary project name for the job submission. When uploading a large batch of files, HPCF requests the /stjudecloud/uploads job group be used to rate limit upload jobs. This can be done using the -g option when submitting a job. $ bsub \\ -P dx-upload \\ -g /stjudecloud/uploads \\ -R \"rusage[mem=2882]\" \\ ua \\ --do-not-compress \\ --project <project-name-or-id> \\ <src> Where does the 2882 MiB resource requirement come from? There is a note in the source code of Upload Agent that gives an estimate of how much memory is used for a transfer: <read-threads> + 2 * (<compress-threads> + <upload-threads>) * <chunk-size> Thus, using the default values (see ua --help ) and adding 20% as a buffer: (2 + 2 * (8 + 8) * 75 MiB) * 1.2 = ~2882 MiB","title":"Batched jobs"},{"location":"guides/covid-19/upload-cluster/#further-examples","text":"The following examples are common usages of Upload Agent. If you use any of these options, please be sure submit them to the HPC job group as shown in batched jobs to ensure the number of uploads is throttled.","title":"Further examples"},{"location":"guides/covid-19/upload-cluster/#upload-multiple-files","text":"ua takes multiple source arguments. $ ua --do-not-compress --project <project-name-or-id> <src...> For example, for two files sample.1.bam and sample.1.bam.bai , the command is $ ua --do-not-compress --project flagstat sample.1.bam sample.1.bam.bai","title":"Upload multiple files"},{"location":"guides/covid-19/upload-cluster/#upload-a-folder-in-dnanexus","text":"By default, all files uploaded via ua get placed at the root of the project, i.e., / . To upload to a particular directory, use the --folder option. $ ua --do-not-compress --project <project-name-or-id> --folder /data sample.1.bam The resulting uploaded file will be located at /data/sample.1.bam . The directory does not have a previously exist in the DNAnexus project and will be created automatically, along with its parents (similar to mkdir -p ).","title":"Upload a folder in DNAnexus"},{"location":"guides/covid-19/upload-cluster/#upload-a-local-directory","text":"When uploading a directory, ua will only use the files at the top level (similar to find data -type f -maxdepth 1 ). For example, take the following directory structure and ua command. $ tree data data \u251c\u2500\u2500 extra \u2502 \u251c\u2500\u2500 sample.2.bam \u2502 \u2514\u2500\u2500 sample.2.bam.bai \u251c\u2500\u2500 sample.1.bam \u2514\u2500\u2500 sample.1.bam.bai 1 directory, 4 files $ ua --do-not-compress --project <project-name-or-id> data The resulting files will be /sample.1.bam and /sample.1.bam.bai . Use the --folder option and name it the same as the local source directory to include the directory on DNAnexus. $ ua --do-not-compress --project <project-name-or-id> --folder /data data The resulting files will be /data/sample.1.bam and /data/sample1.bam.bai . To include the subdirectories, use the --recursive option. $ ua --do-not-compress --project <project-name-or-id> --folder /data --recursive data Thus, using both --folder and --recursive uploads an entire directory: /data/sample.1.bam , /data/sample.1.bam.bai , /data/extras/sample.2.bam , and /data/extras/sample.2.bam.bai .","title":"Upload a local directory"},{"location":"guides/covid-19/upload-local/","text":"Uploading Data from your Local Computer Warning Uploading to DNAnexus from your laptop should not be done over the VPN \u2014 ensure you are disconnected before continuing with this guide! DNAnexus provides a command line utility, dx , to enable users to upload input data, download results, and run workflows in the cloud. Click here to learn about all of the commands included in dx . The dx command line tool is written in Python. It can be installed onto your computer using pip install dxpy . However, many computers ship with multiple versions of Python, and that can lead to many really strange errors (e.g. one version of Python trying to load libraries from a different version of Python). Thus, we highly recommend the use of conda environments to create an isolated Python environment and install dx there. Tip If you are interested in learning more about the features provided by conda, their getting started guide is quite good. This is not strictly necessary for continuing with this guide. Installing via conda \u00b6 To install conda and start creating isolated Python environments, you can visit the conda download documentation and complete the install instructions for your operating system. Be sure to select \"Python 3.X version\" when choosing which version to download. Once you complete installation, you should be able to use the conda command in your terminal. conda can be used to create multiple, independent Python environments. To leverage it to use dx , you'll need to do two things: a one-time creation of a Python 3 environment with dx installed. an environment activation step for every new terminal you open. Note that the environment we create will not be accessible without this explicit activation step. To create an isolated Python 3 environment with dxpy installed, use the following command (you can give your environment any name, here we name it dx-env ). # conda create -n [environment-name] python=3 dxpy -y conda create -n dx-env python = 3 dxpy -y Once you the environment is created, you can run this command each time you open a new terminal to ensure that environment is active. # conda activate [environment-name] conda activate dx-env You should now be able to use the dx command line tool in your terminal. dx --help Authenticating \u00b6 Next, you'll need to configure the dx command line tool with access to your St. Jude Cloud account. Rather than exposing your username and password, best practice is to generate an authentication token that lives for a short period of time instead. You can do so by following this guide on how to generate a DNAnexus authentication token . Replace <auth-token> with your own token in the example below. dx login --token <auth-token> --noprojects Upload and download files \u00b6 With DNAnexus toolkit installed and configured, files can be transferred between St. Jude Cloud and your local computer by running dx upload and dx download . To get acquainted with the command, you can view the relevant help messages. dx upload -h dx download -h To upload a file sample.1.bam to the /test/ folder in the project-alpha cloud project, you could use the following command: dx upload sample.1.bam --destination \"project-alpha:/test/\" To download all files in the /results/ folder in the project-alpha cloud project to the current working directory, you could use the following command: dx download -r \"project-alpha:/results/\" The dx command line utility and its upload / download subcommands have many options you can configure based on your use case. We recommend you view the help messages or reach out to us on the St. Jude Cloud helpdesk Slack channel for more information.","title":"Data upload (local)"},{"location":"guides/covid-19/upload-local/#installing-via-conda","text":"To install conda and start creating isolated Python environments, you can visit the conda download documentation and complete the install instructions for your operating system. Be sure to select \"Python 3.X version\" when choosing which version to download. Once you complete installation, you should be able to use the conda command in your terminal. conda can be used to create multiple, independent Python environments. To leverage it to use dx , you'll need to do two things: a one-time creation of a Python 3 environment with dx installed. an environment activation step for every new terminal you open. Note that the environment we create will not be accessible without this explicit activation step. To create an isolated Python 3 environment with dxpy installed, use the following command (you can give your environment any name, here we name it dx-env ). # conda create -n [environment-name] python=3 dxpy -y conda create -n dx-env python = 3 dxpy -y Once you the environment is created, you can run this command each time you open a new terminal to ensure that environment is active. # conda activate [environment-name] conda activate dx-env You should now be able to use the dx command line tool in your terminal. dx --help","title":"Installing via conda"},{"location":"guides/covid-19/upload-local/#authenticating","text":"Next, you'll need to configure the dx command line tool with access to your St. Jude Cloud account. Rather than exposing your username and password, best practice is to generate an authentication token that lives for a short period of time instead. You can do so by following this guide on how to generate a DNAnexus authentication token . Replace <auth-token> with your own token in the example below. dx login --token <auth-token> --noprojects","title":"Authenticating"},{"location":"guides/covid-19/upload-local/#upload-and-download-files","text":"With DNAnexus toolkit installed and configured, files can be transferred between St. Jude Cloud and your local computer by running dx upload and dx download . To get acquainted with the command, you can view the relevant help messages. dx upload -h dx download -h To upload a file sample.1.bam to the /test/ folder in the project-alpha cloud project, you could use the following command: dx upload sample.1.bam --destination \"project-alpha:/test/\" To download all files in the /results/ folder in the project-alpha cloud project to the current working directory, you could use the following command: dx download -r \"project-alpha:/results/\" The dx command line utility and its upload / download subcommands have many options you can configure based on your use case. We recommend you view the help messages or reach out to us on the St. Jude Cloud helpdesk Slack channel for more information.","title":"Upload and download files"},{"location":"guides/covid-19/visualizing-ngs-data/","text":"Visualizing NGS Data In the future, we will release an alpha version of the GenomePaint visualization tool on St. Jude Cloud. In the meantime, the best way to visualize NGS data on St. Jude Cloud is using IGV. You can use the desktop or web browser versions of IGV. Desktop IGV \u00b6 DNAnexus has an existing guide on viewing files in DNAnexus using the desktop version of IGV. You can view the guide here . Web Browser IGV \u00b6 Within any DNAnexus project, you can view the data in the IGV web browser by doing the following steps. Select \"Visualize\" Select IGV v2.0.0 Select the files you want to view","title":"Visualizing NGS data"},{"location":"guides/covid-19/visualizing-ngs-data/#desktop-igv","text":"DNAnexus has an existing guide on viewing files in DNAnexus using the desktop version of IGV. You can view the guide here .","title":"Desktop IGV"},{"location":"guides/covid-19/visualizing-ngs-data/#web-browser-igv","text":"Within any DNAnexus project, you can view the data in the IGV web browser by doing the following steps. Select \"Visualize\" Select IGV v2.0.0 Select the files you want to view","title":"Web Browser IGV"},{"location":"guides/genomics-platform/","text":"St. Jude Cloud Genomics Platform Genomics Platform is an app that allows you to browse, request, host, and analyze raw next-generation sequencing data. This app is a collaboration between St. Jude Children's Research Hospital , Microsoft Azure , and DNAnexus . We provide high quality next generation Whole Genome (WGS), Whole Exome (WES), and RNA-Seq data to researchers around the world. There is a request process to protect this data even though it is de-identified. One of the benefits of partnering with DNAnexus is that users can not only access our data, but also upload their own. Click here to learn how . Our homepage offers our most popular actions, including visiting our data browser, accessing our analysis workflows, or managing your current access through your dashboard. Browsing our data and workflows is free and does not require an account. If you would like to access the data or workflows, you will have to make a free account so that you can access them through your dashboard. Once approved for access to our data or workflows, you will not be charged for any associated storage fees. See here how to request data and our process for approval.","title":"Getting Started"},{"location":"guides/genomics-platform/accounts-and-billing/","text":"Accounts and Billing St. Jude Cloud is built on top of DNAnexus , a data-analysis and management platform that specializes in the field of bioinformatics. All of our account management is shared between St. Jude Cloud and DNAnexus. In other words, if you have a DNAnexus account, you automatically have a St. Jude Cloud account, and vice versa. Each new user receives a $50 credit upon creation of their St. Jude Cloud account with DNAnexus (see the note in the Billing Setup section). St. Jude Employees The account creation and login process is slightly different if you are an internal user (you work at St. Jude). Internal users please go to the intranet home page and type 'Bioinformatics Self-Service' into the search bar. From there, click on the link that says 'Bioinformatics Self-Service on St. Jude Cloud' to access the internal guide to creating an account. St. Jude Employees \u00b6 Create An Account \u00b6 Go to the St. Jude Employee login page and log in with your current St. Jude credentials. Note If you are unable to log in at this link, and you know you have been on St. Jude Cloud before, you may have already set up a DNAnexus account through the DNANexus log in page using your St. Jude email address. To continue using this account, you will need to log in through the DNAnexus log in page . You can request that your accounts be merged by contacting DNAnexus support . Example Email to DNAnexus Support Hello DNAnexus support, I am a St. Jude employee and I would like to have my account switched over so I can use my St. Jude credentials to log in. Thank you, Billing \u00b6 Go to the Bioinformatics Self Service page on the Intranet for information on how to set up your billing account. If your account is already set up and you would like to access your Billing information: Click on the drop down next to your user name in the far right of the DNAnexus navigation bar, and select 'Profile'. Click on 'Billing Account' from the tabs listed just under the navigation bar. Non-St. Jude Employees \u00b6 Create An Account \u00b6 Go to the St. Jude Cloud log in page on DNAnexus. Click \"Create an Account\". Fill in your information. On the Create New Account page, make sure to select \"Microsoft Azure (westus)\" as the Default Cloud Region. Click 'CREATE ACCOUNT' Billing \u00b6 Click on the drop down next to your user name in the far right of the DNAnexus navigation bar, and select 'Profile'. Click on 'Billing Account' from the tabs listed just under the navigation bar. Click on the green 'ADD BILLING INFO' button to the right of your account name. A window labeled 'ACCOUNT UPGRADE' will pop up. In this window agree to DNAnexus's Terms of Service, agree to DNAnexus's pricing model, review your account information, and finally enter your billing information. Click 'Upgrade Account'. This will send an email to the individual listed as the billing contact requesting that they verify the change. Note On step 4, you must enter the billing contact's name, physical address, email address and phone number. You do not need to enter any credit card information. Once the billing contact has verified the account upgrade request, your account will be credited $50. Please contact us for help if you encounter any problems creating an account.","title":"Accounts and Billing"},{"location":"guides/genomics-platform/accounts-and-billing/#st-jude-employees","text":"","title":"St. Jude Employees"},{"location":"guides/genomics-platform/accounts-and-billing/#create-an-account","text":"Go to the St. Jude Employee login page and log in with your current St. Jude credentials. Note If you are unable to log in at this link, and you know you have been on St. Jude Cloud before, you may have already set up a DNAnexus account through the DNANexus log in page using your St. Jude email address. To continue using this account, you will need to log in through the DNAnexus log in page . You can request that your accounts be merged by contacting DNAnexus support . Example Email to DNAnexus Support Hello DNAnexus support, I am a St. Jude employee and I would like to have my account switched over so I can use my St. Jude credentials to log in. Thank you,","title":"Create An Account"},{"location":"guides/genomics-platform/accounts-and-billing/#billing","text":"Go to the Bioinformatics Self Service page on the Intranet for information on how to set up your billing account. If your account is already set up and you would like to access your Billing information: Click on the drop down next to your user name in the far right of the DNAnexus navigation bar, and select 'Profile'. Click on 'Billing Account' from the tabs listed just under the navigation bar.","title":"Billing"},{"location":"guides/genomics-platform/accounts-and-billing/#non-st-jude-employees","text":"","title":"Non-St. Jude Employees"},{"location":"guides/genomics-platform/accounts-and-billing/#create-an-account_1","text":"Go to the St. Jude Cloud log in page on DNAnexus. Click \"Create an Account\". Fill in your information. On the Create New Account page, make sure to select \"Microsoft Azure (westus)\" as the Default Cloud Region. Click 'CREATE ACCOUNT'","title":"Create An Account"},{"location":"guides/genomics-platform/accounts-and-billing/#billing_1","text":"Click on the drop down next to your user name in the far right of the DNAnexus navigation bar, and select 'Profile'. Click on 'Billing Account' from the tabs listed just under the navigation bar. Click on the green 'ADD BILLING INFO' button to the right of your account name. A window labeled 'ACCOUNT UPGRADE' will pop up. In this window agree to DNAnexus's Terms of Service, agree to DNAnexus's pricing model, review your account information, and finally enter your billing information. Click 'Upgrade Account'. This will send an email to the individual listed as the billing contact requesting that they verify the change. Note On step 4, you must enter the billing contact's name, physical address, email address and phone number. You do not need to enter any credit card information. Once the billing contact has verified the account upgrade request, your account will be credited $50. Please contact us for help if you encounter any problems creating an account.","title":"Billing"},{"location":"guides/genomics-platform/faq/","text":"Frequently Asked Questions Will I be charged for using St. Jude Cloud Genomics Platform? How can I set up billing for my lab? Why do I need to sign the Data Access Agreement (DAA)? How do I submit edits/revisions to the DAA? Can I get a Microsoft Word version of the DAA? Where can I find the latest version of the Data Access Agreement (DAA)? Where do I submit the Data Access Agreement (DAA)? What if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data? What clinical information is available about samples in St. Jude Cloud? Can I gain access to further clinical information than what is currently available? Can I get a copy of IRB consent forms? Can I request FASTQ files on St. Jude Cloud? How can I work with genomics data in the cloud? Why am I getting a connectivity error when connecting to DNAnexus API via SSH? How can I delete my account? Will I be charged for using St. Jude Cloud Genomics Platform? \u00b6 Within St. Jude Cloud Genomics Platform, any St. Jude data you receive through a data request is sponsored , meaning that you do not have to pay a fee to store this data in St. Jude Cloud. You will not incur any costs except in the following situations: Any other files, such as input files uploaded to or results produced by St. Jude Cloud, will incur a monthly fee. See your DNAnexus billing information for the cost per GB. Any analysis workflows, including those provided by St. Jude or your own that you have uploaded and packaged into the cloud, will incur a charge. The charge depends on the underlying compute resources used and the amount of time taken. Documentation for specific workflows we provide should contain guidance on how much the workflow costs. See your DNAnexus billing information for the price of each VM size per hour. At the current time, downloading data is free to end-users. Note, however, that downloading is not without cost to St. Jude. All cloud environments charge an egress fee for anyone downloading data outside of the cloud. At the current time, St. Jude has chosen to sponsor any egress fees associated with downloading data. However, we reserve the right to alter this in the future. How can I set up billing for my lab? \u00b6 Billing setup is different based on whether you are an internal user (you work at St. Jude) or an external user. If you are a St. Jude Employee, please refer to the dedicated intranet page for instructions. If you are not a St. Jude Employee, please refer to our Create an Account guide. Why do I need to sign the Data Access Agreement (DAA)? \u00b6 The data access agreement serves many purposes. Ultimately, the terms included in the data access agreement are in place to protect our patients. We take patient security very seriously, and we require that requesters are committed to protecting that privacy to the fullest extent. How do I submit edits/revisions to the DAA? \u00b6 We do not alter the terms of the data access agreement for any reason except when the terms are found to be directly in conflict with state or national law. In this case, please send a reference to law and a short description to support@stjude.cloud . Otherwise, please understand that we are a small team that simply cannot operational overhead of differing agreements with different parties. Can I get a Microsoft Word version of the DAA? \u00b6 We do not provide any editable format of the DAA, as we do not accept edits or revisions from external parties. Where can I find the latest version of the Data Access Agreement (DAA)? \u00b6 You can download the latest version of the DAA here . Where do I submit the Data Access Agreement (DAA)? \u00b6 You can submit your Data Access Agreement in the drag and drop box on the last step of the data request process . What if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data? \u00b6 This would be a change in terms from the original agreement, you would need to fill out a new DAA (including the Data Download Permission section for any data sets you want to download. What clinical information is available about samples in St. Jude Cloud? \u00b6 You can view the basic clinical and phenotypic information we currently provide here . Can I gain access to further clinical information than what is currently available? \u00b6 We are working towards being able to provide additional clinical annotations such as treatment, outcome, and survival data in the future. Unfortunately, we do not offer it today and we do not have a timeline for when it will be available. Can I get a copy of IRB consent forms? \u00b6 We do not provide individual consent forms or blank consent forms for any samples on St. Jude Cloud. We have chosen to remain consistent with the requirements of the other major genomic data repositories in that (1) there is an internal vetting process by the St. Jude IRB to ensure samples may be shared with the research community, but (2) we do not share the informed consents with data requesters. Can I request FASTQ files on St. Jude Cloud? \u00b6 We do not store FASTQ files in St. Jude Cloud because it would double the storage cost without any benefit. Several tools exist that you can leverage to revert BAM to FASTQ files \u2014 we recommend using Picard SamToFastq to revert BAM files. You can efficiently revert BAMs to FASTQs in the cloud by wrapping the conversion tool of your choice into a Cloud App How can I work with genomics data in the cloud? \u00b6 You can view this guide to learn how create a cloud application. Why am I getting a connectivity error when connecting to DNAnexus API via SSH? \u00b6 If you are trying to run something like $ dx run --ssh <executable> and are getting a connectivity error, it may be that your firewall is too restrictive. Are you able to perform the command from an unrestricted network (like a home network)? If yes, you can resolve this issue by asking your network administrator to whitelist connections to Azure US West. All subnets (Region Name=\"uswest\") are provided here . How can I delete my account? \u00b6 Today, a St. Jude Cloud Genomics Platform account is simply a DNAnexus account. Thus, of you'd like to delete your account, you'll need to email DNAnexus asking for it to be removed. You can do so by contacting DNAnexus support at support@dnanexus.com with the following email. Subject: St. Jude Cloud account deletion Hi DNAnexus, Would you please assist me in deleting my St. Jude Cloud account? My username is _____. Thank you!","title":"Frequently Asked Questions"},{"location":"guides/genomics-platform/faq/#will-i-be-charged-for-using-st-jude-cloud-genomics-platform","text":"Within St. Jude Cloud Genomics Platform, any St. Jude data you receive through a data request is sponsored , meaning that you do not have to pay a fee to store this data in St. Jude Cloud. You will not incur any costs except in the following situations: Any other files, such as input files uploaded to or results produced by St. Jude Cloud, will incur a monthly fee. See your DNAnexus billing information for the cost per GB. Any analysis workflows, including those provided by St. Jude or your own that you have uploaded and packaged into the cloud, will incur a charge. The charge depends on the underlying compute resources used and the amount of time taken. Documentation for specific workflows we provide should contain guidance on how much the workflow costs. See your DNAnexus billing information for the price of each VM size per hour. At the current time, downloading data is free to end-users. Note, however, that downloading is not without cost to St. Jude. All cloud environments charge an egress fee for anyone downloading data outside of the cloud. At the current time, St. Jude has chosen to sponsor any egress fees associated with downloading data. However, we reserve the right to alter this in the future.","title":"Will I be charged for using St. Jude Cloud Genomics Platform?"},{"location":"guides/genomics-platform/faq/#how-can-i-set-up-billing-for-my-lab","text":"Billing setup is different based on whether you are an internal user (you work at St. Jude) or an external user. If you are a St. Jude Employee, please refer to the dedicated intranet page for instructions. If you are not a St. Jude Employee, please refer to our Create an Account guide.","title":"How can I set up billing for my lab?"},{"location":"guides/genomics-platform/faq/#why-do-i-need-to-sign-the-data-access-agreement-daa","text":"The data access agreement serves many purposes. Ultimately, the terms included in the data access agreement are in place to protect our patients. We take patient security very seriously, and we require that requesters are committed to protecting that privacy to the fullest extent.","title":"Why do I need to sign the Data Access Agreement (DAA)?"},{"location":"guides/genomics-platform/faq/#how-do-i-submit-editsrevisions-to-the-daa","text":"We do not alter the terms of the data access agreement for any reason except when the terms are found to be directly in conflict with state or national law. In this case, please send a reference to law and a short description to support@stjude.cloud . Otherwise, please understand that we are a small team that simply cannot operational overhead of differing agreements with different parties.","title":"How do I submit edits/revisions to the DAA?"},{"location":"guides/genomics-platform/faq/#can-i-get-a-microsoft-word-version-of-the-daa","text":"We do not provide any editable format of the DAA, as we do not accept edits or revisions from external parties.","title":"Can I get a Microsoft Word version of the DAA?"},{"location":"guides/genomics-platform/faq/#where-can-i-find-the-latest-version-of-the-data-access-agreement-daa","text":"You can download the latest version of the DAA here .","title":"Where can I find the latest version of the Data Access Agreement (DAA)?"},{"location":"guides/genomics-platform/faq/#where-do-i-submit-the-data-access-agreement-daa","text":"You can submit your Data Access Agreement in the drag and drop box on the last step of the data request process .","title":"Where do I submit the Data Access Agreement (DAA)?"},{"location":"guides/genomics-platform/faq/#what-if-i-did-not-fill-out-the-data-download-permission-section-of-the-original-daa-but-now-i-want-to-download-data","text":"This would be a change in terms from the original agreement, you would need to fill out a new DAA (including the Data Download Permission section for any data sets you want to download.","title":"What if I did not fill out the Data Download Permission section of the original DAA, but now I want to download data?"},{"location":"guides/genomics-platform/faq/#what-clinical-information-is-available-about-samples-in-st-jude-cloud","text":"You can view the basic clinical and phenotypic information we currently provide here .","title":"What clinical information is available about samples in St. Jude Cloud?"},{"location":"guides/genomics-platform/faq/#can-i-gain-access-to-further-clinical-information-than-what-is-currently-available","text":"We are working towards being able to provide additional clinical annotations such as treatment, outcome, and survival data in the future. Unfortunately, we do not offer it today and we do not have a timeline for when it will be available.","title":"Can I gain access to further clinical information than what is currently available?"},{"location":"guides/genomics-platform/faq/#can-i-get-a-copy-of-irb-consent-forms","text":"We do not provide individual consent forms or blank consent forms for any samples on St. Jude Cloud. We have chosen to remain consistent with the requirements of the other major genomic data repositories in that (1) there is an internal vetting process by the St. Jude IRB to ensure samples may be shared with the research community, but (2) we do not share the informed consents with data requesters.","title":"Can I get a copy of IRB consent forms?"},{"location":"guides/genomics-platform/faq/#can-i-request-fastq-files-on-st-jude-cloud","text":"We do not store FASTQ files in St. Jude Cloud because it would double the storage cost without any benefit. Several tools exist that you can leverage to revert BAM to FASTQ files \u2014 we recommend using Picard SamToFastq to revert BAM files. You can efficiently revert BAMs to FASTQs in the cloud by wrapping the conversion tool of your choice into a Cloud App","title":"Can I request FASTQ files on St. Jude Cloud?"},{"location":"guides/genomics-platform/faq/#how-can-i-work-with-genomics-data-in-the-cloud","text":"You can view this guide to learn how create a cloud application.","title":"How can I work with genomics data in the cloud?"},{"location":"guides/genomics-platform/faq/#why-am-i-getting-a-connectivity-error-when-connecting-to-dnanexus-api-via-ssh","text":"If you are trying to run something like $ dx run --ssh <executable> and are getting a connectivity error, it may be that your firewall is too restrictive. Are you able to perform the command from an unrestricted network (like a home network)? If yes, you can resolve this issue by asking your network administrator to whitelist connections to Azure US West. All subnets (Region Name=\"uswest\") are provided here .","title":"Why am I getting a connectivity error when connecting to DNAnexus API via SSH?"},{"location":"guides/genomics-platform/faq/#how-can-i-delete-my-account","text":"Today, a St. Jude Cloud Genomics Platform account is simply a DNAnexus account. Thus, of you'd like to delete your account, you'll need to email DNAnexus asking for it to be removed. You can do so by contacting DNAnexus support at support@dnanexus.com with the following email. Subject: St. Jude Cloud account deletion Hi DNAnexus, Would you please assist me in deleting my St. Jude Cloud account? My username is _____. Thank you!","title":"How can I delete my account?"},{"location":"guides/genomics-platform/analyzing-data/chipseq/","text":"Authors Xing Tang, Yong Cheng Publication N/A (not published) Technical Support Contact Us Overview \u00b6 The ChIP-Seq Peak Calling workflow follows ENCODE best practices to call broad or narrow peaks on Illumina-generated ChIP-Seq data. Here, a Gzipped FastQ file from an Immunoprecipitation (IP) experiment is considered the \"case sample file\" and a Gzipped FastQ file from a control experiment is considered the \"control sample file\". The pipeline can run on matched case/control samples (recommended for better results) or just a case sample. Inputs \u00b6 Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz Outputs \u00b6 Name Format Description BED file .bed Peak calls Binary file .bb Binary format for BED file BigWig file .bw Shows read coverage Metrics file .txt Shows mapping and duplication rate Cross correlation plot .pdf Quality plot showing if the forward and reverse reads tend to be centered around binding sites. Workflow Steps \u00b6 The reads of the FastQ file(s) are aligned to the specified reference genome. The aligned reads are then post-processed based on best-practice QC techniques (removing multiple mapped reads, removing duplicated reads, etc). Peaks are called by SICER (broad peak analysis) or MACS2 (narrow peak analysis). Qualified peaks will be output as BED (.bed) and big BED (.bb) files. The coverage information will be output as a bigWig (.bw) file. A cross correlation plot and general metrics file are generated to help check the quality of experiment. Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the ChIP-Seq Peak Calling workflow page here . Uploading Input Files \u00b6 The ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an IP experiment as input . Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. For the ChIP-Seq workflow, you will see special preset options in the \"Launch Tool\" dropdown. These are explained below. You'll need to decide (1) whether you'd like to run broad OR narrow peak calling and (2) whether you have a case sample and a control sample (preferred) OR just a case sample. This will determine which preset you should click in this dropdown. Note that if you are not doing a case/control run, when you get to the hooking up inputs step you only need to hook up the case sample. Broad vs. narrow peak calling \u00b6 Choosing between broad and narrow peak calling depends on the experiment design. The following are good rules of thumb for choosing between the two configurations. If you are not sure which configuration to use, please consult with an expert at your institution or contact us . Narrow Peak Calling If your target protein is a transcription factor, you should probably choose narrow peak calling. You can also try the narrow peak calling workflows for the following histone marks: H3K4me3 H3K4me2 H3K9-14ac H3K27ac H2A.Z Broad Peak Calling You should try the broad peak calling workflows for the following histone marks: H3K36me3 H3K79me2 H3K27me3 H3K9me3 H3K9me1 Special Cases In some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad shape, you might need to look into each peak region and consult experts. Warning If your fragment size is less than 50 base pairs, please refer to the frequently asked questions . Selecting parameters \u00b6 The following are the parameters that can be set, a short description of each parameter, and an example value. How to customize parameters is covered in the general workflow guide . If you have questions, please contact us . Parameter Name Description Example Output prefix ( required ) A name used a prefix for all outputs in the run SAMPLE1 Reference genome ( required ) Supported reference genome from one of hg19, GRCh38, mm9, mm10, dm3 GRCh38 Output bigWig Whether or not to include a bigwig file in the output True Remove blacklist peaks Whether or not to remove known problem areas True Fragment length Hardcoded fragment length of your reads. 'NA' for auto-detect. NA Caution Please be aware of the following stumbling points when setting parameters: Do not use spaces anywhere in your input file names, your output prefix, or any of the other parameters. This is generally bad practice and doesn't play well with the pipeline (consider using \"_\" instead). Do not change the output directory when you run the pipeline. At the top of parameter input page, there is a text box that allows you to change the output folder. Please ignore that setting . You only need to specify an output prefix as described above. All of the results will be put under /Results/[OUTPUT_PREFIX] . Analysis of Results \u00b6 Today, the ChIP-Seq pipeline does not produce an interactive visualization. We are working on adding this! In the meantime, you can view the cross-correlation plot(s) as outlined in the sections below. Refer to the general workflow guide to learn how to access raw results files. ChIP-Seq results will be in the Results folder. Select the output folder name you specified in the parameters part of this workflow run. Interpreting results \u00b6 For the ChIP-Seq pipeline, every pipeline run outputs a README.doc file which contains the latest information on which results are included. You can refer to that file for the most up to date information on raw outputs. Frequently asked questions \u00b6 If you have any questions not covered here, feel free to contact us . Q: Should I choose narrow peak calling pipeline or broad peak calling pipeline? A. We built two workflows: one for narrow peak calling and another broad peak calling. If your target protein is a transcription factor, please use narrow peak calling workflow. For histone marks H3K4me3, H3K4me2, H3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling workflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and H3K9me1, you could try broad peak calling workflow. In some scenario, H3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad shape, you might need to look into each peak region and consult experts. Q. What to do if your fragment size is less than 50 base pairs? A. We estimate fragment size from the data based on the cross correlation plot. Usually the fragment size is above 50bp. If the estimated fragment size lower than 50bp, the workflow will stop at the peak calling stage (MACS2/SICER) after BWA mapping finishes. You can rerun the analysis with a specified fragment length. Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"ChIP-Seq Peak Calling"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#overview","text":"The ChIP-Seq Peak Calling workflow follows ENCODE best practices to call broad or narrow peaks on Illumina-generated ChIP-Seq data. Here, a Gzipped FastQ file from an Immunoprecipitation (IP) experiment is considered the \"case sample file\" and a Gzipped FastQ file from a control experiment is considered the \"control sample file\". The pipeline can run on matched case/control samples (recommended for better results) or just a case sample.","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#inputs","text":"Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#outputs","text":"Name Format Description BED file .bed Peak calls Binary file .bb Binary format for BED file BigWig file .bw Shows read coverage Metrics file .txt Shows mapping and duplication rate Cross correlation plot .pdf Quality plot showing if the forward and reverse reads tend to be centered around binding sites.","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#workflow-steps","text":"The reads of the FastQ file(s) are aligned to the specified reference genome. The aligned reads are then post-processed based on best-practice QC techniques (removing multiple mapped reads, removing duplicated reads, etc). Peaks are called by SICER (broad peak analysis) or MACS2 (narrow peak analysis). Qualified peaks will be output as BED (.bed) and big BED (.bb) files. The coverage information will be output as a bigWig (.bw) file. A cross correlation plot and general metrics file are generated to help check the quality of experiment.","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the ChIP-Seq Peak Calling workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#uploading-input-files","text":"The ChIP-Seq Peak Caller takes Gzipped FastQ files generated from an IP experiment as input . Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input Files"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. For the ChIP-Seq workflow, you will see special preset options in the \"Launch Tool\" dropdown. These are explained below. You'll need to decide (1) whether you'd like to run broad OR narrow peak calling and (2) whether you have a case sample and a control sample (preferred) OR just a case sample. This will determine which preset you should click in this dropdown. Note that if you are not doing a case/control run, when you get to the hooking up inputs step you only need to hook up the case sample.","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#broad-vs-narrow-peak-calling","text":"Choosing between broad and narrow peak calling depends on the experiment design. The following are good rules of thumb for choosing between the two configurations. If you are not sure which configuration to use, please consult with an expert at your institution or contact us . Narrow Peak Calling If your target protein is a transcription factor, you should probably choose narrow peak calling. You can also try the narrow peak calling workflows for the following histone marks: H3K4me3 H3K4me2 H3K9-14ac H3K27ac H2A.Z Broad Peak Calling You should try the broad peak calling workflows for the following histone marks: H3K36me3 H3K79me2 H3K27me3 H3K9me3 H3K9me1 Special Cases In some scenarios, H3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad shape, you might need to look into each peak region and consult experts. Warning If your fragment size is less than 50 base pairs, please refer to the frequently asked questions .","title":"Broad vs. narrow peak calling"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#selecting-parameters","text":"The following are the parameters that can be set, a short description of each parameter, and an example value. How to customize parameters is covered in the general workflow guide . If you have questions, please contact us . Parameter Name Description Example Output prefix ( required ) A name used a prefix for all outputs in the run SAMPLE1 Reference genome ( required ) Supported reference genome from one of hg19, GRCh38, mm9, mm10, dm3 GRCh38 Output bigWig Whether or not to include a bigwig file in the output True Remove blacklist peaks Whether or not to remove known problem areas True Fragment length Hardcoded fragment length of your reads. 'NA' for auto-detect. NA Caution Please be aware of the following stumbling points when setting parameters: Do not use spaces anywhere in your input file names, your output prefix, or any of the other parameters. This is generally bad practice and doesn't play well with the pipeline (consider using \"_\" instead). Do not change the output directory when you run the pipeline. At the top of parameter input page, there is a text box that allows you to change the output folder. Please ignore that setting . You only need to specify an output prefix as described above. All of the results will be put under /Results/[OUTPUT_PREFIX] .","title":"Selecting parameters"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#analysis-of-results","text":"Today, the ChIP-Seq pipeline does not produce an interactive visualization. We are working on adding this! In the meantime, you can view the cross-correlation plot(s) as outlined in the sections below. Refer to the general workflow guide to learn how to access raw results files. ChIP-Seq results will be in the Results folder. Select the output folder name you specified in the parameters part of this workflow run.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#interpreting-results","text":"For the ChIP-Seq pipeline, every pipeline run outputs a README.doc file which contains the latest information on which results are included. You can refer to that file for the most up to date information on raw outputs.","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#frequently-asked-questions","text":"If you have any questions not covered here, feel free to contact us . Q: Should I choose narrow peak calling pipeline or broad peak calling pipeline? A. We built two workflows: one for narrow peak calling and another broad peak calling. If your target protein is a transcription factor, please use narrow peak calling workflow. For histone marks H3K4me3, H3K4me2, H3K9-14ac, H3K27ac and H2A.Z, you could try narrow peak calling workflow. For histone marks H3K36me3, H3K79me2, H3K27me3, H3K9me3 and H3K9me1, you could try broad peak calling workflow. In some scenario, H3K4me1, H3K9me2 and H3K9me3 might behave between narrow and broad shape, you might need to look into each peak region and consult experts. Q. What to do if your fragment size is less than 50 base pairs? A. We estimate fragment size from the data based on the cross correlation plot. Usually the fragment size is above 50bp. If the estimated fragment size lower than 50bp, the workflow will stop at the peak calling stage (MACS2/SICER) after BWA mapping finishes. You can rerun the analysis with a specified fragment length.","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/chipseq/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/cis-x/","text":"Authors Yu Liu, Chunliang Li, Shuhong Shen Publication N/A (not published) Technical Support Contact Us Overview \u00b6 Activating regular variants usually cause the cis-activation of target genes. To find cis-activated genes, allelic specific/imbalance expressions (ASE) and outlier high expression (OHE) signals are used. Variants in the same topologically associated domains with the candidates can then be searched, including structural variants (SV), copy number aberrations (CNA), and single nucleotide variations (SNV) and insertion/deletions (indel). A transcription factor binding analysis is also done, using motifs from HOCOMOCO v10 models. cis-X currently only works with hg19 (GRCh37). Inputs \u00b6 Name Type Description Example Sample ID String The ID of the input sample SJALL018373_D1 Disease subtype String The disease name under analysis. Must be either TALL or AML. TALL Single nucleotide variants File Tab-delimited file containing raw sequence variants *.txt CNV/LOH regions File Tab-delimited file containing any aneuploidy region existing in the tumor genome under analysis *.txt RNA-seq BAM File BAM file aligned to hg19 (GRCh37) *.bam RNA-seq BAM index File BAM index for the given BAM *.bam.bai Gene expression table File Tab-delimited file containing gene level expressions for the tumor under analysis in FPKM *.txt Somatic SNV/indels File Tab-delimited file containing somatic SNV/indels in the tumor genome *.txt Somatic SVs File Tab-delimited file containing somatic acquired structural variants in the tumor genome *.txt Somatic CNVs File Tab-delimited file containing copy number aberrations in the tumor genome *.txt CNV/LOH action String The behavior when handling markers in CNV/LOH regions. Can be either keep or drop . drop Minimum coverage for WGS Integer The minimum coverage in WGS to be included in the analysis 10 Minimum coverage for RNA-seq Integer The minimum coverage in RNA-seq to be included in the analysis 5 Candidate FPKM threshold Float The FPKM threshold for the nomination of a cis-activated candidate 0.1 Input file configuration \u00b6 cis-X requires six tab-delimited input files to be prepared in advance. These files can be uploaded via the data transfer application or command line . Note Even though CNV/LOH regions, somatic SNV/indels, somatic SVs, and somatic CNVs can be \"empty\", using such inputs will produce results with a much higher false positive rate. Single nucleotide variants A list of single nucleotide markers is a tab-delimited file with the following columns: Chr : chromosome name for the marker Pos : genomic start location for the marker Chr_Allele : reference allele Alternative_Allele : alternative allele reference_tumor_count : reference allele count in the tumor genome alternative_tumor_count : alternative allele count in the tumor genome reference_normal_count : reference allele count in the matched normal genome alternative_normal_count : alternative count in the matched normal genome This file can be generated with Bambino. Example Chr Pos Chr_Allele Alternative_Allele reference_tumor_count alternative_tumor_count reference_normal_count alternative_normal_count chr11 61396 TT 0 3 0 10 chr11 72981 T 1 3 2 3 CNV/LOH regions The CNV/LOH regions are all the genomic regions carrying copy number variations (CNV) or loss of heterozygosity (LOH), which will be filtered out during analysis. This is a tab-delimited file in the bed format. It must have at least the following three columns: chrom : chromosome name loc.start : genomic start location loc.end : genomic end location If no CNV/LOH are in the genome under analysis, a file with no rows (but including headers) can be provided. This file can be generated with CONSERTING. Example chrom loc.start loc.end Sample seg.mean LogRatio source chr9 10712 37855747 SJALL018373_D1 0.471181417 LOH chr9 20276901 20703900 SJALL018373_D1 -0.978 -5.696 CNV Gene expression table The gene expression table is a tab-delimited file containing gene level expressions for the tumor under analysis. The expressions are in FPKM (fragments per kilobase of transcript per million mapped reads). GeneID : gene Ensembl ID GeneName : gene symbol Type : transcript type Status : transcript status (must be KNOWN , NOVEL , or PUTATIVE ) Chr : chromosome name Start genomic start location End : genomic end location [SampleID...]: FPKM for the given sample This file can be generated with the output of HTseq-count preprocessed through mergeData_geneName.pl (available with the distribution of cis-X). The data must be able to match values in the given gene specific reference expression matrices generated from a larger cohort. Example GeneID GeneName Type Status Chr Start End SJALL018373_D1 ENSG00000261122.2 5S_rRNA lincRNA NOVEL chr16 34977639 34990886 0.0000 ENSG00000249352.3 7SK lincRNA NOVEL chr5 68266266 68325992 4.5937 Somatic SNV/indels This is a tab-delimited file containing somatic sequence mutations present in the genome under analysis. It includes both single nucleotide variants (SNV) and small insertion/deletions (indel). The file must have the following columns: chr : chromosome name pos : genomic start location ref : reference nucleotide mutant : mutant nucleotide type : mutation type (must be either snv or indel ) Note that the coordinate used for an indel is after the inserted sequence. If no SNV/indels are in the sample under analysis, a file with no rows (but including headers) can be provided. This file can can be created with Bambino and then preprocessed using the steps taken in \" The genetic basis of early T-cell precursor acute lymphoblastic leukaemia \". Example chr pos ref mut type chr1 24782720 G A snv chr11 82896176 T C snv Somatic SVs This is a tab-delimited file containing somatic-acquired structural variants (SV) in the cancer genome. The file must have the following columns: chrA : chromosome name of the left breakpoint posA : genomic location of the left breakpoint ortA : strand orientation of the left breakpoint chrB : chromosome name of the right breakpoint posB : genomic location of the right breakpoint ortB : strand orientation of the right breakpoint Strand orientations are denoted with a + for a sense or coding strand and - for a antisense or non-coding strand. If no somatic SVs are in the sample under analysis, a file with no rows (but including headers) can be provided. This file can be generated by CREST. Example chrA posA ortA chrB posB ortB type chr11 33913169 + chr7 142494049 - CTX chr11 64219334 + chr2 205042527 - CTX Somatic CNVs This is a tab-delimited file containing the genomic regions with somatic-acquired copy number aberrations (CNA) in the cancer genome. chr : chromosome name start : genomic start location end : genomic end location logR : log2 ratio If no somatic CNVs are in the sample under analysis, a file with no rows (but including headers) can be provided. This file can be generating by CONSERTING. Example chr start end logR chr9 20276901 20703900 -5.696 Outputs \u00b6 Name Description cis-activated candidates cis-activated candidates in the tumor genome under analysis SV candidates Structural variant (SV) candidates predicted as the causal for the cis-activated genes in the regulatory territory CNA candidates Copy number aberrations (CNA) predicted as the causal for the cis-activated genes in the regulatory territory SNV/indel candidates SNV/indel candidates predicted as functional and predicted transcription factors OHE results Raw outlier high expression (OHE) results Gene level ASE results Raw gene level allelic specific expression (ASE) results Single marker ASE results Raw single marker allelic specific expression (ASE) results Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the Cis-X workflow page here Uploading Input Files \u00b6 cis-X requires a total of eight files to be uploaded, as input . Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 cis-activated candidates \u00b6 The main result file contains the cis-activated candidates in the tumor genome under analysis. gene : gene accession number ( RefSeq ID) gsym : gene symbol chrom : chromosome name strand : strand orientation start : genomic start location end : genomic end location cdsStartStat : coding sequence (CDS) start status cdsEndStat : coding sequence (CDS) end status markers : number of heterozygous markers in this gene ase_markers : number of heterozygous markers showing allelic specific expressions (ASE) average_ai_all : average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers average_ai_ase : average BAF difference between RNA and DNA for ASE markers pval_all_markers : p-value for each marker in the ASE test pval_ase_markers : p-value for ASE markers in the ASE test ai_all_markers : BAF difference between RNA and DNA for all heterozygrous markers ai_ase_markers : BAF difference between RNA and DNA for ASE markers comb.pval : combined p-value for the ASE test mean.delta : average BAF difference between RNA and DNA for all markers rawp : raw p-value for the ASE test Bonferroni : adjusted p-value for the ASE test (single-step Bonferroni) ABH : adjusted p-value for the ASE test (Benjamini-Hochberg) FPKM : FPKM value loo.source : which reference expression matrix was used in the outlier high expression (OHE) test loo.cohort.size : number of cases in the reference expression matrix for this gene loo.pval : p-value of the OHE test loo.rank : rank of the case under analysis among the reference cases imprinting.status : imprinting status of the gene candidate.group : status of the gene, combining both ASE and outlier tests Strand orientations are denoted with a + for a sense or coding strand and - for a antisense or non-coding strand. Coding sequence status is typically one of \"none\" (not specified), \"unk\" (unknown), \"incmpl\" (incomplete), or \"cmpl\" (complete). Example gene gsym chrom strand start end cdsStartStat cdsEndStat markers ase_markers average_ai_all average_ai_ase pval_all_markers pval_ase_markers ai_all_markers ai_ase_markers comb.pval mean.delta rawp Bonferroni ABH FPKM loo.source loo.cohort.size loo.pval loo.rank imprinting.status candidate.group NM_145804 ABTB2 chr11 - 34172533 34379555 cmpl cmpl 5 5 0.5 0.500 0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625 0.001953125,0.001953125,0.001953125,6.10351562500001e-05,0.000244140625 0.5,0.5,0.5,0.5,0.5 0.5,0.5,0.5,0.5,0.5 0.000644290972057077 0.5 0.000644290972057077 0.632049443587993 0.0110866672927557 7.6776 bi_cohort 40 0.0367241086505276 1 ase_outlier NM_003189 TAL1 chr1 - 47681961 47698007 cmpl cmpl 2 2 0.482 0.482 6.66361745922277e-28,3.30872245021211e-24 6.66361745922277e-28,3.30872245021211e-24 0.464912280701754,0.5 0.464912280701754,0.5 4.69553625126628e-26 0.482456140350877 4.69553625126628e-26 4.60632106249222e-23 6.11761294450693e-24 8.8168 white_list 167 0.0139385771987089 1 ase_outlier SV candidates \u00b6 Structural variant (SV) candidates include candidates predicted as the causal for the cis-activated genes in the regulatory territory. left.candidate.inTAD : cis-activated candidate near the left breakpoint right.candidate.inTAD : cis-activated candidate near the right breakpoint chrA : chromosome name of the left breakpoint posA : genomic location of the left breakpoint ortA : strand orientation of the left breakpoint chrB : chromosome name of the right breakpoint posB : genomic location of the right breakpoint ortB : strand orientation of the right breakpoint type : type of translocation Example left.candidate.inTAD right.candidate.inTAD chrA posA ortA chrB posB ortB type LMO2 chr11 33913169 + chr7 142494049 - CTX CNA candidates \u00b6 Copy number aberration (CNA) candidates include candidates predicted as the causal for the cis-activated genes in the regulatory territory. candidate.inTAD : cis-activated candidate by the CNA chr : chromosome name start : genomic start position end : genomic end location logR : log ratio of the CNA SNV/indel candidates \u00b6 SNV/indel candidates include predicted candidates as functional and predicted transcription factors. The mutations are also annotated for known regulatory elements reported by the NIH Roadmap Epigenomics Project by collecting 111 cell lines. chrom : chromosome name pos : genomic start position ref : reference allele genotype mut : mutant allele genotype type : mutation type (either snv or indel ) target : cis-activated candidate dist : distance between the mutation and transcription start sites of the target gene tf : transcription factors predicted to have the binding motif introduced by the mutation EpiRoadmap_enhancer : enhancer regions that overlap with the mutation (from the NIH Roadmap Epigenomics Project ) EpiRoadmap_promoter : promoter regions that overlap with the mutation (from the NIH Roadmap Epigenomics Project ) EpiRoadmap_dyadic : dyadic regions that overlap with the mutation (from the NIH Roadmap Epigenomics Project ) Example chrom pos ref mut type target dist tf EpiRoadmap_enhancer EpiRoadmap_promoter EpiRoadmap_dyadic chr1 47696311 C T snv TAL1 1696 BCL11A,CEBPG,PBX2,YY1,ZBTB4 Brain,Digestive,ES-deriv,ESC,HSC & B-cell,Heart,Muscle,Other,Sm. Muscle,iPSC OHE results \u00b6 OHE results are the raw results for the outlier expression test. Gene : gene symbol fpkm.raw : FPKM value size.bi : number of cases in the bi-allelic reference cohort p.bi : p-value in the outlier test using the bi-allelic reference cohort rank.bi : rank of the expression level in the case under analysis compared to the bi-allelic reference cohort size.cohort : number of cases in the entire reference cohort p.cohort : p-value in the outlier test using the entire reference cohort rank.cohort : rank of the expression level in the case under analysis compared to the entire reference cohort size.white : number of cases in the whitelist reference cohort p.white : p-value in the outlier test using the whitelist reference cohort rank.white : rank of the expression level in the case under analysis compared to the whitelist reference cohort Example Gene fpkm.raw size.bi p.bi rank.bi size.cohort p.cohort rank.cohort size.white p.white rank.white 7SK 4.5937 na na na 264 0.716284011918374 162 na na na A1BG 0.2312 24 0.900132642257996 21 264 0.84055666600945 222 na na na Gene level ASE results \u00b6 Gene level ASE results are the raw results from the gene level ASE test. gene : gene accession number ( RefSeq ID) gsym : gene symbol chrom : chromosome name strand : strand orientation start : genomic start location end : genomic end location cdsStartStat : coding sequence (CDS) start status cdsEndStat : coding sequence (CDS) end status markers : number of heterozygous markers in this gene ase_markers : number of heterozygous markers showing allelic specific expressions (ASE) average_ai_all : average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers average_ai_ase : average BAF difference between RNA and DNA for ASE markers pval_all_markers : p-value for each marker in the ASE test pval_ase_markers : p-value for ASE markers in the ASE test ai_all_markers : BAF difference between RNA and DNA for all heterozygrous markers ai_ase_markers : BAF difference between RNA and DNA for ASE markers comb.pval : combined p-value for the ASE test mean.delta : average BAF difference between RNA and DNA for all markers rawp : raw p-value for the ASE test Bonferroni : adjusted p-value for the ASE test (single-step Bonferroni) ABH : adjusted p-value for the ASE test (Benjamini-Hochberg) Example gene gsym chrom strand start end cdsStartStat cdsEndStat markers ase_markers average_ai_all average_ai_ase pval_all_markers pval_ase_markers ai_all_markers ai_ase_markers comb.pval mean.delta rawp Bonferroni ABH NM_024684 AAMDC chr11 + 77532207 77583398 cmpl cmpl 2 0 0.079 na 0.924775093657227,0.0331439677875056 na 0.00892857142857145,0.149122807017544 na 0.175073458624837 0.0790256892230577 0.175073458624837 1 0.480780882445856 NM_015423 AASDHPPT chr11 + 105948291 105969419 cmpl cmpl 2 0 0.023 na 0.749258624760841,1 na 0.0384615384615384,0.00769230769230766 na 0.86559726476049 0.023076923076923 0.86559726476049 1 0.873257417545981 Single marker ASE results \u00b6 Single marker ASE results are the raw results from the single marker ASE test. chrom : chromosome name pos : genomic start position ref : reference allele genotype mut : non-reference allele genotype cvg_wgs : coverage of the marker from the whole genome sequence (WGS) mut_freq_wgs : non-reference allele fraction in the WGS cvg_rna : coverage of the marker from the RNA-seq mut_freq_rna : non-reference allele fraction in the RNA-seq ref.1 : read count of the reference allele in the RNA-seq var : read count of the non-reference allele in the RNA-seq pvalue : p-value from the binomial test delta.abs : absolute difference of the non-reference allele fraction between the WGS and RNA-seq Example chrom pos ref mut cvg_wgs mut_freq_wgs cvg_rna mut_freq_rna ref.1 var pvalue delta.abs chr11 204147 G A 36 0.472 85 0.553 38 47 0.385669420119278 0.0529411764705883 chr11 205198 C A 23 0.522 83 0.313 57 26 0.000877551780002863 0.186746987951807 Frequently asked questions \u00b6 None yet! If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Cis x"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#overview","text":"Activating regular variants usually cause the cis-activation of target genes. To find cis-activated genes, allelic specific/imbalance expressions (ASE) and outlier high expression (OHE) signals are used. Variants in the same topologically associated domains with the candidates can then be searched, including structural variants (SV), copy number aberrations (CNA), and single nucleotide variations (SNV) and insertion/deletions (indel). A transcription factor binding analysis is also done, using motifs from HOCOMOCO v10 models. cis-X currently only works with hg19 (GRCh37).","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#inputs","text":"Name Type Description Example Sample ID String The ID of the input sample SJALL018373_D1 Disease subtype String The disease name under analysis. Must be either TALL or AML. TALL Single nucleotide variants File Tab-delimited file containing raw sequence variants *.txt CNV/LOH regions File Tab-delimited file containing any aneuploidy region existing in the tumor genome under analysis *.txt RNA-seq BAM File BAM file aligned to hg19 (GRCh37) *.bam RNA-seq BAM index File BAM index for the given BAM *.bam.bai Gene expression table File Tab-delimited file containing gene level expressions for the tumor under analysis in FPKM *.txt Somatic SNV/indels File Tab-delimited file containing somatic SNV/indels in the tumor genome *.txt Somatic SVs File Tab-delimited file containing somatic acquired structural variants in the tumor genome *.txt Somatic CNVs File Tab-delimited file containing copy number aberrations in the tumor genome *.txt CNV/LOH action String The behavior when handling markers in CNV/LOH regions. Can be either keep or drop . drop Minimum coverage for WGS Integer The minimum coverage in WGS to be included in the analysis 10 Minimum coverage for RNA-seq Integer The minimum coverage in RNA-seq to be included in the analysis 5 Candidate FPKM threshold Float The FPKM threshold for the nomination of a cis-activated candidate 0.1","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#input-file-configuration","text":"cis-X requires six tab-delimited input files to be prepared in advance. These files can be uploaded via the data transfer application or command line . Note Even though CNV/LOH regions, somatic SNV/indels, somatic SVs, and somatic CNVs can be \"empty\", using such inputs will produce results with a much higher false positive rate.","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#outputs","text":"Name Description cis-activated candidates cis-activated candidates in the tumor genome under analysis SV candidates Structural variant (SV) candidates predicted as the causal for the cis-activated genes in the regulatory territory CNA candidates Copy number aberrations (CNA) predicted as the causal for the cis-activated genes in the regulatory territory SNV/indel candidates SNV/indel candidates predicted as functional and predicted transcription factors OHE results Raw outlier high expression (OHE) results Gene level ASE results Raw gene level allelic specific expression (ASE) results Single marker ASE results Raw single marker allelic specific expression (ASE) results","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the Cis-X workflow page here","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#uploading-input-files","text":"cis-X requires a total of eight files to be uploaded, as input . Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input Files"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress.","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#interpreting-results","text":"","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#cis-activated-candidates","text":"The main result file contains the cis-activated candidates in the tumor genome under analysis. gene : gene accession number ( RefSeq ID) gsym : gene symbol chrom : chromosome name strand : strand orientation start : genomic start location end : genomic end location cdsStartStat : coding sequence (CDS) start status cdsEndStat : coding sequence (CDS) end status markers : number of heterozygous markers in this gene ase_markers : number of heterozygous markers showing allelic specific expressions (ASE) average_ai_all : average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers average_ai_ase : average BAF difference between RNA and DNA for ASE markers pval_all_markers : p-value for each marker in the ASE test pval_ase_markers : p-value for ASE markers in the ASE test ai_all_markers : BAF difference between RNA and DNA for all heterozygrous markers ai_ase_markers : BAF difference between RNA and DNA for ASE markers comb.pval : combined p-value for the ASE test mean.delta : average BAF difference between RNA and DNA for all markers rawp : raw p-value for the ASE test Bonferroni : adjusted p-value for the ASE test (single-step Bonferroni) ABH : adjusted p-value for the ASE test (Benjamini-Hochberg) FPKM : FPKM value loo.source : which reference expression matrix was used in the outlier high expression (OHE) test loo.cohort.size : number of cases in the reference expression matrix for this gene loo.pval : p-value of the OHE test loo.rank : rank of the case under analysis among the reference cases imprinting.status : imprinting status of the gene candidate.group : status of the gene, combining both ASE and outlier tests Strand orientations are denoted with a + for a sense or coding strand and - for a antisense or non-coding strand. Coding sequence status is typically one of \"none\" (not specified), \"unk\" (unknown), \"incmpl\" (incomplete), or \"cmpl\" (complete).","title":"cis-activated candidates"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#sv-candidates","text":"Structural variant (SV) candidates include candidates predicted as the causal for the cis-activated genes in the regulatory territory. left.candidate.inTAD : cis-activated candidate near the left breakpoint right.candidate.inTAD : cis-activated candidate near the right breakpoint chrA : chromosome name of the left breakpoint posA : genomic location of the left breakpoint ortA : strand orientation of the left breakpoint chrB : chromosome name of the right breakpoint posB : genomic location of the right breakpoint ortB : strand orientation of the right breakpoint type : type of translocation","title":"SV candidates"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#cna-candidates","text":"Copy number aberration (CNA) candidates include candidates predicted as the causal for the cis-activated genes in the regulatory territory. candidate.inTAD : cis-activated candidate by the CNA chr : chromosome name start : genomic start position end : genomic end location logR : log ratio of the CNA","title":"CNA candidates"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#snvindel-candidates","text":"SNV/indel candidates include predicted candidates as functional and predicted transcription factors. The mutations are also annotated for known regulatory elements reported by the NIH Roadmap Epigenomics Project by collecting 111 cell lines. chrom : chromosome name pos : genomic start position ref : reference allele genotype mut : mutant allele genotype type : mutation type (either snv or indel ) target : cis-activated candidate dist : distance between the mutation and transcription start sites of the target gene tf : transcription factors predicted to have the binding motif introduced by the mutation EpiRoadmap_enhancer : enhancer regions that overlap with the mutation (from the NIH Roadmap Epigenomics Project ) EpiRoadmap_promoter : promoter regions that overlap with the mutation (from the NIH Roadmap Epigenomics Project ) EpiRoadmap_dyadic : dyadic regions that overlap with the mutation (from the NIH Roadmap Epigenomics Project )","title":"SNV/indel candidates"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#ohe-results","text":"OHE results are the raw results for the outlier expression test. Gene : gene symbol fpkm.raw : FPKM value size.bi : number of cases in the bi-allelic reference cohort p.bi : p-value in the outlier test using the bi-allelic reference cohort rank.bi : rank of the expression level in the case under analysis compared to the bi-allelic reference cohort size.cohort : number of cases in the entire reference cohort p.cohort : p-value in the outlier test using the entire reference cohort rank.cohort : rank of the expression level in the case under analysis compared to the entire reference cohort size.white : number of cases in the whitelist reference cohort p.white : p-value in the outlier test using the whitelist reference cohort rank.white : rank of the expression level in the case under analysis compared to the whitelist reference cohort","title":"OHE results"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#gene-level-ase-results","text":"Gene level ASE results are the raw results from the gene level ASE test. gene : gene accession number ( RefSeq ID) gsym : gene symbol chrom : chromosome name strand : strand orientation start : genomic start location end : genomic end location cdsStartStat : coding sequence (CDS) start status cdsEndStat : coding sequence (CDS) end status markers : number of heterozygous markers in this gene ase_markers : number of heterozygous markers showing allelic specific expressions (ASE) average_ai_all : average B-allele frequency (BAF) difference between RNA and DNA for all heterozygous markers average_ai_ase : average BAF difference between RNA and DNA for ASE markers pval_all_markers : p-value for each marker in the ASE test pval_ase_markers : p-value for ASE markers in the ASE test ai_all_markers : BAF difference between RNA and DNA for all heterozygrous markers ai_ase_markers : BAF difference between RNA and DNA for ASE markers comb.pval : combined p-value for the ASE test mean.delta : average BAF difference between RNA and DNA for all markers rawp : raw p-value for the ASE test Bonferroni : adjusted p-value for the ASE test (single-step Bonferroni) ABH : adjusted p-value for the ASE test (Benjamini-Hochberg)","title":"Gene level ASE results"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#single-marker-ase-results","text":"Single marker ASE results are the raw results from the single marker ASE test. chrom : chromosome name pos : genomic start position ref : reference allele genotype mut : non-reference allele genotype cvg_wgs : coverage of the marker from the whole genome sequence (WGS) mut_freq_wgs : non-reference allele fraction in the WGS cvg_rna : coverage of the marker from the RNA-seq mut_freq_rna : non-reference allele fraction in the RNA-seq ref.1 : read count of the reference allele in the RNA-seq var : read count of the non-reference allele in the RNA-seq pvalue : p-value from the binomial test delta.abs : absolute difference of the non-reference allele fraction between the WGS and RNA-seq","title":"Single marker ASE results"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#frequently-asked-questions","text":"None yet! If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/cis-x/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/command-line/","text":"Command Line Interaction Before you begin interacting with St. Jude Cloud Platform from the command line, you'll need to understand some details on the underlying architecture of the platform. The St. Jude Cloud Platform is built on top of a genomics cloud ecosystem provided by DNAnexus . Overview \u00b6 Workspaces in DNAnexus are organized by projects, which are essentially folders in the cloud. Each data request and tool in St. Jude Cloud creates its own unique cloud workspace (DNAnexus project). For instance, a data request creates a DNAnexus project behind the scenes with the same name as the request name you specify when you request data. Installation \u00b6 Open-source software provided by DNAnexus called the dx-toolkit is used to interact with the St. Jude Cloud Platform from the command line. You can use this to create these projects, upload and download data, and many other operations. You'll need to install that software on your computer by following this guide . Tip A quickstart to getting up and running with the dx-toolkit: Install Python 2.7.13+. Note that using the system-level Python is usually not a good idea (by default, system level Python is typically too old/does not support the latest security protocols required). You can install using Anaconda (recommended) or using the default Python installer . Run pip install dxpy . Type dx --help at the command line. A quick tour \u00b6 Logging in \u00b6 To log in using the dx-toolkit, run the following command: dx login --noprojects # enter username and password when prompted Note If you are a St. Jude employee, you'll need to follow this guide to log in instead. Selecting a project \u00b6 First, you'll need to choose which cloud workspace you would like to access. This depends on if you are downloading data from a request or working with input/output files from a tool. You can see the workspaces available to you by running the following command in your terminal: dx select This will present with a prompt similar to the below screenshot. A list of your available cloud workspaces will be shown with a number out to the left of each. You should enter the number corresponding to the workspace you are wanting to interact with. In the example below, the user has selected the Rapid RNA-Seq tool. Some useful commands \u00b6 Moving data back and forth between the cloud and your local computer is simple once you have selected the correct project for your tool. You will find that many common Linux commands with dx prepended work as expected. # list available files for the tool for the main folder dx ls # list all available files for the tool dx find . # list all commands dx --help Uploading data \u00b6 You can use the following process to upload data to be used by St. Jude Cloud Platform tools: First, click \"View\" on the tool you'd like to run from this page . In this example, we will choose the Rapid RNA-Seq tool. If you have not already, click \"Start\" on the tool you'd like to run. This will create a cloud workspace for you to upload your data to with the same name as the tool. Open up your terminal application and select the cloud workspace with the same name as the tool you are trying to run. Last, navigate to the local files you'd like to upload to the cloud and use the dx upload command as specified in [upload-download-data]{role=\"ref\"} to upload your data to St. Jude Cloud. Downloading data \u00b6 Warning To download data from a St. Jude Cloud data request, you must have indicated that you wished to download the data in your Data Access Agreement (DAA) during your submission. Any downloading of St. Jude data without completing this step is strictly PROHIBITED. You can use the following steps to download data from a St. Jude Cloud data request: Complete a data request using the Genomics Platform application. See this guide for instructions. Open up your terminal application and select the cloud workspace relevant to your data request. For instance, in this case we would type dx select \"Retinoblastoma Data\" . You can use typical commands like dx ls , dx pwd , and dx cd to navigate around your cloud folder as you would a local folder. Your project may look different based on what data you requested and whether you were previously approved to access the data. Your data should either be in the restricted folder (if this is your first time requesting access) or the immediate folder (if you were previously granted access permission). In the root of every data request is a file called SAMPLE_INFO.txt . This should contain all of the information about the samples you checked out as well as the associated metadata we provide. To download data from the cloud to local storage, use the dx download command as specified in [upload-download-data]{role=\"ref\"}. For instance, if I wanted to download all of the BAM files to my local computer, I would type dx download immediate/bam/* . Similar Topics \u00b6 About our Data Making a Data Request Working with our Data Overview Downloading/Uploading Data","title":"Command Line Interaction"},{"location":"guides/genomics-platform/analyzing-data/command-line/#overview","text":"Workspaces in DNAnexus are organized by projects, which are essentially folders in the cloud. Each data request and tool in St. Jude Cloud creates its own unique cloud workspace (DNAnexus project). For instance, a data request creates a DNAnexus project behind the scenes with the same name as the request name you specify when you request data.","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/command-line/#installation","text":"Open-source software provided by DNAnexus called the dx-toolkit is used to interact with the St. Jude Cloud Platform from the command line. You can use this to create these projects, upload and download data, and many other operations. You'll need to install that software on your computer by following this guide . Tip A quickstart to getting up and running with the dx-toolkit: Install Python 2.7.13+. Note that using the system-level Python is usually not a good idea (by default, system level Python is typically too old/does not support the latest security protocols required). You can install using Anaconda (recommended) or using the default Python installer . Run pip install dxpy . Type dx --help at the command line.","title":"Installation"},{"location":"guides/genomics-platform/analyzing-data/command-line/#a-quick-tour","text":"","title":"A quick tour"},{"location":"guides/genomics-platform/analyzing-data/command-line/#logging-in","text":"To log in using the dx-toolkit, run the following command: dx login --noprojects # enter username and password when prompted Note If you are a St. Jude employee, you'll need to follow this guide to log in instead.","title":"Logging in"},{"location":"guides/genomics-platform/analyzing-data/command-line/#selecting-a-project","text":"First, you'll need to choose which cloud workspace you would like to access. This depends on if you are downloading data from a request or working with input/output files from a tool. You can see the workspaces available to you by running the following command in your terminal: dx select This will present with a prompt similar to the below screenshot. A list of your available cloud workspaces will be shown with a number out to the left of each. You should enter the number corresponding to the workspace you are wanting to interact with. In the example below, the user has selected the Rapid RNA-Seq tool.","title":"Selecting a project"},{"location":"guides/genomics-platform/analyzing-data/command-line/#some-useful-commands","text":"Moving data back and forth between the cloud and your local computer is simple once you have selected the correct project for your tool. You will find that many common Linux commands with dx prepended work as expected. # list available files for the tool for the main folder dx ls # list all available files for the tool dx find . # list all commands dx --help","title":"Some useful commands"},{"location":"guides/genomics-platform/analyzing-data/command-line/#uploading-data","text":"You can use the following process to upload data to be used by St. Jude Cloud Platform tools: First, click \"View\" on the tool you'd like to run from this page . In this example, we will choose the Rapid RNA-Seq tool. If you have not already, click \"Start\" on the tool you'd like to run. This will create a cloud workspace for you to upload your data to with the same name as the tool. Open up your terminal application and select the cloud workspace with the same name as the tool you are trying to run. Last, navigate to the local files you'd like to upload to the cloud and use the dx upload command as specified in [upload-download-data]{role=\"ref\"} to upload your data to St. Jude Cloud.","title":"Uploading data"},{"location":"guides/genomics-platform/analyzing-data/command-line/#downloading-data","text":"Warning To download data from a St. Jude Cloud data request, you must have indicated that you wished to download the data in your Data Access Agreement (DAA) during your submission. Any downloading of St. Jude data without completing this step is strictly PROHIBITED. You can use the following steps to download data from a St. Jude Cloud data request: Complete a data request using the Genomics Platform application. See this guide for instructions. Open up your terminal application and select the cloud workspace relevant to your data request. For instance, in this case we would type dx select \"Retinoblastoma Data\" . You can use typical commands like dx ls , dx pwd , and dx cd to navigate around your cloud folder as you would a local folder. Your project may look different based on what data you requested and whether you were previously approved to access the data. Your data should either be in the restricted folder (if this is your first time requesting access) or the immediate folder (if you were previously granted access permission). In the root of every data request is a file called SAMPLE_INFO.txt . This should contain all of the information about the samples you checked out as well as the associated metadata we provide. To download data from the cloud to local storage, use the dx download command as specified in [upload-download-data]{role=\"ref\"}. For instance, if I wanted to download all of the BAM files to my local computer, I would type dx download immediate/bam/* .","title":"Downloading data"},{"location":"guides/genomics-platform/analyzing-data/command-line/#similar-topics","text":"About our Data Making a Data Request Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/","text":"Creating a Cloud Application Info If you came here from the remote work quickstart guide, you can use this link to quickly jump back to your place in that guide. This guide will take you through the process of writing an application for working with and manipulating the St. Jude data you've requested. By creating your own application, you will be able to wrap genomic tools and packages from external sources, as well as any tool or application you might have written yourself. Tip We have created a wide variety of example cloud applications which you can view at this GitHub repo . The completed source code for the particular application created here is available in the dx-fastqc-example-app folder. You are welcome to clone the repository and use it as a reference while following this tutorial or try building the application and running it on your own project. Overview \u00b6 The biggest difference between running an application in the cloud (as opposed to running it in a local environment) is the way we access that data and manipulate it. Writing and running your own cloud application grants numerous benefits. It allows you to submit numerous jobs in parallel, access your data from anywhere with an Internet connection, and utilize resources and compute power at a fraction of the cost (when compared to building your own infrastructure). Writing your own application will allow you to wrap custom tools to manipulate any data that you have previously requested. When you run your application, the request gets sent to a virtualized Linux container (Ubuntu 14.04 or 16.04) where any dependencies are installed and where your script will be run. Any tools or packages that you include (either through the included package managers, or bundled together in your project) will be available locally on the virtual Linux machine. However, there are differences in how we manage our data. When a job is submitted, a virtual machine is provisioned specifically for that job request, meaning that it is spun up at-will or when needed. It also implies that once the job has completed, the virtual machine will be reprovisioned or deleted. Any job output or data must be uploaded back to the project space. In this tutorial, we will be wrapping the FastQC , a quality control tool for raw sequence data, into our application. This will allow us to run FastQC on any of the St. Jude next generation sequencing data in the cloud. For specific information about how FastQC works, please refer to the FastQC documentation . Data \u00b6 For this tutorial I have requested the PCGP dataset, and once my access request has been approved, my project directory space will look like the following. Note If you do not yet have data in a DNAnexus project, you may request data from St. Jude Cloud by following the directions here or you may upload your own data using the data transfer app . In order to make a data request or upload your own data using the data transfer app, you must first create a St. Jude Cloud account . Writing the Application \u00b6 Requirements \u00b6 Tool Download Website Version dx-toolkit Source DNAnexus v0.291.1 FastQC Source Babraham Bioinformatics v0.11.8 Installing the dx-toolkit requires Python to be installed locally. Using the system provided version of Python can be problematic for a number of reasons, so we recommend using the Anaconda environment manager to install Python. You can view the following guides for how to install conda on your system. Windows. https://docs.anaconda.com/anaconda/install/windows/ Mac OS. https://docs.anaconda.com/anaconda/install/mac-os/ Linux. https://docs.anaconda.com/anaconda/install/linux/ Once you have conda installed, run the following commands to create a new environment with Python, activate it, and install the dx-toolkit. conda create -n dx python = 3 .7 conda activate dx pip install dxpy Now whenever you want to develop something for the cloud using dx-toolkit , just open your terminal and type conda activate dx . To access your DNAnexus projects from the commandline, you must login using dx-toolkit . For users not affiliated with St. Jude, simply type dx login and enter your username and password when prompted. Users with a St. Jude account will need to generate an API token for authentication. Instructions can be found here . Getting started \u00b6 The easiest way to install dx-toolkit is through pip , the Python package manager. Simply run the following command in your terminal: pip install dxpy --upgrade For this application, we will be using the dx-app-wizard command that is included in the dx-toolkit . dx-app-wizard is an interactive prompt that creates a boilerplate project that will allow you to quickly create an application. For more on dx-app-wizard , refer to the DNAnexus wiki article on Intro to Building Apps . Before continuing, be sure to refer to the command line interaction page for a walkthrough on how to install dx-toolkit and how to select your project workspace. Tip It is not necessary to use dx-app-wizard . All the necessary files and project directory structure can be created manually. However, dx-app-wizard provides a quick and easy way to get started. For more information, refer to the Advanced App Tutorial . All DNAnexus project applications will have the following structure: dx-fastqc-example-app/ \u251c\u2500\u2500 dxapp.json \u251c\u2500\u2500 resources/ \u2502 \u2514\u2500\u2500 usr/ \u2502 \u2514\u2500\u2500 bin/ \u2514\u2500\u2500 src/ \u2514\u2500\u2500 dx-fastqc-example-app.sh The dxapp.json file is a JSON file that contains metadata about the application we are writing that are needed to build and run the app on the DNAnexus Platform. Most notably, you will need to specify all of the inputs your app requires (both input files or any settings you can tune), output files, and other options such as the number of cores and memory required to run the tool. To see the full list of fields, refer to the DNAnexus wiki guide on the application metadata. The dx-fastqc-example-app.sh file is a bash script is what will be executed when the application is run. Any executable binaries that accompany the application, such as other tools or scripts, are placed in the resources folder. From there, we can call the executable from within the app when it is run. Creating the Project \u00b6 Start by running the dx-app-wizard command from your terminal. Info This helper tool will create a local directory on your machine. Any code changes we make will be done inside this local project directory created by dx-app-wizard . This is because we can write our application locally, build the application , and then run the application in the cloud. Building the application will compile dx-fastqc-example-app and then upload it into the project space on the cloud. When we run an application, it will be submitted as a job to be run in the cloud. With this process, we can write the application locally and run it on our data in the cloud, without ever having to utilize personal bandwidth and compute time. $ dx-app-wizard For our inputs, we will enter the following: $ App Name: dx-fastqc-example-app ... $ Title [] : FastQC Example Application ... $ Summary [] : Uses FastQC to generate quality control reports on raw sequence data. ... $ Version [ 0 .0.1 ] : 0 .0.1 ... $ 1st input name ( <ENTER> to finish ) : bam_file $ Label ( optional human-readable name ) [] : BAM File ... $ Choose a class ( <TAB> twice for choices ) : file $ This is an optional parameter [ y/n ] : n ... $ 1st output name ( <ENTER> to finish ) : fastqc_html $ Label ( optional human-readable name ) [] : FastQC HTML Report $ Choose a class ( <TAB> twice for choices ) : file $ 2nd output name ( <ENTER> to finish ) : fastqc_zip $ Label ( optional human-readable name ) [] : FastQC Zip File $ Choose a class ( <TAB> twice for choices ) : file ... $ Timeout policy [ 48h ] : 48h ... $ Programming language: bash ... $ Will this app need access to the Internet? [ y/N ] : N ... $ Will this app need access to the parent project? [ y/N ] : y ... $ Choose an instance type for your app [ mem1_ssd1_x4 ] : azure:mem1_ssd1_x4 Tip Although our app doesn't need any Internet access in this example, it may be required for your project. Also be sure to check what instance type you will need in the API Specifications . The FastQC executable supports a variety of file formats (BAM, SAM, FastQ, etc.), and outputs a HTML report and a zip file that contains all the graphs and data. We will use that knowledge to write the input and output parameters for our application. We can also specify other parameters such as the timeout policy, programming language, and instance type. For more information, refer to the IO and Run Specification guide. Integrating Tools and Packages \u00b6 Once we have finished creating the basic FastQC application using dx-app-wizard , the project structure should look like: dx-fastqc-example-app/ \u251c\u2500\u2500 Readme.developer.md \u251c\u2500\u2500 Readme.md \u251c\u2500\u2500 dxapp.json \u251c\u2500\u2500 resources/ \u251c\u2500\u2500 src/ \u2502 \u2514\u2500\u2500 dx-fastqc-example-app.sh \u2514\u2500\u2500 test/ Info Anything in the resources folder is unpacked into the root directory ( / ) of the virtual Linux machine that your application will run on. If we create the directory path dx-fastqc-example-app/resources/usr/bin/ , anything in the bin folder would be unpacked into /usr/bin/ on the Linux machine. This is handy because that path is included in the default $PATH environment variable. Your application's executable will use /home/dnanexus/ as its current working directory. Though dx-app-wizard does not create this, we can create it ourselves. Paste the following lines into your terminal. $ mkdir -p dx-fastqc-example-app/resources/usr/bin Packaging FastQC \u00b6 To incorporate FastQC into this project, we need to download the executable binary and package it within the dx-fastqc-example-app . Download the FastQC v0.11.8 (Win/Linux zip file) and unzip it. After unzipping, move the FastQC folder into the resources folder. $ unzip fastqc_v0.11.8.zip $ mv FastQC /path/to/project/dx-fastqc-example-app/resources/ Now, our project will look like this: dx-fastqc-example-app/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 Readme.developer.md \u251c\u2500\u2500 dxapp.json \u251c\u2500\u2500 test/ \u251c\u2500\u2500 resources/ \u2502 \u251c\u2500\u2500 FastQC/ \u2502 \u2502 \u251c\u2500\u2500 fastqc \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 usr/ \u2502 \u2514\u2500\u2500 bin/ \u2514\u2500\u2500 src/ \u2514\u2500\u2500 dx-fastqc-example-app.sh Installing Dependencies \u00b6 Tip If you are importing custom tools, or are using tools that rely on various packages and requirements, they can be specified in the \"runSpec\". For more information on installing dependencies and available software packages, refer to the Execution Environment Reference . Some external package managers that we can leverage when building an app include: Package Manager Application APT Advanced Packaging Tool for Ubuntu CPAN Comprehensive Perl Archive Network CRAN Comprehensive R Archive Network gem Package Manager for Ruby pip PyPI (Python Package Index) One requirement for FastQC is that it must have a suitable Java Runtime Environment . To include this in the app, we have to edit the dxapp.json file. Open dxapp.json and append the following line to \"runSpec\" : \"execDepends\" : [ { \"name\" : \"openjdk-7-jre-headless\" , \"package_manager\" : \"apt\" } ] Be sure to add a comma at the very end of the \"file\" object line to accommodate the new \"execDepends\" lines. Now, the \"runSpec\" object should look like the following: ... \"runSpec\" : { \"timeoutPolicy\" : { \"*\" : { \"hours\" : 48 } }, \"interpreter\" : \"bash\" , \"release\" : \"14.04\" , \"distribution\" : \"Ubuntu\" , \"file\" : \"src/dx-fastqc-example-app.sh\" , \"execDepends\" : [ { \"name\" : \"openjdk-7-jre-headless\" , \"package_manager\" : \"apt\" } ] } , ... When you build and run your application, the virtual environment will now download openjdk-7 from Ubuntu's APT package manager as a prerequisite. For more information on how to specify packages from Git, R, or Python, refer to the Software Packages wiki page. Calling FastQC \u00b6 The last step is to call the FastQC executable from within the app. Open up src/dx-fastqc-example-app.sh with a text editor. Inside this Bash script is where we will be working with FastQC and our data. Before we dive in, its a good idea to add a few useful parameters for the script execution. Right after the Bash shebang ( #!/bin/bash ), add the following line: set -e -x Below is a table describing what each flag does: Flag Description -e Exit immediately if a command exits with a non-zero status. -x Print each command to standard error before execution. Our first change has to do with how our BAM file is downloaded. Although dx-app-wizard automatically generates a line that will download the input file and rename it, we want to keep the original file name because FastQC uses the input file as part of the report name. Remove the -o bam_file portion so the line looks like the following: dx download \" $bam_file \" # Downloads our input BAM file without renaming After the application downloads the input file, we need to create the appropriate output directories and run FastQC on our BAM file. Add the following lines to the bash script within the main function: mkdir ~/fastqc-out/ # FastQC Output Folder /FastQC/fastqc \" $bam_file_name \" -o ~/fastqc-out # Runs FastQC on BAM File Tip Be sure to use \"$bam_file_name\" as our input for FastQC. Using \"$bam_file\" only returns the DNAnexus file-id associated with the input file. For more information on helper variables, refer to the Advanced App Tutorial . Uploading Files \u00b6 After FastQC finishes, the last thing to do is to upload the reports generated by FastQC to our project. These virtual Linux machines are provisioned at-will, meaning that they are only spun up when a job is submitted. When we create an application and run it in the cloud, we submit it as a job to be executed. When a job gets executed, a virtual machine will download all the necessary requirements (tools, packages, data, etc.) and run the job. Any output files on the machine must be uploaded back to the project space after a job finishes executing. Any information and data not uploaded to the project space will be inaccessible and lost. You will see two lines generated for us by dx-app-wizard when we specified the outputs for our application. We need to change these to upload the correct files from our output directory that we specified for FastQC. Otherwise, it assumes they are in the home directory. Before this, we can also (optionally) rename the files to be uploaded. Add the following lines, making sure to replace the two original upload lines. Lines to remove/overwrite: # Generated by dx-app-wizard fastqc_html = $( dx upload fastqc_html --brief ) fastqc_zip = $( dx upload fastqc_zip --brief ) Lines to add: # (Optional) Renames the FastQC reports mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip # Uploads the respective HTML and Zip file (lines to change) fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief ) fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief ) In this step, we are also moving the HTML and Zip file generated by FastQC to the directories which will be uploaded. After this step, dx-fastqc-example-app.sh should look like: #!/bin/bash set -e -x main () { echo \"Value of bam_file: ' $bam_file '\" # Downloads file from project to virtual machine workspace dx download \" $bam_file \" # Creating output directory for FastQC mkdir ~/fastqc-out # Runs FastQC on BAM file /FastQC/fastqc \" $bam_file_name \" -o ~/fastqc-out # Renames the FastQC reports to include the BAM file prefix mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip # Uploads the respective HTML and Zip file fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief ) fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief ) # Adds and formats appropriate output variables for your app dx-jobutil-add-output fastqc_html \" $fastqc_html \" --class = file dx-jobutil-add-output fastqc_zip \" $fastqc_zip \" --class = file } Building Your App \u00b6 Before building, ensure that you are in the parent directory of the local project folder generated by dx-app-wizard . To check, if you enter the command ls , you should see the project folder dx-fastqc-example-app/ appear in the output. To build your application, enter the following into your terminal: $ dx build dx-fastqc-example-app This command will package the tools and files as an application which can then be run on the DNAnexus Platform. In the screenshot below, you can see the compiled app in our project workspace selected and highlighted in blue. To verify that the build was completed successfully, you can enter dx ls . This should show you all the files in your project space in the cloud. # This will show what files are in your root directory for your project space in the cloud $ dx ls You should see something along the lines of this printed out in your terminal. Note that a compiled copy of our dx-fastqc-example-app now lives in the project. . \u251c\u2500\u2500 immediate/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 dx-fastqc-example-app \u2514\u2500\u2500 SAMPLE_INFO.txt You can also view the project directly from your browser. You will see a similar result. Any time you make any changes to the scripts or the application, you will need to rebuild the application. To overwrite a previous version of the app, specify the -f command. You can also inspect and configure the application by clicking on it. Running Your App \u00b6 To run the dx-fastqc-example-app , enter the following into the terminal: $ dx run dx-fastqc-example-app -i bam_file = /path/to/<bam-file>.bam For this example, I am using the PCGP dataset and my run command will look like the following: $ dx run dx-fastqc-example-app -i bam_file = /immediate/bam/SJBALL020073_D1.RNA-Seq.bam The input path will vary depending on how the data looks inside your DNAnexus project, but it might look like the following: /restricted/bam/<bam-file>.bam You will be prompted to confirm that you wish to run the application with the following JSON input and whether you would like to monitor the job in your terminal. Using input JSON: { \"bam_file\" : { \" $dnanexus_link \" : { \"project\" : \"project-FV9XFG0991ZbPVgQ2jx1vZv5\" , \"id\" : \"file-FV9gzf8991ZXQ1kv7V3BqgjV\" } } } Confirm running the executable with this input [ Y/n ] : Y Calling applet-FVbY8Qj991ZQ1863BGK6x0bk with output destination project-FV9XFG0991ZbPVgQ2jx1vZv5:/ Job ID: job-FVbY8Z0991ZXx5v1Fk3QgJPV Watch launched job now? [ Y/n ] Y Job Log ------- Watching job job-FVbY8Z0991ZXx5v1Fk3QgJPV. Press Ctrl+C to stop. You can also monitor active jobs by going to the project space and selecting the \"Monitor\" tab. Job Completion \u00b6 Once the job finishes, you will receive an email from DNAnexus ( notification@dnanexus.com ) about whether the job has completed successfully or failed. Make sure to check that these emails don't get sent to your spam folder. Clicking the links in the email should open up a new tab in your browser and take you to the appropriate project. Here, we can see that FastQC has run successfully and that the two files generated by FastQC have been uploaded back into our project space. Again, if we run the dx ls command, we can verify that two new files titled \"fastqc-report.html\" and \"fastqc-report.zip\" are in the root directory of our project. . \u251c\u2500\u2500 immediate/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 dx-fastqc-example-app \u251c\u2500\u2500 fastqc-report.html \u251c\u2500\u2500 fastqc-report.zip \u2514\u2500\u2500 SAMPLE_INFO.txt Conclusion \u00b6 If you have made it this far, you have likely wrapped your first genomic analysis tool for use in the cloud. For your reference, we have included the final FastQC application at the St. Jude App Tutorial Repository . If you have any questions or suggestions on how we can improve this tutorial, please file an issue , contact us at https://stjude.cloud/contact , or email us at support@stjude.cloud . Similar Topics \u00b6 About our Data Making a Data Request Working with our Data Overview Downloading/Uploading Data Command Line Interaction","title":"Creating your own Workflow"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#overview","text":"The biggest difference between running an application in the cloud (as opposed to running it in a local environment) is the way we access that data and manipulate it. Writing and running your own cloud application grants numerous benefits. It allows you to submit numerous jobs in parallel, access your data from anywhere with an Internet connection, and utilize resources and compute power at a fraction of the cost (when compared to building your own infrastructure). Writing your own application will allow you to wrap custom tools to manipulate any data that you have previously requested. When you run your application, the request gets sent to a virtualized Linux container (Ubuntu 14.04 or 16.04) where any dependencies are installed and where your script will be run. Any tools or packages that you include (either through the included package managers, or bundled together in your project) will be available locally on the virtual Linux machine. However, there are differences in how we manage our data. When a job is submitted, a virtual machine is provisioned specifically for that job request, meaning that it is spun up at-will or when needed. It also implies that once the job has completed, the virtual machine will be reprovisioned or deleted. Any job output or data must be uploaded back to the project space. In this tutorial, we will be wrapping the FastQC , a quality control tool for raw sequence data, into our application. This will allow us to run FastQC on any of the St. Jude next generation sequencing data in the cloud. For specific information about how FastQC works, please refer to the FastQC documentation .","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#data","text":"For this tutorial I have requested the PCGP dataset, and once my access request has been approved, my project directory space will look like the following. Note If you do not yet have data in a DNAnexus project, you may request data from St. Jude Cloud by following the directions here or you may upload your own data using the data transfer app . In order to make a data request or upload your own data using the data transfer app, you must first create a St. Jude Cloud account .","title":"Data"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#writing-the-application","text":"","title":"Writing the Application"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#requirements","text":"Tool Download Website Version dx-toolkit Source DNAnexus v0.291.1 FastQC Source Babraham Bioinformatics v0.11.8 Installing the dx-toolkit requires Python to be installed locally. Using the system provided version of Python can be problematic for a number of reasons, so we recommend using the Anaconda environment manager to install Python. You can view the following guides for how to install conda on your system. Windows. https://docs.anaconda.com/anaconda/install/windows/ Mac OS. https://docs.anaconda.com/anaconda/install/mac-os/ Linux. https://docs.anaconda.com/anaconda/install/linux/ Once you have conda installed, run the following commands to create a new environment with Python, activate it, and install the dx-toolkit. conda create -n dx python = 3 .7 conda activate dx pip install dxpy Now whenever you want to develop something for the cloud using dx-toolkit , just open your terminal and type conda activate dx . To access your DNAnexus projects from the commandline, you must login using dx-toolkit . For users not affiliated with St. Jude, simply type dx login and enter your username and password when prompted. Users with a St. Jude account will need to generate an API token for authentication. Instructions can be found here .","title":"Requirements"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#getting-started","text":"The easiest way to install dx-toolkit is through pip , the Python package manager. Simply run the following command in your terminal: pip install dxpy --upgrade For this application, we will be using the dx-app-wizard command that is included in the dx-toolkit . dx-app-wizard is an interactive prompt that creates a boilerplate project that will allow you to quickly create an application. For more on dx-app-wizard , refer to the DNAnexus wiki article on Intro to Building Apps . Before continuing, be sure to refer to the command line interaction page for a walkthrough on how to install dx-toolkit and how to select your project workspace. Tip It is not necessary to use dx-app-wizard . All the necessary files and project directory structure can be created manually. However, dx-app-wizard provides a quick and easy way to get started. For more information, refer to the Advanced App Tutorial . All DNAnexus project applications will have the following structure: dx-fastqc-example-app/ \u251c\u2500\u2500 dxapp.json \u251c\u2500\u2500 resources/ \u2502 \u2514\u2500\u2500 usr/ \u2502 \u2514\u2500\u2500 bin/ \u2514\u2500\u2500 src/ \u2514\u2500\u2500 dx-fastqc-example-app.sh The dxapp.json file is a JSON file that contains metadata about the application we are writing that are needed to build and run the app on the DNAnexus Platform. Most notably, you will need to specify all of the inputs your app requires (both input files or any settings you can tune), output files, and other options such as the number of cores and memory required to run the tool. To see the full list of fields, refer to the DNAnexus wiki guide on the application metadata. The dx-fastqc-example-app.sh file is a bash script is what will be executed when the application is run. Any executable binaries that accompany the application, such as other tools or scripts, are placed in the resources folder. From there, we can call the executable from within the app when it is run.","title":"Getting started"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#creating-the-project","text":"Start by running the dx-app-wizard command from your terminal. Info This helper tool will create a local directory on your machine. Any code changes we make will be done inside this local project directory created by dx-app-wizard . This is because we can write our application locally, build the application , and then run the application in the cloud. Building the application will compile dx-fastqc-example-app and then upload it into the project space on the cloud. When we run an application, it will be submitted as a job to be run in the cloud. With this process, we can write the application locally and run it on our data in the cloud, without ever having to utilize personal bandwidth and compute time. $ dx-app-wizard For our inputs, we will enter the following: $ App Name: dx-fastqc-example-app ... $ Title [] : FastQC Example Application ... $ Summary [] : Uses FastQC to generate quality control reports on raw sequence data. ... $ Version [ 0 .0.1 ] : 0 .0.1 ... $ 1st input name ( <ENTER> to finish ) : bam_file $ Label ( optional human-readable name ) [] : BAM File ... $ Choose a class ( <TAB> twice for choices ) : file $ This is an optional parameter [ y/n ] : n ... $ 1st output name ( <ENTER> to finish ) : fastqc_html $ Label ( optional human-readable name ) [] : FastQC HTML Report $ Choose a class ( <TAB> twice for choices ) : file $ 2nd output name ( <ENTER> to finish ) : fastqc_zip $ Label ( optional human-readable name ) [] : FastQC Zip File $ Choose a class ( <TAB> twice for choices ) : file ... $ Timeout policy [ 48h ] : 48h ... $ Programming language: bash ... $ Will this app need access to the Internet? [ y/N ] : N ... $ Will this app need access to the parent project? [ y/N ] : y ... $ Choose an instance type for your app [ mem1_ssd1_x4 ] : azure:mem1_ssd1_x4 Tip Although our app doesn't need any Internet access in this example, it may be required for your project. Also be sure to check what instance type you will need in the API Specifications . The FastQC executable supports a variety of file formats (BAM, SAM, FastQ, etc.), and outputs a HTML report and a zip file that contains all the graphs and data. We will use that knowledge to write the input and output parameters for our application. We can also specify other parameters such as the timeout policy, programming language, and instance type. For more information, refer to the IO and Run Specification guide.","title":"Creating the Project"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#integrating-tools-and-packages","text":"Once we have finished creating the basic FastQC application using dx-app-wizard , the project structure should look like: dx-fastqc-example-app/ \u251c\u2500\u2500 Readme.developer.md \u251c\u2500\u2500 Readme.md \u251c\u2500\u2500 dxapp.json \u251c\u2500\u2500 resources/ \u251c\u2500\u2500 src/ \u2502 \u2514\u2500\u2500 dx-fastqc-example-app.sh \u2514\u2500\u2500 test/ Info Anything in the resources folder is unpacked into the root directory ( / ) of the virtual Linux machine that your application will run on. If we create the directory path dx-fastqc-example-app/resources/usr/bin/ , anything in the bin folder would be unpacked into /usr/bin/ on the Linux machine. This is handy because that path is included in the default $PATH environment variable. Your application's executable will use /home/dnanexus/ as its current working directory. Though dx-app-wizard does not create this, we can create it ourselves. Paste the following lines into your terminal. $ mkdir -p dx-fastqc-example-app/resources/usr/bin","title":"Integrating Tools and Packages"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#packaging-fastqc","text":"To incorporate FastQC into this project, we need to download the executable binary and package it within the dx-fastqc-example-app . Download the FastQC v0.11.8 (Win/Linux zip file) and unzip it. After unzipping, move the FastQC folder into the resources folder. $ unzip fastqc_v0.11.8.zip $ mv FastQC /path/to/project/dx-fastqc-example-app/resources/ Now, our project will look like this: dx-fastqc-example-app/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 Readme.developer.md \u251c\u2500\u2500 dxapp.json \u251c\u2500\u2500 test/ \u251c\u2500\u2500 resources/ \u2502 \u251c\u2500\u2500 FastQC/ \u2502 \u2502 \u251c\u2500\u2500 fastqc \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 usr/ \u2502 \u2514\u2500\u2500 bin/ \u2514\u2500\u2500 src/ \u2514\u2500\u2500 dx-fastqc-example-app.sh","title":"Packaging FastQC"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#installing-dependencies","text":"Tip If you are importing custom tools, or are using tools that rely on various packages and requirements, they can be specified in the \"runSpec\". For more information on installing dependencies and available software packages, refer to the Execution Environment Reference . Some external package managers that we can leverage when building an app include: Package Manager Application APT Advanced Packaging Tool for Ubuntu CPAN Comprehensive Perl Archive Network CRAN Comprehensive R Archive Network gem Package Manager for Ruby pip PyPI (Python Package Index) One requirement for FastQC is that it must have a suitable Java Runtime Environment . To include this in the app, we have to edit the dxapp.json file. Open dxapp.json and append the following line to \"runSpec\" : \"execDepends\" : [ { \"name\" : \"openjdk-7-jre-headless\" , \"package_manager\" : \"apt\" } ] Be sure to add a comma at the very end of the \"file\" object line to accommodate the new \"execDepends\" lines. Now, the \"runSpec\" object should look like the following: ... \"runSpec\" : { \"timeoutPolicy\" : { \"*\" : { \"hours\" : 48 } }, \"interpreter\" : \"bash\" , \"release\" : \"14.04\" , \"distribution\" : \"Ubuntu\" , \"file\" : \"src/dx-fastqc-example-app.sh\" , \"execDepends\" : [ { \"name\" : \"openjdk-7-jre-headless\" , \"package_manager\" : \"apt\" } ] } , ... When you build and run your application, the virtual environment will now download openjdk-7 from Ubuntu's APT package manager as a prerequisite. For more information on how to specify packages from Git, R, or Python, refer to the Software Packages wiki page.","title":"Installing Dependencies"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#calling-fastqc","text":"The last step is to call the FastQC executable from within the app. Open up src/dx-fastqc-example-app.sh with a text editor. Inside this Bash script is where we will be working with FastQC and our data. Before we dive in, its a good idea to add a few useful parameters for the script execution. Right after the Bash shebang ( #!/bin/bash ), add the following line: set -e -x Below is a table describing what each flag does: Flag Description -e Exit immediately if a command exits with a non-zero status. -x Print each command to standard error before execution. Our first change has to do with how our BAM file is downloaded. Although dx-app-wizard automatically generates a line that will download the input file and rename it, we want to keep the original file name because FastQC uses the input file as part of the report name. Remove the -o bam_file portion so the line looks like the following: dx download \" $bam_file \" # Downloads our input BAM file without renaming After the application downloads the input file, we need to create the appropriate output directories and run FastQC on our BAM file. Add the following lines to the bash script within the main function: mkdir ~/fastqc-out/ # FastQC Output Folder /FastQC/fastqc \" $bam_file_name \" -o ~/fastqc-out # Runs FastQC on BAM File Tip Be sure to use \"$bam_file_name\" as our input for FastQC. Using \"$bam_file\" only returns the DNAnexus file-id associated with the input file. For more information on helper variables, refer to the Advanced App Tutorial .","title":"Calling FastQC"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#uploading-files","text":"After FastQC finishes, the last thing to do is to upload the reports generated by FastQC to our project. These virtual Linux machines are provisioned at-will, meaning that they are only spun up when a job is submitted. When we create an application and run it in the cloud, we submit it as a job to be executed. When a job gets executed, a virtual machine will download all the necessary requirements (tools, packages, data, etc.) and run the job. Any output files on the machine must be uploaded back to the project space after a job finishes executing. Any information and data not uploaded to the project space will be inaccessible and lost. You will see two lines generated for us by dx-app-wizard when we specified the outputs for our application. We need to change these to upload the correct files from our output directory that we specified for FastQC. Otherwise, it assumes they are in the home directory. Before this, we can also (optionally) rename the files to be uploaded. Add the following lines, making sure to replace the two original upload lines. Lines to remove/overwrite: # Generated by dx-app-wizard fastqc_html = $( dx upload fastqc_html --brief ) fastqc_zip = $( dx upload fastqc_zip --brief ) Lines to add: # (Optional) Renames the FastQC reports mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip # Uploads the respective HTML and Zip file (lines to change) fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief ) fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief ) In this step, we are also moving the HTML and Zip file generated by FastQC to the directories which will be uploaded. After this step, dx-fastqc-example-app.sh should look like: #!/bin/bash set -e -x main () { echo \"Value of bam_file: ' $bam_file '\" # Downloads file from project to virtual machine workspace dx download \" $bam_file \" # Creating output directory for FastQC mkdir ~/fastqc-out # Runs FastQC on BAM file /FastQC/fastqc \" $bam_file_name \" -o ~/fastqc-out # Renames the FastQC reports to include the BAM file prefix mv ~/fastqc-out/*.html ~/fastqc-out/fastqc-report.html mv ~/fastqc-out/*.zip ~/fastqc-out/fastqc-report.zip # Uploads the respective HTML and Zip file fastqc_html = $( dx upload ~/fastqc-out/fastqc-report.html --brief ) fastqc_zip = $( dx upload ~/fastqc-out/fastqc-report.zip --brief ) # Adds and formats appropriate output variables for your app dx-jobutil-add-output fastqc_html \" $fastqc_html \" --class = file dx-jobutil-add-output fastqc_zip \" $fastqc_zip \" --class = file }","title":"Uploading Files"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#building-your-app","text":"Before building, ensure that you are in the parent directory of the local project folder generated by dx-app-wizard . To check, if you enter the command ls , you should see the project folder dx-fastqc-example-app/ appear in the output. To build your application, enter the following into your terminal: $ dx build dx-fastqc-example-app This command will package the tools and files as an application which can then be run on the DNAnexus Platform. In the screenshot below, you can see the compiled app in our project workspace selected and highlighted in blue. To verify that the build was completed successfully, you can enter dx ls . This should show you all the files in your project space in the cloud. # This will show what files are in your root directory for your project space in the cloud $ dx ls You should see something along the lines of this printed out in your terminal. Note that a compiled copy of our dx-fastqc-example-app now lives in the project. . \u251c\u2500\u2500 immediate/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 dx-fastqc-example-app \u2514\u2500\u2500 SAMPLE_INFO.txt You can also view the project directly from your browser. You will see a similar result. Any time you make any changes to the scripts or the application, you will need to rebuild the application. To overwrite a previous version of the app, specify the -f command. You can also inspect and configure the application by clicking on it.","title":"Building Your App"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#running-your-app","text":"To run the dx-fastqc-example-app , enter the following into the terminal: $ dx run dx-fastqc-example-app -i bam_file = /path/to/<bam-file>.bam For this example, I am using the PCGP dataset and my run command will look like the following: $ dx run dx-fastqc-example-app -i bam_file = /immediate/bam/SJBALL020073_D1.RNA-Seq.bam The input path will vary depending on how the data looks inside your DNAnexus project, but it might look like the following: /restricted/bam/<bam-file>.bam You will be prompted to confirm that you wish to run the application with the following JSON input and whether you would like to monitor the job in your terminal. Using input JSON: { \"bam_file\" : { \" $dnanexus_link \" : { \"project\" : \"project-FV9XFG0991ZbPVgQ2jx1vZv5\" , \"id\" : \"file-FV9gzf8991ZXQ1kv7V3BqgjV\" } } } Confirm running the executable with this input [ Y/n ] : Y Calling applet-FVbY8Qj991ZQ1863BGK6x0bk with output destination project-FV9XFG0991ZbPVgQ2jx1vZv5:/ Job ID: job-FVbY8Z0991ZXx5v1Fk3QgJPV Watch launched job now? [ Y/n ] Y Job Log ------- Watching job job-FVbY8Z0991ZXx5v1Fk3QgJPV. Press Ctrl+C to stop. You can also monitor active jobs by going to the project space and selecting the \"Monitor\" tab.","title":"Running Your App"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#job-completion","text":"Once the job finishes, you will receive an email from DNAnexus ( notification@dnanexus.com ) about whether the job has completed successfully or failed. Make sure to check that these emails don't get sent to your spam folder. Clicking the links in the email should open up a new tab in your browser and take you to the appropriate project. Here, we can see that FastQC has run successfully and that the two files generated by FastQC have been uploaded back into our project space. Again, if we run the dx ls command, we can verify that two new files titled \"fastqc-report.html\" and \"fastqc-report.zip\" are in the root directory of our project. . \u251c\u2500\u2500 immediate/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 dx-fastqc-example-app \u251c\u2500\u2500 fastqc-report.html \u251c\u2500\u2500 fastqc-report.zip \u2514\u2500\u2500 SAMPLE_INFO.txt","title":"Job Completion"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#conclusion","text":"If you have made it this far, you have likely wrapped your first genomic analysis tool for use in the cloud. For your reference, we have included the final FastQC application at the St. Jude App Tutorial Repository . If you have any questions or suggestions on how we can improve this tutorial, please file an issue , contact us at https://stjude.cloud/contact , or email us at support@stjude.cloud .","title":"Conclusion"},{"location":"guides/genomics-platform/analyzing-data/creating-a-cloud-app/#similar-topics","text":"About our Data Making a Data Request Working with our Data Overview Downloading/Uploading Data Command Line Interaction","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/fkpm/","text":"FKPM Authors John Doe Publication N/A (not published) Technical Support Contact Us Overview \u00b6 abstract-type description of tool Inputs \u00b6 table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500 Input file configuration \u00b6 if needed Outputs \u00b6 table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format Workflow Steps \u00b6 description of algorithm(s) or workflow steps Additional Info \u00b6 description of any additional information that the user might need to know/do before running the workflow Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here . Uploading Input files \u00b6 note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 detailed explanations with helpful screenshots or gifs Frequently asked questions \u00b6 faqs If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"FKPM"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#overview","text":"abstract-type description of tool","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#inputs","text":"table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#input-file-configuration","text":"if needed","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#outputs","text":"table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#workflow-steps","text":"description of algorithm(s) or workflow steps","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#additional-info","text":"description of any additional information that the user might need to know/do before running the workflow","title":"Additional Info"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#uploading-input-files","text":"note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input files"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#interpreting-results","text":"detailed explanations with helpful screenshots or gifs","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#frequently-asked-questions","text":"faqs If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/fkpm/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/m2a/","text":"M2A Authors John Doe Publication N/A (not published) Technical Support Contact Us Overview \u00b6 abstract-type description of tool Inputs \u00b6 table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500 Input file configuration \u00b6 if needed Outputs \u00b6 table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format Workflow Steps \u00b6 description of algorithm(s) or workflow steps Additional Info \u00b6 description of any additional information that the user might need to know/do before running the workflow Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here . Uploading Input files \u00b6 note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 detailed explanations with helpful screenshots or gifs Frequently asked questions \u00b6 faqs If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"M2A"},{"location":"guides/genomics-platform/analyzing-data/m2a/#overview","text":"abstract-type description of tool","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/m2a/#inputs","text":"table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/m2a/#input-file-configuration","text":"if needed","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/m2a/#outputs","text":"table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/m2a/#workflow-steps","text":"description of algorithm(s) or workflow steps","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/m2a/#additional-info","text":"description of any additional information that the user might need to know/do before running the workflow","title":"Additional Info"},{"location":"guides/genomics-platform/analyzing-data/m2a/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/m2a/#uploading-input-files","text":"note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input files"},{"location":"guides/genomics-platform/analyzing-data/m2a/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/m2a/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/m2a/#interpreting-results","text":"detailed explanations with helpful screenshots or gifs","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/m2a/#frequently-asked-questions","text":"faqs If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/m2a/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/","text":"Authors Scott Newman, Michael Macias Publication Mutational Signatures employs MutationalPatterns: \" MutationalPatterns: comprehensive genome-wide analysis of mutational processes. \" Technical Support Contact Us Overview \u00b6 Mutational Signatures finds and quantifies COSMIC mutational signatures across samples. This is done by finding the optimal non-negative linear combination of mutation signatures to reconstruct a mutation matrix. It builds the initial mutation matrix from multiple single-sample VCFs and, by default, fits it to mutational signatures from COSMIC . Mutational Signatures employs MutationalPatterns ( Blokzijl, et al. (2018) ) to achieve this. Mutational Signatures supports both hg19 (GRCh37) and hg38 (GRCh38). Inputs \u00b6 Name Type Description Example VCF(s) Array of files List of VCF inputs. Can be single-sample or multi-sample and uncompressed or gzipped. [ *.vcf , *.vcf.gz ] Sample sheet File Tab-delimited file (no headers) with sample ID and tag pairs [optional] *.txt Genome build String Genome build used as reference. Can be either \"GRCh37\" or \"GRCh38\". [default: \"GRCh38\"] GRCh38 Minimum mutation burden Integer Minimum number of somatic SNVs a sample must have to be considered for analysis [default: 9] 15 Minimum signature contribution Integer Minimum number of mutations attributable to a single signature [default: 9] 100 Output prefix String Prefix to append to output filenames [optional] mtsg Disabled VCF column Integer VCF column (starting from sample names, zero-based) to ignore when reading VCFS [optional] 1 Input configuration \u00b6 Mutational Signatures only requires VCFs as inputs. This can be a single multi-sample VCF, multiple single-sample VCFs, or a combination of both. All other inputs are optional. VCF(s) VCF(s) is a list of VCF inputs. The inputs can be single-sample or multi-sample and uncompressed or gzipped. Sample names are taken from the VCF header. When using multi-sample VCFs, empty cells/absent variant calls must be denoted with .:. . gVCFs are not supported. Sample Sheet Sample sheet is a tab-delimited file (no headers) with two columns: the sample ID and a tag. The tag is an arbitrary identifier used to group the samples, typically a disease abbreviation or tissue of origin. If not given, a sample sheet will be generated automatically. Example SJACT001_D ACT SJACT002_D ACT SJBALL063_D BALL SJHGG017_D HGG Output prefix Output prefix is the prefix to append to the output filenames. By default, if a single input VCF is given, its basename is used as the output prefix. If multiple input VCFs are given, a default \"mtsg\" prefix is used. This behavior can be overridden by a user-defined prefix. Example VCF(s) Prefix Output filename for raw signatures [ pcgp.b38.refseq.goodbad.vcf ] pcgp.b38.refseq.goodbad pcgp.b38.refseq.goodbad.signatures.txt [ SJOS013_D.vcf , SJRHB007_D.vcf ] mtsg mtsg.signatures.txt Disabled VCF column Disabled VCF column is the column index to ignore when reading VCFs. This is useful when the inputs are tumor-normal VCFs, and one column should be ignored. Otherwise, the results would likely be duplicated. The argument is a zero-based index relative to the sample names in the header of the VCF. For example, in a VCF with samples SJEPD003_D and SJEPD003_G , the germline sample ( SJEPD003_G ) can be discarded by setting the disabled VCF column to 1 . Example 0 1 #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT SJEPD003_D SJEPD003_G Outputs \u00b6 Name Type Description Raw signatures File Tab-delimited file of the raw results with sample contributions for each signature Signatures visualization File HTML file for interactive plotting Sample sheet File Tab-delimited file (no headers) with sample ID and tag pairs Workflow Steps \u00b6 Mutational Signatures runs four steps using subcommands of mtsg . Split VCFs (single or multi-sample) to multiple single-sample VCFs. If not given, generate a sample sheet from the directory of single-sample VCFs. Build a mutation matrix and reconstruct/fit it using COSMIC mutation signatures. Create a visualization file using the fitted signatures. Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the Mutational Signatures workflow page here . Uploading Input Files \u00b6 Mutational Signatures requires at least one VCF and an optional sample sheet to be uploaded. Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 Upon a successful run of Mutational Signatures, three files are saved to the results directory: raw signature contributions, a visualization file, and a sample sheet. Raw signatures \u00b6 Raw signatures is a tab-delimited file of the raw results with sample contributions for each signature. Column 1 is the sample name, columns 2-(N-1) are the COSMIC signatures contribution counts, and column N is the group tag, where N is the total number of columns. The number of columns is variable since if the signature has no contributions for all samples, it is completely omitted. Note that the last column tissue is a misnomer. It aligns to the arbitrary tag given in the sample sheet . Example Signature.1 Signature.2 \u2026 Signature.30 tissue SJACT001_D 1.71758029 131.033723 \u2026 18.6910151 ACT SJAMLM7005_D 51.9627312 7.10850351 \u2026 0 AMLM7 Signatures visualization \u00b6 Signatures visualization is an HTML file that can be used for interactive plotting. When opened in a web browser, a set of controls allows plotting various stacked bar charts: total contributions by signature, total contributions by tag, and total contributions by sample per tag. The total contributions can be stacked as absolute values or as a percentage of the total. Sample Sheet \u00b6 When no sample sheet is given as an input, one is generated automatically, but it is not guaranteed the derived tags will be of any use. This generated sample sheet is given as an output in the case the tags need to be manually edited, and the job is resubmitted with it as an input. When a sample sheet is given as an input, the sample sheet output is a copy of the input. See also the description for the input sample sheet . Troubleshooting \u00b6 To troubleshoot a failed run of Mutational Signatures, check the job log for details. Wrong genome build If the \"Building mutation matrix\" step during run fails, it is likely that the selected genome build does not match the input VCF(s). Rerun the job with a matching genome build. Example R: Building mutation matrix from 6 VCFs R: Error in mut_matrix(vcf_list = filtered_vcfs, ref_genome = ref_genome) : R: Error in .Call2(\"solve_user_SEW\", refwidths, start, end, width, translate.negative.coord, : R: solving row 526: 'allow.nonnarrowing' is FALSE and the supplied start (79440206) is > refwidth + 1 Frequently asked questions \u00b6 None yet! If you have any questions not covered here, feel free to reach out on our contact form . References \u00b6 Blokzijl F, Janssen R, van Boxtel R, Cuppen E (2018). \"MutationalPatterns: comprehensive genome-wide analysis of mutational processes.\" Genome Medicine . doi: 10.1186/s13073-018-0539-0 . PMID: 29695279 . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Mutational Signatures"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#overview","text":"Mutational Signatures finds and quantifies COSMIC mutational signatures across samples. This is done by finding the optimal non-negative linear combination of mutation signatures to reconstruct a mutation matrix. It builds the initial mutation matrix from multiple single-sample VCFs and, by default, fits it to mutational signatures from COSMIC . Mutational Signatures employs MutationalPatterns ( Blokzijl, et al. (2018) ) to achieve this. Mutational Signatures supports both hg19 (GRCh37) and hg38 (GRCh38).","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#inputs","text":"Name Type Description Example VCF(s) Array of files List of VCF inputs. Can be single-sample or multi-sample and uncompressed or gzipped. [ *.vcf , *.vcf.gz ] Sample sheet File Tab-delimited file (no headers) with sample ID and tag pairs [optional] *.txt Genome build String Genome build used as reference. Can be either \"GRCh37\" or \"GRCh38\". [default: \"GRCh38\"] GRCh38 Minimum mutation burden Integer Minimum number of somatic SNVs a sample must have to be considered for analysis [default: 9] 15 Minimum signature contribution Integer Minimum number of mutations attributable to a single signature [default: 9] 100 Output prefix String Prefix to append to output filenames [optional] mtsg Disabled VCF column Integer VCF column (starting from sample names, zero-based) to ignore when reading VCFS [optional] 1","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#input-configuration","text":"Mutational Signatures only requires VCFs as inputs. This can be a single multi-sample VCF, multiple single-sample VCFs, or a combination of both. All other inputs are optional.","title":"Input configuration"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#outputs","text":"Name Type Description Raw signatures File Tab-delimited file of the raw results with sample contributions for each signature Signatures visualization File HTML file for interactive plotting Sample sheet File Tab-delimited file (no headers) with sample ID and tag pairs","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#workflow-steps","text":"Mutational Signatures runs four steps using subcommands of mtsg . Split VCFs (single or multi-sample) to multiple single-sample VCFs. If not given, generate a sample sheet from the directory of single-sample VCFs. Build a mutation matrix and reconstruct/fit it using COSMIC mutation signatures. Create a visualization file using the fitted signatures.","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the Mutational Signatures workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#uploading-input-files","text":"Mutational Signatures requires at least one VCF and an optional sample sheet to be uploaded. Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input Files"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress.","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#interpreting-results","text":"Upon a successful run of Mutational Signatures, three files are saved to the results directory: raw signature contributions, a visualization file, and a sample sheet.","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#raw-signatures","text":"Raw signatures is a tab-delimited file of the raw results with sample contributions for each signature. Column 1 is the sample name, columns 2-(N-1) are the COSMIC signatures contribution counts, and column N is the group tag, where N is the total number of columns. The number of columns is variable since if the signature has no contributions for all samples, it is completely omitted. Note that the last column tissue is a misnomer. It aligns to the arbitrary tag given in the sample sheet .","title":"Raw signatures"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#signatures-visualization","text":"Signatures visualization is an HTML file that can be used for interactive plotting. When opened in a web browser, a set of controls allows plotting various stacked bar charts: total contributions by signature, total contributions by tag, and total contributions by sample per tag. The total contributions can be stacked as absolute values or as a percentage of the total.","title":"Signatures visualization"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#sample-sheet","text":"When no sample sheet is given as an input, one is generated automatically, but it is not guaranteed the derived tags will be of any use. This generated sample sheet is given as an output in the case the tags need to be manually edited, and the job is resubmitted with it as an input. When a sample sheet is given as an input, the sample sheet output is a copy of the input. See also the description for the input sample sheet .","title":"Sample Sheet"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#troubleshooting","text":"To troubleshoot a failed run of Mutational Signatures, check the job log for details.","title":"Troubleshooting"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#frequently-asked-questions","text":"None yet! If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#references","text":"Blokzijl F, Janssen R, van Boxtel R, Cuppen E (2018). \"MutationalPatterns: comprehensive genome-wide analysis of mutational processes.\" Genome Medicine . doi: 10.1186/s13073-018-0539-0 . PMID: 29695279 .","title":"References"},{"location":"guides/genomics-platform/analyzing-data/mutational-signatures/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/","text":"Authors Ti-Cheng Chang Publication The Neoepitope Landscape in Pediatric Cancers. Genome Medicine. 2017. 9.1: 78 . Technical Support Contact Us Overview \u00b6 Cancers are caused by somatically acquired alterations including single nucleotide variations (SNVs), small insertion/deletions (indels), translocations, and other types of rearrangements. The genes affected by these mutations may produce altered proteins, some of which may lead to the emergence of tumor-specific immunogenic epitopes. We developed an analytical workflow for identification of putative neoepitopes based on somatic missense mutations and gene fusions using whole genome sequencing data. The workflow has been used to characterize neoepitope landscape of 23 subtypes of pediatric cancer in the Pediatric Cancer Genome Project 1 . Inputs \u00b6 Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500 Input file configuration \u00b6 Users need to provide a mutation file for SNV or gene fusion. The format of the mutation file is shown in the following example. The file can be prepared in Excel and saved as a tab-delimited text file to use as input. The HLA alleles for testing will be derived from the HLA typing module using the workflow. The peptide size and affinity cutoff can be modified by users. Mutation file format GeneName Sample Chr Postion_hg19 Class AAChange mRNA_acc ReferenceAllele MutantAllele Gene1 SampleA chr10 106150600 missense R663H NM_00101 A T Gene2 SampleA chr2 32330151 missense N329N NM_00102 T G Notes on preparing the above file The chromosome requires a 'chr' prefix. The position requires a suffix of HG19/HG38 to indicate the human genome assembly version. Only the missense mutations/gene fusion is supported currently and the other types of mutations will not be processed. Mutation file example Outputs \u00b6 Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format Workflow Steps \u00b6 HLA Typing Algorithm \u00b6 The HLA typing algorithm is used to predict the HLA class I alleles. Users can either provide FastQ (paired or single end reads) or a BAM file as input. When using a BAM file as input, the reads surrounding the HLA loci and unmapped reads will be extracted. The reads will be fed into Optitype for HLA typing. The default settings for Optitype are used. The output of the HLA type can be combined with the our epitope detection algorithm to perform affinity prediction of neoepitopes. If you use FastQ files as input: The input FastQs will be aligned against the Optitype HLA reference sequences using razers3 (see https://github.com/FRED-2/OptiType ). The fished FastQs will be used for HLA typing using Opitype. If you use BAM files as input: The reads falling within the HLA loci and their paralogous loci will be extracted. The reads unmapped to the human genome will be extracted. The reads from step 1 and 2 will be combined and deduplicated (in FastQ format). The input FastQs will be aligned against the Optitype HLA reference sequences using razers3 (see https://github.com/FRED-2/OptiType ). The fished FastQs will be used for HLA typing using Opitype. Epitope Prediction Algorithm \u00b6 The epitope prediction algorithm first extracts peptides covering an array of tiling peptides (size defined by users) overlapping each missense mutation or gene fusion. Fusion junctions can be identified using RNA-Seq by fusion detection tools (Li et. al, unpublished). NetMHCcons 3 is subsequently used to predict affinities of the peptide array for each HLA receptor in each sample. The neoepitope with affinity lower than the threshold will be highlighted in output file (default 500 nM). Below is an outline of internal steps the algorithm performs in order to generate the final report. Check the version of the genomic position of the input SNV/fusion file. Lift over the genomic coordinations if the reference genomic position is not HG19. Currently, the internal genome annotation was based on HG19 and the genome coordinates of the mutation files will be adjusted to HG19 for peptide extraction. Extract the peptide flanking the mutations. Run NetMHCcons to obtain the affinity prediction of the peptides. Produce the affinity report of each peptide. Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the NeoepitopePred workflow page here . Uploading Input files \u00b6 NeoepitopePred takes the following files as input : A pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be generated from whole genome sequencing, whole exome sequencing, or RNA-Seq. A file describing the mutations in a sample. Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Caution This pipeline assumes HG19 coordinates in the mutation file. If the coordinates are based on HG38, the coordinates will lifted over to HG19 to perform epitope affinity prediction. Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 HLA typing The output of this app contain the prediction of the HLA class I alleles from OptiType. A folder stamped with the time will present in the output folder (optitype), which contains the raw output. The file contains the predicted HLA alleles of the sample. Neoepitope prediction The output contains one summary HTML, one folder with raw output, and one folder with outputs in Excel formats: Epitope_affinity_prediction.html (shown below): This file provides a summary of the epitope prediction that can be visualized directly from web browser. The peptides with affinity lower than user-defined cutoff will be highlighted in green in the webpage. Raw_output (shown below): this folder contains the raw output of the affinity prediction. There will two major types files present here: affinity.out and flanking.seq. affinity.out : these files are the prediction results from the netMHCcons for each peptide. The following columns will be shown in the output: Column Description Gene name the name of the genes Sample the name of the samples Chromosome (chr) the chromosome location of the variation Position the chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19. Class class of the variation Reference allele reference allele at the position Mutant allele mutated allele at the position mRNA_acc NCBI accession number of the mRNA Allele HLA allele tested Peptide the neoepitope sequences tested Gene_variant the gene and variant residues 1-log50k Prediction score from netMHCcons nM Affinity as IC50 values in nM %Rank % Rank of prediction score to a set of 200.000 random natural 9mer peptides HLAtype All of the hla alleles predicted in the specific sample flanking.seq : these files contain the sequences used for the prediction. XLSX : this folder contains the raw output of the affinity prediction as described above in Excel files. The files can be downloaded and opened with Excel for downstream filtering and analyses. Frequently asked questions \u00b6 None yet! If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data Downing JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome Project. Nature genetics. 2012;44(6):619-622. \u21a9 Szolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O: OptiType: precision HLA typing from next-generation sequencing data. Bioinformatics 2014, 30:3310-3316. \u21a9 Karosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a consensus method for the major histocompatibility complex class I predictions. Immunogenetics 2012, 64:177-186. \u21a9","title":"NeoepitopePred"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#overview","text":"Cancers are caused by somatically acquired alterations including single nucleotide variations (SNVs), small insertion/deletions (indels), translocations, and other types of rearrangements. The genes affected by these mutations may produce altered proteins, some of which may lead to the emergence of tumor-specific immunogenic epitopes. We developed an analytical workflow for identification of putative neoepitopes based on somatic missense mutations and gene fusions using whole genome sequencing data. The workflow has been used to characterize neoepitope landscape of 23 subtypes of pediatric cancer in the Pediatric Cancer Genome Project 1 .","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#inputs","text":"Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#input-file-configuration","text":"Users need to provide a mutation file for SNV or gene fusion. The format of the mutation file is shown in the following example. The file can be prepared in Excel and saved as a tab-delimited text file to use as input. The HLA alleles for testing will be derived from the HLA typing module using the workflow. The peptide size and affinity cutoff can be modified by users. Mutation file format GeneName Sample Chr Postion_hg19 Class AAChange mRNA_acc ReferenceAllele MutantAllele Gene1 SampleA chr10 106150600 missense R663H NM_00101 A T Gene2 SampleA chr2 32330151 missense N329N NM_00102 T G Notes on preparing the above file The chromosome requires a 'chr' prefix. The position requires a suffix of HG19/HG38 to indicate the human genome assembly version. Only the missense mutations/gene fusion is supported currently and the other types of mutations will not be processed. Mutation file example","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#outputs","text":"Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#workflow-steps","text":"","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#hla-typing-algorithm","text":"The HLA typing algorithm is used to predict the HLA class I alleles. Users can either provide FastQ (paired or single end reads) or a BAM file as input. When using a BAM file as input, the reads surrounding the HLA loci and unmapped reads will be extracted. The reads will be fed into Optitype for HLA typing. The default settings for Optitype are used. The output of the HLA type can be combined with the our epitope detection algorithm to perform affinity prediction of neoepitopes. If you use FastQ files as input: The input FastQs will be aligned against the Optitype HLA reference sequences using razers3 (see https://github.com/FRED-2/OptiType ). The fished FastQs will be used for HLA typing using Opitype. If you use BAM files as input: The reads falling within the HLA loci and their paralogous loci will be extracted. The reads unmapped to the human genome will be extracted. The reads from step 1 and 2 will be combined and deduplicated (in FastQ format). The input FastQs will be aligned against the Optitype HLA reference sequences using razers3 (see https://github.com/FRED-2/OptiType ). The fished FastQs will be used for HLA typing using Opitype.","title":"HLA Typing Algorithm"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#epitope-prediction-algorithm","text":"The epitope prediction algorithm first extracts peptides covering an array of tiling peptides (size defined by users) overlapping each missense mutation or gene fusion. Fusion junctions can be identified using RNA-Seq by fusion detection tools (Li et. al, unpublished). NetMHCcons 3 is subsequently used to predict affinities of the peptide array for each HLA receptor in each sample. The neoepitope with affinity lower than the threshold will be highlighted in output file (default 500 nM). Below is an outline of internal steps the algorithm performs in order to generate the final report. Check the version of the genomic position of the input SNV/fusion file. Lift over the genomic coordinations if the reference genomic position is not HG19. Currently, the internal genome annotation was based on HG19 and the genome coordinates of the mutation files will be adjusted to HG19 for peptide extraction. Extract the peptide flanking the mutations. Run NetMHCcons to obtain the affinity prediction of the peptides. Produce the affinity report of each peptide.","title":"Epitope Prediction Algorithm"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the NeoepitopePred workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#uploading-input-files","text":"NeoepitopePred takes the following files as input : A pair of Gzipped FastQ files or an HG19/HG38 aligned BAM file. These can be generated from whole genome sequencing, whole exome sequencing, or RNA-Seq. A file describing the mutations in a sample. Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input files"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Caution This pipeline assumes HG19 coordinates in the mutation file. If the coordinates are based on HG38, the coordinates will lifted over to HG19 to perform epitope affinity prediction.","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#interpreting-results","text":"HLA typing The output of this app contain the prediction of the HLA class I alleles from OptiType. A folder stamped with the time will present in the output folder (optitype), which contains the raw output. The file contains the predicted HLA alleles of the sample. Neoepitope prediction The output contains one summary HTML, one folder with raw output, and one folder with outputs in Excel formats: Epitope_affinity_prediction.html (shown below): This file provides a summary of the epitope prediction that can be visualized directly from web browser. The peptides with affinity lower than user-defined cutoff will be highlighted in green in the webpage. Raw_output (shown below): this folder contains the raw output of the affinity prediction. There will two major types files present here: affinity.out and flanking.seq. affinity.out : these files are the prediction results from the netMHCcons for each peptide. The following columns will be shown in the output: Column Description Gene name the name of the genes Sample the name of the samples Chromosome (chr) the chromosome location of the variation Position the chromosomal position of the variation. Currently, the position will be lifted over to HG19 to ensure correct translation of peptid sequences based on the internal annotation database of the pipeline. Therefore, the position will be labeled as HG19. Class class of the variation Reference allele reference allele at the position Mutant allele mutated allele at the position mRNA_acc NCBI accession number of the mRNA Allele HLA allele tested Peptide the neoepitope sequences tested Gene_variant the gene and variant residues 1-log50k Prediction score from netMHCcons nM Affinity as IC50 values in nM %Rank % Rank of prediction score to a set of 200.000 random natural 9mer peptides HLAtype All of the hla alleles predicted in the specific sample flanking.seq : these files contain the sequences used for the prediction. XLSX : this folder contains the raw output of the affinity prediction as described above in Excel files. The files can be downloaded and opened with Excel for downstream filtering and analyses.","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#frequently-asked-questions","text":"None yet! If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/neoepitope/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data Downing JR, Wilson RK, Zhang J, et al. The Pediatric Cancer Genome Project. Nature genetics. 2012;44(6):619-622. \u21a9 Szolek A, Schubert B, Mohr C, Sturm M, Feldhahn M, Kohlbacher O: OptiType: precision HLA typing from next-generation sequencing data. Bioinformatics 2014, 30:3310-3316. \u21a9 Karosiene E, Lundegaard C, Lund O, Nielsen M: NetMHCcons: a consensus method for the major histocompatibility complex class I predictions. Immunogenetics 2012, 64:177-186. \u21a9","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/","text":"Authors Scott Newman, Clay McLeod, Yongjin Li Publication N/A (not published) Technical Support Contact Us Overview \u00b6 Fusion genes are important for cancer diagnosis, subtype definition and targeted therapy. RNASeq is useful for detecting fusion transcripts; however, computational methods face challenges to identify fusion transcripts due to events such as internal tandem duplication (ITD), multiple genes, low expression, or non-templated insertions. To address some of these challenges, St. Jude Cloud offers \"Rapid RNA-Seq\", an end-to-end clinically validated pipeline that detects gene fusions and ITDs from human RNA-Seq. Inputs \u00b6 The input can be either of the two entries below, based on whether you want to start with FastQ files or a BAM file. Name Description Example Paired FastQ files Gzipped FastQ files generated by human RNA-Seq Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM file Aligned reads file from human RNA-Seq Sample.bam Caution If you provide a BAM file to the pipeline, it must be aligned to GRCh37-lite. Running a BAM aligned to any other reference genome is not supported. Maybe more importantly, we do not check the genome build of the BAM, so errors in computation or the results can occur. If your BAM is not aligned to this genome build, we recommend converting the BAM back to FastQ files using Picard's SamToFastq functionality and using the FastQ version of the pipeline. Outputs \u00b6 The Rapid RNA-Seq pipeline produces the following outputs: Name Description Predicted gene fusions (.txt) File containing putative gene fusions. Coverage file (.bw) bigWig file containing coverage information. Splice junction read counts (.txt) Read counts for the splice junction detected. Interactive fusion visualization Fusion visualization produced by ProteinPaint. Interactive coverage visualization Coverage visualization produced by ProteinPaint. Workflow Steps \u00b6 The raw sequence data is aligned to GRCh37-lite using standard STAR mapping. A coverage bigWig (.bw) file is produced to allow the user to assess sample quality across the genome. Two gene fusion detection algorithms are run in parallel. The Fuzzion (Rice et al. unpublished data) fusion detection algorithm is run to provide high sensitivity for recurrent gene fusions. The RNAPEG (Edmonson et al. unpublished data) splice junction read counting algorithm is run to quantify read counts for splice junctions. These splice junction read counts are then used by Cicero (Li et al. unpublished data) to detect putative gene fusions. Custom visualizations for putative gene fusions and genome coverage are produced by ProteinPaint. Mapping We use the STAR aligner to rapidly map reads to the GRCh37 human reference genome. This step generally takes around one hour to complete assuming approximately 55-75 million paired reads are supplied. Coverage Internally developed scripts calculate the coverage of mapped reads genome wide. The resulting bigWig file can be viewed in ProteinPaint or used for quality control. Splice junction read quantification We use our RNAPEG software to quantify reads spanning known and novel splice junctions. RNAPEG also corrects improper mappings at splice junction boundaries for more accurate definition of novel splice junctions. The resulting junctions file can be viewed along with the coverage bigWig file to gain insights into gene expression and splicing patterns Genome-wide fusion prediction We developed an assembly-based algorithm CICERO (Clipped-reads Extended for RNA Optimization) that is able to extend the read-length spanning fusion junctions for detecting complex fusions. CICERO finds clipped reads and junction spanning reads, assembles them into a contig and maps the contig back to the reference genome. Mapped contigs are then annotated and filtered. Those with potential genic effects including gene fusion, ITD, readthrough or circular RNA are reported in the final_fusions.txt file. An interactive version of this file with predictions sorted by quality can be inspected with the ProteinPaint interactive fusion viewer. An abstract describing CICERO was presented at ASHG, 2014: http://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm Low stringency fusion gene \"Hotpot\" search We have observed that certain fusions such as KIAA1549-BRAF in low-grade glioma have apparently limited read support in the bam file \u2014 either due to low expression or low tumor purity. In these cases, we use a secondary tool, FUZZION, that performs fuzzy matching for known fusion gene junctions for every read in the bam file (both mapped and unmapped). FUZZION can recover even a single low quality read potentially supporting a known fusion gene junction. The FUZZION output is a simple text file with read IDs and sequences supporting a particular gene fusion. The fusion point is indicated with square brackets [] . Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the Rapid RNA-Seq workflow page here . Uploading Input Files \u00b6 Caution This pipeline assumes GRCh37-lite coordinates. If your BAM is not aligned to this genome build, we recommend converting the BAM back to FastQ files using Picard's SamToFastq functionality. The Rapid RNA-Seq pipeline takes as input either a paired set of Gzipped FastQ files or a GRCh37-lite aligned BAM from human RNA-Seq. Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 The complete output file specification is listed in the overview section of this guide. Here, we will discuss each of the different output files in more detail. Predicted gene fusions : The putative gene fusions will be contained in the file [SAMPLE].final_fusions.txt . This file is a tab-delimited file containing many fields for each of the predicted SV. The most important columns are the following. Field Name Description sample Sample name gene* Gene name chr* Chromosome name pos* Genomic Location ort* Strand reads* Supporting reads medal Estimated pathogenicity assessment using St. Jude Medal Ceremony Coverage file : A standard bigWig file used to describe genomic read coverage. Splice junction read counts : A custom file format describing the junction read counts. The following fields are included in the tab-delimited output file. Field Name Description junction Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in .bed output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future. count Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file. type Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection). genes Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table. transcripts List of known transcript IDs matching the junction. qc_flanking Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt). qc_plus Count of supporting reads aligned to the + strand. qc_minus Count of supporting reads aligned to the - strand. qc_perfect_reads Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips). qc_clean_reads Count of supporting reads whose alignments are not perfect but which have a ratio of <= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together. Known issues \u00b6 Adapter contamination This pipeline does not, at present, remove adapter sequences. If the sequencing library is contaminated with adapters, CICERO runtimes can increase exponentially. We recommend running FastQ files through a QC pipeline such as FastQC and trimming adapters with tools such as Trimmomatic if adapters are found. High coverage regions Certain cell types show very high transcription of certain loci, for example, the immunoglobulin heavy chain locus in plasma cells. The presence of very highly covered regions (typically 100,000-1,000,000+ X) has an adverse effect on CICERO runtimes. Presently, we have no good solution to this problem as strategies such as down-sampling may reduce sensitivity over important regions of the genome. Interactive Visualizations Exon vs Intron Nomenclature When a codon is split over a fusion gene junction, the annotation software marks the event as intronic when really, the event should be exonic. We are working to fix this bug. In the mean time, if a fusion is predicted to be in frame but the interactive plot shows \"intronic\", we suggest the user blat the contig shown just below to clarify if the true junction is either in the intron or exon. Frequently asked questions \u00b6 If you have any questions not covered here, feel free to reach out on our contact form . Submit batch jobs on command line \u00b6 See How can I run an analysis workflow on multiple sample files at the same time? Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Rapid RNA-Seq Fusion Detection"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#overview","text":"Fusion genes are important for cancer diagnosis, subtype definition and targeted therapy. RNASeq is useful for detecting fusion transcripts; however, computational methods face challenges to identify fusion transcripts due to events such as internal tandem duplication (ITD), multiple genes, low expression, or non-templated insertions. To address some of these challenges, St. Jude Cloud offers \"Rapid RNA-Seq\", an end-to-end clinically validated pipeline that detects gene fusions and ITDs from human RNA-Seq.","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#inputs","text":"The input can be either of the two entries below, based on whether you want to start with FastQ files or a BAM file. Name Description Example Paired FastQ files Gzipped FastQ files generated by human RNA-Seq Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM file Aligned reads file from human RNA-Seq Sample.bam Caution If you provide a BAM file to the pipeline, it must be aligned to GRCh37-lite. Running a BAM aligned to any other reference genome is not supported. Maybe more importantly, we do not check the genome build of the BAM, so errors in computation or the results can occur. If your BAM is not aligned to this genome build, we recommend converting the BAM back to FastQ files using Picard's SamToFastq functionality and using the FastQ version of the pipeline.","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#outputs","text":"The Rapid RNA-Seq pipeline produces the following outputs: Name Description Predicted gene fusions (.txt) File containing putative gene fusions. Coverage file (.bw) bigWig file containing coverage information. Splice junction read counts (.txt) Read counts for the splice junction detected. Interactive fusion visualization Fusion visualization produced by ProteinPaint. Interactive coverage visualization Coverage visualization produced by ProteinPaint.","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#workflow-steps","text":"The raw sequence data is aligned to GRCh37-lite using standard STAR mapping. A coverage bigWig (.bw) file is produced to allow the user to assess sample quality across the genome. Two gene fusion detection algorithms are run in parallel. The Fuzzion (Rice et al. unpublished data) fusion detection algorithm is run to provide high sensitivity for recurrent gene fusions. The RNAPEG (Edmonson et al. unpublished data) splice junction read counting algorithm is run to quantify read counts for splice junctions. These splice junction read counts are then used by Cicero (Li et al. unpublished data) to detect putative gene fusions. Custom visualizations for putative gene fusions and genome coverage are produced by ProteinPaint. Mapping We use the STAR aligner to rapidly map reads to the GRCh37 human reference genome. This step generally takes around one hour to complete assuming approximately 55-75 million paired reads are supplied. Coverage Internally developed scripts calculate the coverage of mapped reads genome wide. The resulting bigWig file can be viewed in ProteinPaint or used for quality control. Splice junction read quantification We use our RNAPEG software to quantify reads spanning known and novel splice junctions. RNAPEG also corrects improper mappings at splice junction boundaries for more accurate definition of novel splice junctions. The resulting junctions file can be viewed along with the coverage bigWig file to gain insights into gene expression and splicing patterns Genome-wide fusion prediction We developed an assembly-based algorithm CICERO (Clipped-reads Extended for RNA Optimization) that is able to extend the read-length spanning fusion junctions for detecting complex fusions. CICERO finds clipped reads and junction spanning reads, assembles them into a contig and maps the contig back to the reference genome. Mapped contigs are then annotated and filtered. Those with potential genic effects including gene fusion, ITD, readthrough or circular RNA are reported in the final_fusions.txt file. An interactive version of this file with predictions sorted by quality can be inspected with the ProteinPaint interactive fusion viewer. An abstract describing CICERO was presented at ASHG, 2014: http://www.ashg.org/2014meeting/abstracts/fulltext/f140120024.htm Low stringency fusion gene \"Hotpot\" search We have observed that certain fusions such as KIAA1549-BRAF in low-grade glioma have apparently limited read support in the bam file \u2014 either due to low expression or low tumor purity. In these cases, we use a secondary tool, FUZZION, that performs fuzzy matching for known fusion gene junctions for every read in the bam file (both mapped and unmapped). FUZZION can recover even a single low quality read potentially supporting a known fusion gene junction. The FUZZION output is a simple text file with read IDs and sequences supporting a particular gene fusion. The fusion point is indicated with square brackets [] .","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the Rapid RNA-Seq workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#uploading-input-files","text":"Caution This pipeline assumes GRCh37-lite coordinates. If your BAM is not aligned to this genome build, we recommend converting the BAM back to FastQ files using Picard's SamToFastq functionality. The Rapid RNA-Seq pipeline takes as input either a paired set of Gzipped FastQ files or a GRCh37-lite aligned BAM from human RNA-Seq. Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input Files"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress.","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#interpreting-results","text":"The complete output file specification is listed in the overview section of this guide. Here, we will discuss each of the different output files in more detail. Predicted gene fusions : The putative gene fusions will be contained in the file [SAMPLE].final_fusions.txt . This file is a tab-delimited file containing many fields for each of the predicted SV. The most important columns are the following. Field Name Description sample Sample name gene* Gene name chr* Chromosome name pos* Genomic Location ort* Strand reads* Supporting reads medal Estimated pathogenicity assessment using St. Jude Medal Ceremony Coverage file : A standard bigWig file used to describe genomic read coverage. Splice junction read counts : A custom file format describing the junction read counts. The following fields are included in the tab-delimited output file. Field Name Description junction Splice junction in the TCGA format \"chrX:start:+,chrX:end,+\". \"start\" and \"end\" are the 1-based position of the last mapped nucleotide before the skip and the first mapped nucleotide after the skip (i.e. the last base of the previous exon and the first base of the next exon). Note that in .bed output these coordinates will be different, see the .bed output section below. The \"+\" is currently hardcoded, though this may change in the future. count Raw count of reads supporting the junction. During correction counts for ambiguous junctions can be combined, though obviously these additional reads will not be visible in the raw BAM file. type Either \"known\" (matching a reference junction) or \"novel\" (not observed in the reference junction collection). genes Gene symbols from the junction calling process. These still need work in the raw junction calling process, it's recommended to use the \"annotated\" output files instead which assign matching HUGO gene symbols based on the UCSC refGene table. transcripts List of known transcript IDs matching the junction. qc_flanking Count of supporting reads passing flanking sequence checks (junctions observed adjacent to read ends require 18+ nt of flanking sequence, otherwise 10+ nt). qc_plus Count of supporting reads aligned to the + strand. qc_minus Count of supporting reads aligned to the - strand. qc_perfect_reads Count of supporting reads with perfect alignments (no reference mismatches of quality 15+, indels, or soft clips). qc_clean_reads Count of supporting reads whose alignments are not perfect but which have a ratio of <= 5% of reference mismatches of quality 15+, indels, or soft clips relative to the count of aligned bases on both the left and right flanking sequence. Note: qc_clean_reads does NOT include qc_perfect_reads: to get a count of \"perfect plus pretty good\" reads the two values must be added together.","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#known-issues","text":"Adapter contamination This pipeline does not, at present, remove adapter sequences. If the sequencing library is contaminated with adapters, CICERO runtimes can increase exponentially. We recommend running FastQ files through a QC pipeline such as FastQC and trimming adapters with tools such as Trimmomatic if adapters are found. High coverage regions Certain cell types show very high transcription of certain loci, for example, the immunoglobulin heavy chain locus in plasma cells. The presence of very highly covered regions (typically 100,000-1,000,000+ X) has an adverse effect on CICERO runtimes. Presently, we have no good solution to this problem as strategies such as down-sampling may reduce sensitivity over important regions of the genome. Interactive Visualizations Exon vs Intron Nomenclature When a codon is split over a fusion gene junction, the annotation software marks the event as intronic when really, the event should be exonic. We are working to fix this bug. In the mean time, if a fusion is predicted to be in frame but the interactive plot shows \"intronic\", we suggest the user blat the contig shown just below to clarify if the true junction is either in the intron or exon.","title":"Known issues"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#frequently-asked-questions","text":"If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#submit-batch-jobs-on-command-line","text":"See How can I run an analysis workflow on multiple sample files at the same time?","title":"Submit batch jobs on command line"},{"location":"guides/genomics-platform/analyzing-data/rapid-rnaseq/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/","text":"RNA INDEL Authors John Doe Publication N/A (not published) Technical Support Contact Us Overview \u00b6 abstract-type description of tool Inputs \u00b6 table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500 Input file configuration \u00b6 if needed Outputs \u00b6 table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format Workflow Steps \u00b6 description of algorithm(s) or workflow steps Additional Info \u00b6 description of any additional information that the user might need to know/do before running the workflow Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here . Uploading Input files \u00b6 note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 detailed explanations with helpful screenshots or gifs Frequently asked questions \u00b6 faqs If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"RNA INDEL"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#overview","text":"abstract-type description of tool","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#inputs","text":"table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#input-file-configuration","text":"if needed","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#outputs","text":"table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#workflow-steps","text":"description of algorithm(s) or workflow steps","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#additional-info","text":"description of any additional information that the user might need to know/do before running the workflow","title":"Additional Info"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#uploading-input-files","text":"note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input files"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#interpreting-results","text":"detailed explanations with helpful screenshots or gifs","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#frequently-asked-questions","text":"faqs If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/rna-indel/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/","text":"In this guide, we will explain generally how to run, from end-to-end, any of our in house analysis workflows from within the DNAnexus cloud ecosystem. The DNAnexus genomic ecosystem is the backbone for the computation and storage in St. Jude Cloud. If you'd like, you can read an introduction to the DNAnexus ecosystem here . If you haven't already, follow this guide to request access to St. Jude data in this secure cloud ecosystem. Getting started \u00b6 To get started with any St. Jude Cloud workflow, first navigate to the appropriate workflow page. Below is a complete list of the workflows we offer along with links to their corresponding tool page and documentation page. NeoepitopePred [ tool page ] [ documentation ] ChIP-Seq Peak Calling [ tool page ] [ documentation ] Rapid RNA-Seq Fusion Detection [ tool page ] [ documentation ] WARDEN Differential Expression Analysis [ tool page ] [ documentation ] Mutational Signatures [ tool page ] [ documentation ] SequencErr [ tool page ] [ documentation ] From the appropriate workflow page, click the \"Start\" button in the left hand pane. This creates a new DNAnexus cloud workspace (with the same name as the workflow) and imports the workflow. With subsequent runs, in place of the \"Start\" button will be two buttons \"Launch Tool\" and \"View Results\", meaning a cloud workspace with the workflow has already been created for you. In this case, you're good! You can move on to the next section. Note If you have not yet logged in, in place of the \"Start\" button will be a button the says \"Log In\". If you see this, simply login and try again. For a guide to creating an account go here . If you are still unable to start the workflow, contact us . Uploading Files \u00b6 Now that a DNAnexus cloud workspace has been created, you will be able to upload input files to that workspace. The specific documentation for each workflow will detail what input files you will need to upload. You can upload these files using the data transfer application or by uploading them through the command line . Both of the guides linked here will contain more details on how to upload data using that method, so we defer to those guides here. Tip If you plan to upload data through the St. Jude Cloud Data Transfer application (recommended), you can click the \"Upload Data\" button the appears in the left panel after you click \"Start\". If you have not already downloaded the app, do so by clicking \"Download app\". Once you have the app, you can click \"Open app\" to open the app with the workflow's cloud workspace already opened and ready to drag-and-drop files into it! For more information, check out the data transfer application guide. Running the Workflow \u00b6 Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the workflow's landing page. A dropdown will present any presets required for running the workflow. For example, some workflows ask that you select whether you wish to start with FastQ files or a BAM file. Launching the workflow will redirect you to you workspace in DNAnexus. The gif below shows an example with the Rapid RNA-Seq workflow Selecting Parameters \u00b6 Some workflows allow you to specify or customize one or more run parameters. Many parameters will be set to a default value. To see all parameter options available, click the gear cog next to the substep titled with the workflow name. For a full list of the parameters and their descriptions, see Inputs table on the documentation page for the workflow that you are running. Below is an example showing how to customize parameters for the Neoepitope Prediction workflow. Hooking up Inputs \u00b6 Next, you'll need to hook up the input files you uploaded in the upload files section . In the example below, we are running the Rapid RNA-Seq workflow using the FastQ version of the pipeline. The example gif shows that you hook up the inputs by clicking on the Fastq/R1 and Fastq/R2 slots and selecting the respective input files. This process is similar for all workflows. Starting the Workflow \u00b6 Once your input files are hooked up, you should be able to start the workflow by clicking the Run as Analysis... button in the top right hand corner of the workflow dialog. See the example below using the Rapid RNA-Seq workflow. Tip If you cannot click this button, please ensure that all of the inputs are correctly hooked up. If you're still have trouble, please contact us and include a screenshot of the workflow screen above. Monitoring Run Progress \u00b6 Once you have started one or more workflow runs, you can safely close your browser and come back later to check the status of the jobs. To do this, navigate to the landing page of the workflow that you want to check. Next, click \"View Results\" then select the \"View Running Jobs\" option. You will be redirected to the job monitoring page in your DNAnexus workspace. Each job you kicked off gets one row in this table. See the two examples below for the Rapid RNA-Seq workflow. You can click the \"+\" on any of the runs to check the status of individual steps of the workflow. Other information, such as time, cost of individual steps in the workflow, and even viewing the job logs can accessed by clicking around the sub-items. Tip Refer to the DNAnexus Monitoring Executions Documentation for advanced capabilities for monitoring jobs. Accessing Results \u00b6 Custom Visualizations \u00b6 Most workflows in St. Jude Cloud produce one or more visualizations that helps you to understand the raw results. To access the visualization(s), navigate to the landing page of the workflow that you want to check. Next, click \"View Results\" then select the \"Visualize Results\" option. You should now see a list of visualization files. Click on a visualization name to explore. Below is a brief description of two of our custom visualizations. ProteinPaint BigWig Viewer \u00b6 The ProteinPaint interactive coverage viewer is used to visualize any bigWig file. You can follow these steps to get an understanding of how it works. Open up the custom viewer file output by your pipeline. The name of this file will vary, so consult the specific pipeline guide to know where to find it. Click \"Launch\" in the bottom right corner to launch the custom viewer. Once the page has loaded, you will be able to see the bigWig viewer. You can navigate around the genome by gene or genomic location. Alongside the coverage track is the GENCODE gene reference. ProteinPaint Fusion Viewer \u00b6 The ProteinPaint interactive fusion viewer is used to visualize putative fusions called by Rapid RNA-Seq. You can follow these steps to get an understanding of how it works. Open up the custom viewer file output by your pipeline. The name of this file will vary, so consult the specific pipeline guide to know where to find it. Click \"Launch\" in the bottom right corner to launch the custom viewer. Once the page has finished loading, you will be presented with a summary of all of the fusions produced by the pipeline. Each bullet point is a separate category for the structural variants, with the more interesting fusions at the top. Click one of the categories to view the fusions in that category. You can see all of the fusions in that category are now listed on the screen. Hover over one of the fusions to see the detailed view. The popup contains a large amount of information that might be interesting to you based on your use case, such as the transcript and other metrics like read counts, quality metrics, and recurrence. Raw Results Files \u00b6 If additionally, you would like to view raw output files, you may do so by following the directions below. To access the raw output file, navigate to the landing page of the workflow that you want to check. Next, click \"View Results\" then select the \"View Results Files\" option. You should now be in the filesystem view of your workflow's workspace in DNAnexus with access to files that you uploaded as well as results files that are generated. See the example filesystem view below for the Rapid RNA-Seq workflow. This is similar to your the filesystem on your computer, and you can do many common operations such as deleting, renaming, and moving files. How/where the result files are generated are specific to each pipeline. Please refer to your individual workflow's documentation on where the output files are kept. If you have any unanswered questions about how to run one of our in-house workflows, please contact us . Similar Topics \u00b6 Command Line Interaction Working with our Data Overview Downloading/Uploading Data Technical FAQs","title":"Running our Workflows"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#getting-started","text":"To get started with any St. Jude Cloud workflow, first navigate to the appropriate workflow page. Below is a complete list of the workflows we offer along with links to their corresponding tool page and documentation page. NeoepitopePred [ tool page ] [ documentation ] ChIP-Seq Peak Calling [ tool page ] [ documentation ] Rapid RNA-Seq Fusion Detection [ tool page ] [ documentation ] WARDEN Differential Expression Analysis [ tool page ] [ documentation ] Mutational Signatures [ tool page ] [ documentation ] SequencErr [ tool page ] [ documentation ] From the appropriate workflow page, click the \"Start\" button in the left hand pane. This creates a new DNAnexus cloud workspace (with the same name as the workflow) and imports the workflow. With subsequent runs, in place of the \"Start\" button will be two buttons \"Launch Tool\" and \"View Results\", meaning a cloud workspace with the workflow has already been created for you. In this case, you're good! You can move on to the next section. Note If you have not yet logged in, in place of the \"Start\" button will be a button the says \"Log In\". If you see this, simply login and try again. For a guide to creating an account go here . If you are still unable to start the workflow, contact us .","title":"Getting started"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#uploading-files","text":"Now that a DNAnexus cloud workspace has been created, you will be able to upload input files to that workspace. The specific documentation for each workflow will detail what input files you will need to upload. You can upload these files using the data transfer application or by uploading them through the command line . Both of the guides linked here will contain more details on how to upload data using that method, so we defer to those guides here. Tip If you plan to upload data through the St. Jude Cloud Data Transfer application (recommended), you can click the \"Upload Data\" button the appears in the left panel after you click \"Start\". If you have not already downloaded the app, do so by clicking \"Download app\". Once you have the app, you can click \"Open app\" to open the app with the workflow's cloud workspace already opened and ready to drag-and-drop files into it! For more information, check out the data transfer application guide.","title":"Uploading Files"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#running-the-workflow","text":"Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the workflow's landing page. A dropdown will present any presets required for running the workflow. For example, some workflows ask that you select whether you wish to start with FastQ files or a BAM file. Launching the workflow will redirect you to you workspace in DNAnexus. The gif below shows an example with the Rapid RNA-Seq workflow","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#selecting-parameters","text":"Some workflows allow you to specify or customize one or more run parameters. Many parameters will be set to a default value. To see all parameter options available, click the gear cog next to the substep titled with the workflow name. For a full list of the parameters and their descriptions, see Inputs table on the documentation page for the workflow that you are running. Below is an example showing how to customize parameters for the Neoepitope Prediction workflow.","title":"Selecting Parameters"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#hooking-up-inputs","text":"Next, you'll need to hook up the input files you uploaded in the upload files section . In the example below, we are running the Rapid RNA-Seq workflow using the FastQ version of the pipeline. The example gif shows that you hook up the inputs by clicking on the Fastq/R1 and Fastq/R2 slots and selecting the respective input files. This process is similar for all workflows.","title":"Hooking up Inputs"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#starting-the-workflow","text":"Once your input files are hooked up, you should be able to start the workflow by clicking the Run as Analysis... button in the top right hand corner of the workflow dialog. See the example below using the Rapid RNA-Seq workflow. Tip If you cannot click this button, please ensure that all of the inputs are correctly hooked up. If you're still have trouble, please contact us and include a screenshot of the workflow screen above.","title":"Starting the Workflow"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#monitoring-run-progress","text":"Once you have started one or more workflow runs, you can safely close your browser and come back later to check the status of the jobs. To do this, navigate to the landing page of the workflow that you want to check. Next, click \"View Results\" then select the \"View Running Jobs\" option. You will be redirected to the job monitoring page in your DNAnexus workspace. Each job you kicked off gets one row in this table. See the two examples below for the Rapid RNA-Seq workflow. You can click the \"+\" on any of the runs to check the status of individual steps of the workflow. Other information, such as time, cost of individual steps in the workflow, and even viewing the job logs can accessed by clicking around the sub-items. Tip Refer to the DNAnexus Monitoring Executions Documentation for advanced capabilities for monitoring jobs.","title":"Monitoring Run Progress"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#accessing-results","text":"","title":"Accessing Results"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#custom-visualizations","text":"Most workflows in St. Jude Cloud produce one or more visualizations that helps you to understand the raw results. To access the visualization(s), navigate to the landing page of the workflow that you want to check. Next, click \"View Results\" then select the \"Visualize Results\" option. You should now see a list of visualization files. Click on a visualization name to explore. Below is a brief description of two of our custom visualizations.","title":"Custom Visualizations"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#proteinpaint-bigwig-viewer","text":"The ProteinPaint interactive coverage viewer is used to visualize any bigWig file. You can follow these steps to get an understanding of how it works. Open up the custom viewer file output by your pipeline. The name of this file will vary, so consult the specific pipeline guide to know where to find it. Click \"Launch\" in the bottom right corner to launch the custom viewer. Once the page has loaded, you will be able to see the bigWig viewer. You can navigate around the genome by gene or genomic location. Alongside the coverage track is the GENCODE gene reference.","title":"ProteinPaint BigWig Viewer"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#proteinpaint-fusion-viewer","text":"The ProteinPaint interactive fusion viewer is used to visualize putative fusions called by Rapid RNA-Seq. You can follow these steps to get an understanding of how it works. Open up the custom viewer file output by your pipeline. The name of this file will vary, so consult the specific pipeline guide to know where to find it. Click \"Launch\" in the bottom right corner to launch the custom viewer. Once the page has finished loading, you will be presented with a summary of all of the fusions produced by the pipeline. Each bullet point is a separate category for the structural variants, with the more interesting fusions at the top. Click one of the categories to view the fusions in that category. You can see all of the fusions in that category are now listed on the screen. Hover over one of the fusions to see the detailed view. The popup contains a large amount of information that might be interesting to you based on your use case, such as the transcript and other metrics like read counts, quality metrics, and recurrence.","title":"ProteinPaint Fusion Viewer"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#raw-results-files","text":"If additionally, you would like to view raw output files, you may do so by following the directions below. To access the raw output file, navigate to the landing page of the workflow that you want to check. Next, click \"View Results\" then select the \"View Results Files\" option. You should now be in the filesystem view of your workflow's workspace in DNAnexus with access to files that you uploaded as well as results files that are generated. See the example filesystem view below for the Rapid RNA-Seq workflow. This is similar to your the filesystem on your computer, and you can do many common operations such as deleting, renaming, and moving files. How/where the result files are generated are specific to each pipeline. Please refer to your individual workflow's documentation on where the output files are kept. If you have any unanswered questions about how to run one of our in-house workflows, please contact us .","title":"Raw Results Files"},{"location":"guides/genomics-platform/analyzing-data/running-sj-workflows/#similar-topics","text":"Command Line Interaction Working with our Data Overview Downloading/Uploading Data Technical FAQs","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/","text":"Measuring and suppressing sequencer errors in next generation sequencing \u00b6 Authors Eric M Davis, Yu Sun, Yanling Liu, Pandurang Kolekar, Ying Shao, Karol Szlachta, Heather L Mulder, Dongren Ren, Stephen V Rice, Zhaoming Wang, Joy Nakitandwe, Alex Gout, Leslie L Robison, Stanley Pounds, Jefferey Klco, John Easton, Xiaotu Ma* Publication In submission Technical Support Contact Us Overview \u00b6 There is currently no method to precisely measure the errors that occur in the sequencing instrument, which is critical for next generation sequencing applications aimed at discovering the genetic makeup of heterogeneous cellular populations. We propose a novel computational method, SequencErr, to address this challenge by measuring base concordance in overlapping region between forward and reverse reads. Analysis of 3,777 public datasets from 75 research institutions in 18 countries revealed the sequencer error rate to be ~10 per million (pm) and 1.4% of sequencers and 2.7% of flow cells have error rates >100 pm. At the flow cell level, error rates are elevated in the bottom surfaces and >90% of HiSeq and NovaSeq flow cells have at least one outlier error-prone tiles. By sequencing a common DNA library on different sequencers, we demonstrate that sequencers with high error rates have reduced overall sequencing accuracy, and that removal of outlier error-prone tiles improves sequencing accuracy. Our study revealed novel insights into the nature DNA sequencing errors incurred in sequencers. Our method can be used to assess, calibrate, and monitor sequencer accuracy, and to computationally suppress sequencer errors in existing datasets Inputs \u00b6 Name Type Description Example BAM file Input file Binary version of the SAM file format ( *.bam ) Sample_1.bam BAM index file Input file Index file for the BAM file ( *.bai ) Sample_1.bam.bai Outputs \u00b6 Name Format Description PairError file .txt Base concordance/discordance counts Counts file .txt Base call frequencies for each genomic coordinate Running the Analysis \u00b6 Please refer to the following steps to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. You can navigate to the SequencErr application here . Note This tool is intended free-of-charge for non-profit usages. Please contact Dr. Xiaotu Ma for for-profit usages and modifications Log in, Run the app and Select the Project Configure Analysis Settings and Select Input Files Selecting Input Files and Parameters 1. BAM file name [Required] 1 2 3 4 5 6 7 8 A BAM file to process . !!! example \"Notes on preparing the BAM file\" - Read names must have all the 7 fields as described below , - ** \\ [ instrument ] : \\ [ run number ] : \\ [ flowcell ID ] : \\ [ lane ] : \\ [ tile ] : \\ [ x - pos ] : \\ [ y - pos ] ** - Example : ** A041 : 30 : HHTYVDSXX : 1 : 2242 : 28366 : 18897 ** - BAM format details [ here ]( http : //samtools.github.io/hts-specs/SAMv1.pdf) - See [ this page ]( https : //help.basespace.illumina.com/articles/descriptive/fastq-files/) for details of the fields in readname BAM index file name [Required] The BAM index for your BAM file sample id [Required] A unique name or identifier for the sample trimming length [optional] Number of bases to trim off the 5' and 3' of the read. Default: 5 hard quality cutoff [optional] A hard threshold for discarding reads. If the fraction of bases with quality scores falling below this value exceeds fcut, the read will be filtered. Default: 20 don't report base counts [optional] Set this to prevent large count files from being generated Default: true Provide Input Parameters and Run the Analysis Locate Output File(s) after Completion of the Analysis Interpreting results \u00b6 *.pairError.txt file: A text file containing Instrument, Flowcell, Lane and Tile level base concordance/discordance counts *.counts.txt file [optional] : A text file containing the base call frequencies and coverage for each genomic coordinate at specified base quality cut-off Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"SequencErr"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#measuring-and-suppressing-sequencer-errors-in-next-generation-sequencing","text":"Authors Eric M Davis, Yu Sun, Yanling Liu, Pandurang Kolekar, Ying Shao, Karol Szlachta, Heather L Mulder, Dongren Ren, Stephen V Rice, Zhaoming Wang, Joy Nakitandwe, Alex Gout, Leslie L Robison, Stanley Pounds, Jefferey Klco, John Easton, Xiaotu Ma* Publication In submission Technical Support Contact Us","title":"Measuring and suppressing sequencer errors in next generation sequencing"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#overview","text":"There is currently no method to precisely measure the errors that occur in the sequencing instrument, which is critical for next generation sequencing applications aimed at discovering the genetic makeup of heterogeneous cellular populations. We propose a novel computational method, SequencErr, to address this challenge by measuring base concordance in overlapping region between forward and reverse reads. Analysis of 3,777 public datasets from 75 research institutions in 18 countries revealed the sequencer error rate to be ~10 per million (pm) and 1.4% of sequencers and 2.7% of flow cells have error rates >100 pm. At the flow cell level, error rates are elevated in the bottom surfaces and >90% of HiSeq and NovaSeq flow cells have at least one outlier error-prone tiles. By sequencing a common DNA library on different sequencers, we demonstrate that sequencers with high error rates have reduced overall sequencing accuracy, and that removal of outlier error-prone tiles improves sequencing accuracy. Our study revealed novel insights into the nature DNA sequencing errors incurred in sequencers. Our method can be used to assess, calibrate, and monitor sequencer accuracy, and to computationally suppress sequencer errors in existing datasets","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#inputs","text":"Name Type Description Example BAM file Input file Binary version of the SAM file format ( *.bam ) Sample_1.bam BAM index file Input file Index file for the BAM file ( *.bai ) Sample_1.bam.bai","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#outputs","text":"Name Format Description PairError file .txt Base concordance/discordance counts Counts file .txt Base call frequencies for each genomic coordinate","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#running-the-analysis","text":"Please refer to the following steps to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. You can navigate to the SequencErr application here . Note This tool is intended free-of-charge for non-profit usages. Please contact Dr. Xiaotu Ma for for-profit usages and modifications Log in, Run the app and Select the Project Configure Analysis Settings and Select Input Files Selecting Input Files and Parameters 1. BAM file name [Required] 1 2 3 4 5 6 7 8 A BAM file to process . !!! example \"Notes on preparing the BAM file\" - Read names must have all the 7 fields as described below , - ** \\ [ instrument ] : \\ [ run number ] : \\ [ flowcell ID ] : \\ [ lane ] : \\ [ tile ] : \\ [ x - pos ] : \\ [ y - pos ] ** - Example : ** A041 : 30 : HHTYVDSXX : 1 : 2242 : 28366 : 18897 ** - BAM format details [ here ]( http : //samtools.github.io/hts-specs/SAMv1.pdf) - See [ this page ]( https : //help.basespace.illumina.com/articles/descriptive/fastq-files/) for details of the fields in readname BAM index file name [Required] The BAM index for your BAM file sample id [Required] A unique name or identifier for the sample trimming length [optional] Number of bases to trim off the 5' and 3' of the read. Default: 5 hard quality cutoff [optional] A hard threshold for discarding reads. If the fraction of bases with quality scores falling below this value exceeds fcut, the read will be filtered. Default: 20 don't report base counts [optional] Set this to prevent large count files from being generated Default: true Provide Input Parameters and Run the Analysis Locate Output File(s) after Completion of the Analysis","title":"Running the Analysis"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#interpreting-results","text":"*.pairError.txt file: A text file containing Instrument, Flowcell, Lane and Tile level base concordance/discordance counts *.counts.txt file [optional] : A text file containing the base call frequencies and coverage for each genomic coordinate at specified base quality cut-off","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/sequencerr/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/warden/","text":"Authors Lance Palmer Publication N/A (not published) Technical Support Contact Us Overview \u00b6 The WARDEN ( W**orkflow for the **A**nalysis of **R**NA-Seq **D**ifferential **E**xpressio**N ) software uses RNA-Seq sequence files to perform alignment, coverage analysis, gene counts and differential expression analysis. Inputs \u00b6 The WARDEN workflow requires two types of input files and that two parameters be set manually. All other parameters are preset with reasonable defaults. Name Type Description Example FastQ files ( required ) Input file(s) Gzipped FastQ files generated by experiment Sample1.fastq.gz, Sample2.fastq.gz Sample sheet ( required ) Input file Sample sheet generated and uploaded by the user *.txt Input file configuration \u00b6 You'll need to create a sample sheet which describes the relationship between case and control samples, phenotype/condition information, and the comparisons you would like to perform. The sample sheet is a tab-delimited text document that can be created in Microsoft Excel (recommended) or a text editor. Note You will need to upload your sample sheet in a similar manner as your FastQ files, so you can follow the same uploading instructions to achieve this. Prepare using Microsoft Excel \u00b6 Tip Download the file_download sample excel spreadsheet as a starting point! The final product for the excel spreadsheet will look like the screenshot below. If you create the sample sheet from scratch, please ensure the the columns are exactly in this order. Sample rows Each row in the spreadsheet (except for the last row, which we will talk about in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below: Guidelines The sample name should be unique and should only contain letters, numbers and underscores. The condition/phenotype column associates similar samples together. The values should contain only letters, numbers and underscores. ReadFile1 should contain forward reads (e.g. *.R1.fastq.gz or *_1.fastq.gz ). ReadFile2 will contain reads in reverse orientation to ReadFile2 (e.g. *.R2.fastq.gz or *_2.fastq.gz ). For single end reads a single dash ('-') should be entered in the ReadFile2 column. Comparison row The last line in the sample sheet is called the \"comparison row\". This line specifies the comparisons to be done between conditions/phenotypes. All pairwise combinations of the values in the \"Phenotype\" column can be analyzed. To specify the comparisons, on a separate line, include #comparisons= followed be a comma delimited list of two conditions separated by a dash. Example The following lines are all valid examples. #comparisons=KO-WT #comparisons=Condition1-Control,Condition2-Control #comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1 Note If a comparison has at least 3 samples for each condition/phenotype, VOOM/LIMMA will be run. A simple differential comparison will be run on all samples. Finalizing the sample sheet To finalize the sample sheet, save the Microsoft Excel file with whatever name you like. Save the file as an Excel Workbook with the .xlsx extension. Prepare using a text editor \u00b6 Tip Download the file_download sample text file as a starting point! Creating a sample sheet with a text editor is an option for advanced users. The process of creating a sample sheet with a text editor is the same as creating one with Microsoft Excel, with the small difference that you must manually create your columns using the tab character. Save the file with a .txt extension. Outputs \u00b6 Name Description FastQC Report Quality control analysis by FastQC. Aligned BAM Aligned BAM files from STAR mapping. Splice junctions Splice junction information from STAR mapping. Coverage files bigWig ( .bw ) and BED ( .bed ) files detailing coverage. Gene counts Gene counts generated by HT-Seq count. VOOM/LIMMA results Pairwise comparisons of expression data. Requires at least 3 samples vs 3 samples. Simple DE analysis No statistical analysis, requires only a 1 samples vs 1 sample comparison. MA/Volcano plots Both of the above produce tabular outputs, MA plots and volcano plots. Workflow Steps \u00b6 FastQ files generated by RNA-Seq are mapped to a reference genome using the STAR. HT-Seq count is used to assign mapped reads to genes. Differential expression analysis is performed using VOOM normalization of counts and LIMMA analysis. Coverage plots of mapped reads are generated as interactive visualizations. Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the WARDEN Differential Expression Analysis workflow page here . Uploading Input Files \u00b6 The WARDEN Differential Expression analysis pipeline takes Gzipped FastQ files generated by an RNA-Seq experiment as input . Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Note The WARDEN tool operation is slightly different than the other pipelines because it accepts a variable number of samples. First , you will run a \"bootstrapping\" step that creates a custom executable for your analysis. Second , you will need to manually execute the generated workflow from the first step. This allows us to take advantage of many nice features, like check-pointing and cost reduction. Don't worry, we'll show you how to do this step by step below. Hooking up Inputs \u00b6 You'll need to hook up the FastQ files and sample sheet you uploaded in the upload data section . Click the FASTQ_FILES input field and select all FastQ files. Next, click the sampleList input field and select the corresponding samplesheet. Selecting Parameters \u00b6 We now need to configure the parameters for the pipeline, such as reference genome and sequencing method. You can access all of the available parameters by clicking on the WARDEN WORKFLOW GENERATOR substep. For the general workflow instructions refer here Parameter setup steps In the Output Folder field, select a folder to output to. You can structure your experiments however you like (e.g. /My_Outputs ) In the analysisName field, enter a prefix for all of the output files. This can be any value you want to use to remember this run. Be sure to use underscores instead of spaces here! Select the sequenceStandedness from the drop down menu. This information can be determined from the sequencing or source of the data. If you don't know what to put here, select \"no\". Select the Genome pulldown menu. Choose the appropriate box. The LIMMA parameters can be left alone for most analyses. If you are an advanced LIMMA user, you can change the various settings exposed below the required parameters. When all parameters have been set, press the save button. Starting the workflow \u00b6 Once your input files are hooked up and your parameters are set, you should be able to start the workflow by clicking the \"Run as Analysis...\" button in the top right hand corner of the workflow dialog. For the general workflow instructions refer here . The tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\". When the custom workflow has finished generating, the word 'Done' will appear in green in the status column. This indicates that the bootstrapping step has completed successfully. Custom Workflow Process \u00b6 Wait for the workflow generator to finish. Click on the WARDEN name in the name column. You will now be on a page specific to the running of the workflow. On the left side, you will see the inputs you selected for the workflow generator. On the right side are the output files (including the generated workflow). Select the generated workflow as shown in the picture below. You will now be within the output folder you specified earlier. Select the file that begins with 'WARDEN WORKFLOW:' A workflow generated for your data will be presented to you. Select 'Run as analysis' in the upper right. The workflow will initiate, and you will be brought to the 'Monitor' page. (Note to get back to this page, you can select 'Monitor' on one of the menu bars near the top ) Expand the the workflow progress be selecting the '+' sign next to 'In Progress' As parts of the pipeline are run, you will see different tasks in different colors. Green means done, blue is running, orange is waiting, and red means error. When done the status will be shown as 'Done'. Select the Workflow name under Status. You will be brought to a page that show more information about the workflow analysis. Click on the output folder to go to the output. The output folders will now be shown. For a description of the output, please refer to Interpreting Results . Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting Primary Results \u00b6 Alignment statistics \u00b6 Several files should be examined initially to determine the quality of the results. alignmentStatistics.txt shows alignment statistics for all samples. This file is a plain text tab-delimited file that can be opened in Excel or a text editor such as Notepad++. This file contains information on the total reads per sample, the percentage of duplicate reads and the percentage of mapped reads. An example of this file is below. (Within the DNAnexus output directory structure, these files will be in the COMBINED_FLAGSTAT directory.) Multidimensional scaling (MDS) Plot \u00b6 The second set of files to look at are the Multidimensional scaling (MDS) plots using the plotMDS function within LIMMA. Similar to PCA, these graphs will show how similar samples are to each other. There are different sets of MDS plots. For comparisons where there are 3 or more samples per condition, an MDS plot using Voom (Limma) normalized values are generated. An example can be seen below. These files will be labeled mdsPlot.png . For all comparisons, regardless of sample size, and MDS plot will also be generated with Counts per million (CPM) normalized gene counts. These files will be labeled mdsPlot.normCPM.png . (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) MDS plot from just CPM normalized data. ProteinPaint Visualizations \u00b6 Several files on DNAnexus allow the data to be viewed in the Protein Paint viewer. (Note: We plan to have links downloaded in the future to allow the viewing of these files off of DNAnexus.) LIMMA differential expression viewer Within LIMMA/VIEWERS directory (note if no comparisons meet the 3 sample condition, the LIMMA folder will not exist), there will be a viewer file for each valid comparison ( * results. .txt.viewer**). Simply select the file and press 'Launch viewer' in the lower right. A viewer will pop up showing both the MA Plot and Volcano plot. By moving the mouse over a circle, the circle will highlight and the corresponding gene on the other graph will also highlight. Additional information about the gene and its expression values will also be shown. One can also type in multiple gene symbols in the provided text box. By pressing 'Show gene labels' all these genes will show up on the plots. Simple differential expression viewer There will also be a viewer for the simple differential expression analysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all been set to 1, so the volcano plot will not be relevant. bigWig viewer In the BIGWIG_VIEWER directory there will be a bigwigViewer file. Select this file and then 'Launch viewer'. A graph of coverage for the genome should be visible. Interpreting Secondary Results \u00b6 Interactive MA/Volcano Plots \u00b6 In addition to viewing the MA and volcano plots through the visualization tool Differential expression results \u00b6 Other useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition, results.*.txt will be produced. GSEA.input.txt and GSEA.tStat.txt \u00b6 Input files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram. - (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) For plain text results from the simple differential expression analysis, the files will be named simpleDE.*.txt . (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.) Prelabelled MA and volcano plots are provided for the analysis. These files are labeled maPlot.*.png and volcanoPlot.*.png where * is the comparison (e.g. ko_vs_wt) The MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An example MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) The volcano plot shows the Log2Fold change between the conditions on the X-axis, and the -Log10 of the multiple testing corrected P-value on the Y-axis. An MA plot is generated for all comparisons regardless of number of samples. This is the simpleDEPlot.*.png no statistics are shown and genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.) Differential analysis input \u00b6 Inputs and commands are provided for rerunning differential expression analysis on ones own computer. The R commands used for the analysis are found in voomLimma.R . An experienced R user can rerun the analysis with any desired changes. This analysis requires the input countFile.txt which contains counts per genes, the Rparameters.txt file containing input parameters, and a processed sample list file sampleList.txt (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) The input for the simple differential analysis expression will be Rparameters_simple.txt , simpleDE.R , countFile.txt and sampleList.txt . countFile.txt and sampleList.txt are the same files used by the LIMMA analysis. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.) Coverage results \u00b6 bigWig files will be generated for use in genome browsers (such as IGV http://software.broadinstitute.org/software/igv/ ). For each sample, multiple bigWig files will be found. For all types of sequencing strandedness, there will be bigWig files labeled, *.sortedCoverageFile.bed.bw where ' ' is the sample name. For stranded data there will also be*.sortedPosCoverageFile.bed.bw * and *.sortedNegCoverageFile.bed.bw which contains coverage information for the positive and negative strand of the genome. (Within the DNAnexus output directory structure, these files will be in the BIGWIG directory.) Quality Control Results (FastQC) \u00b6 Within the FastQC directory, for each sample and read direction there will be an html file and a zip file ( *.FastQc.html *.FastQc.zip where '*' is the base FastQ name), containing results from FastQTC. For the average user the html file is sufficient. This file can give some basic statistics on the quality of the data. (Within the DNAnexus output directory structure, these files will be in the FastQC directory.) BAM alignment files \u00b6 There are two BAM files generated per sample that contain mapping information for all reads. The first is labeled *.Aligned.sortedByCoord.dup.bam where ' ' is the sample name. The BAM file is sorted by coordinates and has duplicates marked. The second file is*.Aligned.toTranscriptome.out.bam * and contains reads mapped to transcripts. (Within the DNAnexus output directory structure, these files will be in the ALIGN directory.) Chimeric reads and junction files \u00b6 Additional files created by STAR are provided. More information on these files can be found here . *.SJ.out.tab contain splice junction information. Fusion detection files are labeled *.Chimeric.out.bam and *.Chimeric.out.junction . (Within the DNAnexus output directory structure, these files will be in the ALIGN directory.) FPKM and count files (per sample) \u00b6 Per sample files containing FPKM and raw count values for each gene can be found in *.fpkm.txt and *.htseq_counts.txt where '*' is the sample name. Within the DNAnexus output directory structure, these files will be in the HTSEQ directory. Methods Files \u00b6 A more human readable explanation is found in methods.docx . Detailed documentation can be found in methods.txt (Within the DNAnexus output directory structure, these files will be in the METHODS directory.) Auxiliary Files \u00b6 This section describes the files that exist within the DNAnexus output folder. Most of these files will not be of interest to the average user. However, interactive viewers are describe in LIMMA differential expression viewer and Simple differential expression viewer . The output will be divided into multiple folders. The results being the most useful will be the differential expression analysis results in the LIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage will be in the BIGWIG folder. Other folder contain different types of data and are explained in further detail below. The following description of files is sorted by their output directory. ALIGN \u00b6 This directory contains the BAM files described in BAM alignment files and the chimeric and junction files are described in Chimeric reads and junction files . In addition there are 2 log files. *Log.final.out has relevant statistics for the alignment. The *.Log.out file just contains a log of the analysis run, including input parameters. Per sample FLAGSTAT results are found in *.flagStatOut.txt . Finally the ALIGN directory has multiple .starAlign.methods.txt files. These files can be ignored as they are summarized in the finalmethods.docx * and methods.txt files described in Methods Files . BIGWIG \u00b6 All of the files here are described in section Coverage results . The bgToBw.methods.txt files can be ignored as they are summarized in the files described in Methods Files . BIGWIG_VIEWER \u00b6 See bigWig viewer COMBINED_FLAGSTAT \u00b6 This directory contains the alignmentStatistics.txt file, which contains the combined alignment statistics from all samples. It is generated from the flagstat files describe in the ALIGN directory. COMBINED_HTSEQ \u00b6 Used for input in differential expression analysis. The combineCountFile.txt is the same as countFile.txt described in Differential analysis input COVERAGE \u00b6 BED graph files used to generate bigWig files are here. FastQC \u00b6 See Quality Control Results (FastQC) HTSEQ \u00b6 Per-sample HTSEQ-count results ( *.htseq_counts.txt ) and FPKM results ( *.fpkm.txt ). Temporary methods files are found as *.htseq-count.methods.txt LIMMA \u00b6 mdsPlot.png , maPlot.png , volcanoPlot.png are described in Initial analysis of results results.txt , GSEA.input.txt and GSEA.tStat.txt are describe in Differential expression results voomLimma.R , countFile.txt , Rparameters.txt , and sampleList.txt are described in Differential analysis input See LIMMA differential expression viewer for a description of the VIEWERS directory. Other files in the LIMMA directory include contrastFiles.txt contrastsFile.txt, and limmaSampleList.txt which are used internally. limmaMethods.txt is an intermediate file describing methods. Out.tar.gz is used for testing purposes. The sessionInfo.txt file describe the R session working parameters and modules loaded. meanVariance.png is a plot for assessing quality of count data ( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29 ) METHODS The files here are described in Methods Files . SAMPLELIST These files are used internally by the pipeline. SIMPLE_DIFEX mdsPlot.normCPM.png and simpleDEPlot.png are described in Initial analysis of results simpleDE.txt are describe in Differential expression results simpleDE.R , countFile.txt , Rparameters_simple.txt , and sampleList.txt are described in Differential analysis input See Simple differential expression viewer for a description of the VIEWERS directory. Other files in the SIMPLE_DIFEX directory include contrastFiles.txt contrastsFile.txt, and limmaSampleList.txt which are used internally. simpleDifEx.methods.txt is an intermediate file describing methods. Out.tar.gz is used for testing purposes. The sessionInfo.txt file describe the R session working parameters and modules loaded. Frequently Asked Questions \u00b6 None yet! If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"WARDEN Differential Expression Analysis"},{"location":"guides/genomics-platform/analyzing-data/warden/#overview","text":"The WARDEN ( W**orkflow for the **A**nalysis of **R**NA-Seq **D**ifferential **E**xpressio**N ) software uses RNA-Seq sequence files to perform alignment, coverage analysis, gene counts and differential expression analysis.","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/warden/#inputs","text":"The WARDEN workflow requires two types of input files and that two parameters be set manually. All other parameters are preset with reasonable defaults. Name Type Description Example FastQ files ( required ) Input file(s) Gzipped FastQ files generated by experiment Sample1.fastq.gz, Sample2.fastq.gz Sample sheet ( required ) Input file Sample sheet generated and uploaded by the user *.txt","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/warden/#input-file-configuration","text":"You'll need to create a sample sheet which describes the relationship between case and control samples, phenotype/condition information, and the comparisons you would like to perform. The sample sheet is a tab-delimited text document that can be created in Microsoft Excel (recommended) or a text editor. Note You will need to upload your sample sheet in a similar manner as your FastQ files, so you can follow the same uploading instructions to achieve this.","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/warden/#prepare-using-microsoft-excel","text":"Tip Download the file_download sample excel spreadsheet as a starting point! The final product for the excel spreadsheet will look like the screenshot below. If you create the sample sheet from scratch, please ensure the the columns are exactly in this order. Sample rows Each row in the spreadsheet (except for the last row, which we will talk about in the next section) corresponds to a sample with one or more FastQ files. You should fill in these rows based on your data and the guidelines below: Guidelines The sample name should be unique and should only contain letters, numbers and underscores. The condition/phenotype column associates similar samples together. The values should contain only letters, numbers and underscores. ReadFile1 should contain forward reads (e.g. *.R1.fastq.gz or *_1.fastq.gz ). ReadFile2 will contain reads in reverse orientation to ReadFile2 (e.g. *.R2.fastq.gz or *_2.fastq.gz ). For single end reads a single dash ('-') should be entered in the ReadFile2 column. Comparison row The last line in the sample sheet is called the \"comparison row\". This line specifies the comparisons to be done between conditions/phenotypes. All pairwise combinations of the values in the \"Phenotype\" column can be analyzed. To specify the comparisons, on a separate line, include #comparisons= followed be a comma delimited list of two conditions separated by a dash. Example The following lines are all valid examples. #comparisons=KO-WT #comparisons=Condition1-Control,Condition2-Control #comparisons=Phenotype2-Phenotype1,Phenotype3-Phenotype2,Phenotype3-Phenotype1 Note If a comparison has at least 3 samples for each condition/phenotype, VOOM/LIMMA will be run. A simple differential comparison will be run on all samples. Finalizing the sample sheet To finalize the sample sheet, save the Microsoft Excel file with whatever name you like. Save the file as an Excel Workbook with the .xlsx extension.","title":"Prepare using Microsoft Excel"},{"location":"guides/genomics-platform/analyzing-data/warden/#prepare-using-a-text-editor","text":"Tip Download the file_download sample text file as a starting point! Creating a sample sheet with a text editor is an option for advanced users. The process of creating a sample sheet with a text editor is the same as creating one with Microsoft Excel, with the small difference that you must manually create your columns using the tab character. Save the file with a .txt extension.","title":"Prepare using a text editor"},{"location":"guides/genomics-platform/analyzing-data/warden/#outputs","text":"Name Description FastQC Report Quality control analysis by FastQC. Aligned BAM Aligned BAM files from STAR mapping. Splice junctions Splice junction information from STAR mapping. Coverage files bigWig ( .bw ) and BED ( .bed ) files detailing coverage. Gene counts Gene counts generated by HT-Seq count. VOOM/LIMMA results Pairwise comparisons of expression data. Requires at least 3 samples vs 3 samples. Simple DE analysis No statistical analysis, requires only a 1 samples vs 1 sample comparison. MA/Volcano plots Both of the above produce tabular outputs, MA plots and volcano plots.","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/warden/#workflow-steps","text":"FastQ files generated by RNA-Seq are mapped to a reference genome using the STAR. HT-Seq count is used to assign mapped reads to genes. Differential expression analysis is performed using VOOM normalization of counts and LIMMA analysis. Coverage plots of mapped reads are generated as interactive visualizations.","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/warden/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the WARDEN Differential Expression Analysis workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/warden/#uploading-input-files","text":"The WARDEN Differential Expression analysis pipeline takes Gzipped FastQ files generated by an RNA-Seq experiment as input . Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input Files"},{"location":"guides/genomics-platform/analyzing-data/warden/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. Note The WARDEN tool operation is slightly different than the other pipelines because it accepts a variable number of samples. First , you will run a \"bootstrapping\" step that creates a custom executable for your analysis. Second , you will need to manually execute the generated workflow from the first step. This allows us to take advantage of many nice features, like check-pointing and cost reduction. Don't worry, we'll show you how to do this step by step below.","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/warden/#hooking-up-inputs","text":"You'll need to hook up the FastQ files and sample sheet you uploaded in the upload data section . Click the FASTQ_FILES input field and select all FastQ files. Next, click the sampleList input field and select the corresponding samplesheet.","title":"Hooking up Inputs"},{"location":"guides/genomics-platform/analyzing-data/warden/#selecting-parameters","text":"We now need to configure the parameters for the pipeline, such as reference genome and sequencing method. You can access all of the available parameters by clicking on the WARDEN WORKFLOW GENERATOR substep. For the general workflow instructions refer here Parameter setup steps In the Output Folder field, select a folder to output to. You can structure your experiments however you like (e.g. /My_Outputs ) In the analysisName field, enter a prefix for all of the output files. This can be any value you want to use to remember this run. Be sure to use underscores instead of spaces here! Select the sequenceStandedness from the drop down menu. This information can be determined from the sequencing or source of the data. If you don't know what to put here, select \"no\". Select the Genome pulldown menu. Choose the appropriate box. The LIMMA parameters can be left alone for most analyses. If you are an advanced LIMMA user, you can change the various settings exposed below the required parameters. When all parameters have been set, press the save button.","title":"Selecting Parameters"},{"location":"guides/genomics-platform/analyzing-data/warden/#starting-the-workflow","text":"Once your input files are hooked up and your parameters are set, you should be able to start the workflow by clicking the \"Run as Analysis...\" button in the top right hand corner of the workflow dialog. For the general workflow instructions refer here . The tool will begin running and will automatically take you to the Monitor page, where you should see that your workflow is \"In Progress\". When the custom workflow has finished generating, the word 'Done' will appear in green in the status column. This indicates that the bootstrapping step has completed successfully.","title":"Starting the workflow"},{"location":"guides/genomics-platform/analyzing-data/warden/#custom-workflow-process","text":"Wait for the workflow generator to finish. Click on the WARDEN name in the name column. You will now be on a page specific to the running of the workflow. On the left side, you will see the inputs you selected for the workflow generator. On the right side are the output files (including the generated workflow). Select the generated workflow as shown in the picture below. You will now be within the output folder you specified earlier. Select the file that begins with 'WARDEN WORKFLOW:' A workflow generated for your data will be presented to you. Select 'Run as analysis' in the upper right. The workflow will initiate, and you will be brought to the 'Monitor' page. (Note to get back to this page, you can select 'Monitor' on one of the menu bars near the top ) Expand the the workflow progress be selecting the '+' sign next to 'In Progress' As parts of the pipeline are run, you will see different tasks in different colors. Green means done, blue is running, orange is waiting, and red means error. When done the status will be shown as 'Done'. Select the Workflow name under Status. You will be brought to a page that show more information about the workflow analysis. Click on the output folder to go to the output. The output folders will now be shown. For a description of the output, please refer to Interpreting Results .","title":"Custom Workflow Process"},{"location":"guides/genomics-platform/analyzing-data/warden/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/warden/#interpreting-primary-results","text":"","title":"Interpreting Primary Results"},{"location":"guides/genomics-platform/analyzing-data/warden/#alignment-statistics","text":"Several files should be examined initially to determine the quality of the results. alignmentStatistics.txt shows alignment statistics for all samples. This file is a plain text tab-delimited file that can be opened in Excel or a text editor such as Notepad++. This file contains information on the total reads per sample, the percentage of duplicate reads and the percentage of mapped reads. An example of this file is below. (Within the DNAnexus output directory structure, these files will be in the COMBINED_FLAGSTAT directory.)","title":"Alignment statistics"},{"location":"guides/genomics-platform/analyzing-data/warden/#multidimensional-scaling-mds-plot","text":"The second set of files to look at are the Multidimensional scaling (MDS) plots using the plotMDS function within LIMMA. Similar to PCA, these graphs will show how similar samples are to each other. There are different sets of MDS plots. For comparisons where there are 3 or more samples per condition, an MDS plot using Voom (Limma) normalized values are generated. An example can be seen below. These files will be labeled mdsPlot.png . For all comparisons, regardless of sample size, and MDS plot will also be generated with Counts per million (CPM) normalized gene counts. These files will be labeled mdsPlot.normCPM.png . (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) MDS plot from just CPM normalized data.","title":"Multidimensional scaling (MDS) Plot"},{"location":"guides/genomics-platform/analyzing-data/warden/#proteinpaint-visualizations","text":"Several files on DNAnexus allow the data to be viewed in the Protein Paint viewer. (Note: We plan to have links downloaded in the future to allow the viewing of these files off of DNAnexus.) LIMMA differential expression viewer Within LIMMA/VIEWERS directory (note if no comparisons meet the 3 sample condition, the LIMMA folder will not exist), there will be a viewer file for each valid comparison ( * results. .txt.viewer**). Simply select the file and press 'Launch viewer' in the lower right. A viewer will pop up showing both the MA Plot and Volcano plot. By moving the mouse over a circle, the circle will highlight and the corresponding gene on the other graph will also highlight. Additional information about the gene and its expression values will also be shown. One can also type in multiple gene symbols in the provided text box. By pressing 'Show gene labels' all these genes will show up on the plots. Simple differential expression viewer There will also be a viewer for the simple differential expression analysis in SIMPLE_DIFEX/VIEWERS. The P-value for the results have all been set to 1, so the volcano plot will not be relevant. bigWig viewer In the BIGWIG_VIEWER directory there will be a bigwigViewer file. Select this file and then 'Launch viewer'. A graph of coverage for the genome should be visible.","title":"ProteinPaint Visualizations"},{"location":"guides/genomics-platform/analyzing-data/warden/#interpreting-secondary-results","text":"","title":"Interpreting Secondary Results"},{"location":"guides/genomics-platform/analyzing-data/warden/#interactive-mavolcano-plots","text":"In addition to viewing the MA and volcano plots through the visualization tool","title":"Interactive MA/Volcano Plots"},{"location":"guides/genomics-platform/analyzing-data/warden/#differential-expression-results","text":"Other useful differential expression results will be downloaded by the desktop app. This included tabular output from the differential expression analysis. For each comparison with three or more samples per condition, results.*.txt will be produced.","title":"Differential expression results"},{"location":"guides/genomics-platform/analyzing-data/warden/#gseainputtxt-and-gseatstattxt","text":"Input files that can be used for GSEA analysis. The tStat file is preferred for a more accurate analysis, but will not give a heatmap diagram. - (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) For plain text results from the simple differential expression analysis, the files will be named simpleDE.*.txt . (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.) Prelabelled MA and volcano plots are provided for the analysis. These files are labeled maPlot.*.png and volcanoPlot.*.png where * is the comparison (e.g. ko_vs_wt) The MA plot shows the average expression of the gene on the X-axis, and Log2 fold change between condition/phenotype is on the Y-axis (if the name is for example maPlot.condition2-condition1.png then the fold change would represent condition1 minus condition2). Each gene is represented by a circle. The top 20 genes (by p-value) are identified on the plot. The genes are color coded by the chosen multiple testing correction method (False Discovery Rate (FDR) by default. An example MA plot can be seen below. (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) The volcano plot shows the Log2Fold change between the conditions on the X-axis, and the -Log10 of the multiple testing corrected P-value on the Y-axis. An MA plot is generated for all comparisons regardless of number of samples. This is the simpleDEPlot.*.png no statistics are shown and genes are not labeled. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)","title":"GSEA.input.txt and GSEA.tStat.txt"},{"location":"guides/genomics-platform/analyzing-data/warden/#differential-analysis-input","text":"Inputs and commands are provided for rerunning differential expression analysis on ones own computer. The R commands used for the analysis are found in voomLimma.R . An experienced R user can rerun the analysis with any desired changes. This analysis requires the input countFile.txt which contains counts per genes, the Rparameters.txt file containing input parameters, and a processed sample list file sampleList.txt (Within the DNAnexus output directory structure, these files will be in the LIMMA directory.) The input for the simple differential analysis expression will be Rparameters_simple.txt , simpleDE.R , countFile.txt and sampleList.txt . countFile.txt and sampleList.txt are the same files used by the LIMMA analysis. (Within the DNAnexus output directory structure, these files will be in the SIMPLE_DIFEX directory.)","title":"Differential analysis input"},{"location":"guides/genomics-platform/analyzing-data/warden/#coverage-results","text":"bigWig files will be generated for use in genome browsers (such as IGV http://software.broadinstitute.org/software/igv/ ). For each sample, multiple bigWig files will be found. For all types of sequencing strandedness, there will be bigWig files labeled, *.sortedCoverageFile.bed.bw where ' ' is the sample name. For stranded data there will also be*.sortedPosCoverageFile.bed.bw * and *.sortedNegCoverageFile.bed.bw which contains coverage information for the positive and negative strand of the genome. (Within the DNAnexus output directory structure, these files will be in the BIGWIG directory.)","title":"Coverage results"},{"location":"guides/genomics-platform/analyzing-data/warden/#quality-control-results-fastqc","text":"Within the FastQC directory, for each sample and read direction there will be an html file and a zip file ( *.FastQc.html *.FastQc.zip where '*' is the base FastQ name), containing results from FastQTC. For the average user the html file is sufficient. This file can give some basic statistics on the quality of the data. (Within the DNAnexus output directory structure, these files will be in the FastQC directory.)","title":"Quality Control Results (FastQC)"},{"location":"guides/genomics-platform/analyzing-data/warden/#bam-alignment-files","text":"There are two BAM files generated per sample that contain mapping information for all reads. The first is labeled *.Aligned.sortedByCoord.dup.bam where ' ' is the sample name. The BAM file is sorted by coordinates and has duplicates marked. The second file is*.Aligned.toTranscriptome.out.bam * and contains reads mapped to transcripts. (Within the DNAnexus output directory structure, these files will be in the ALIGN directory.)","title":"BAM alignment files"},{"location":"guides/genomics-platform/analyzing-data/warden/#chimeric-reads-and-junction-files","text":"Additional files created by STAR are provided. More information on these files can be found here . *.SJ.out.tab contain splice junction information. Fusion detection files are labeled *.Chimeric.out.bam and *.Chimeric.out.junction . (Within the DNAnexus output directory structure, these files will be in the ALIGN directory.)","title":"Chimeric reads and junction files"},{"location":"guides/genomics-platform/analyzing-data/warden/#fpkm-and-count-files-per-sample","text":"Per sample files containing FPKM and raw count values for each gene can be found in *.fpkm.txt and *.htseq_counts.txt where '*' is the sample name. Within the DNAnexus output directory structure, these files will be in the HTSEQ directory.","title":"FPKM and count files (per sample)"},{"location":"guides/genomics-platform/analyzing-data/warden/#methods-files","text":"A more human readable explanation is found in methods.docx . Detailed documentation can be found in methods.txt (Within the DNAnexus output directory structure, these files will be in the METHODS directory.)","title":"Methods Files"},{"location":"guides/genomics-platform/analyzing-data/warden/#auxiliary-files","text":"This section describes the files that exist within the DNAnexus output folder. Most of these files will not be of interest to the average user. However, interactive viewers are describe in LIMMA differential expression viewer and Simple differential expression viewer . The output will be divided into multiple folders. The results being the most useful will be the differential expression analysis results in the LIMMA and SIMPLE_DIFEX folders. Bigwig files for viewing read coverage will be in the BIGWIG folder. Other folder contain different types of data and are explained in further detail below. The following description of files is sorted by their output directory.","title":"Auxiliary Files"},{"location":"guides/genomics-platform/analyzing-data/warden/#align","text":"This directory contains the BAM files described in BAM alignment files and the chimeric and junction files are described in Chimeric reads and junction files . In addition there are 2 log files. *Log.final.out has relevant statistics for the alignment. The *.Log.out file just contains a log of the analysis run, including input parameters. Per sample FLAGSTAT results are found in *.flagStatOut.txt . Finally the ALIGN directory has multiple .starAlign.methods.txt files. These files can be ignored as they are summarized in the finalmethods.docx * and methods.txt files described in Methods Files .","title":"ALIGN"},{"location":"guides/genomics-platform/analyzing-data/warden/#bigwig","text":"All of the files here are described in section Coverage results . The bgToBw.methods.txt files can be ignored as they are summarized in the files described in Methods Files .","title":"BIGWIG"},{"location":"guides/genomics-platform/analyzing-data/warden/#bigwig_viewer","text":"See bigWig viewer","title":"BIGWIG_VIEWER"},{"location":"guides/genomics-platform/analyzing-data/warden/#combined_flagstat","text":"This directory contains the alignmentStatistics.txt file, which contains the combined alignment statistics from all samples. It is generated from the flagstat files describe in the ALIGN directory.","title":"COMBINED_FLAGSTAT"},{"location":"guides/genomics-platform/analyzing-data/warden/#combined_htseq","text":"Used for input in differential expression analysis. The combineCountFile.txt is the same as countFile.txt described in Differential analysis input","title":"COMBINED_HTSEQ"},{"location":"guides/genomics-platform/analyzing-data/warden/#coverage","text":"BED graph files used to generate bigWig files are here.","title":"COVERAGE"},{"location":"guides/genomics-platform/analyzing-data/warden/#fastqc","text":"See Quality Control Results (FastQC)","title":"FastQC"},{"location":"guides/genomics-platform/analyzing-data/warden/#htseq","text":"Per-sample HTSEQ-count results ( *.htseq_counts.txt ) and FPKM results ( *.fpkm.txt ). Temporary methods files are found as *.htseq-count.methods.txt","title":"HTSEQ"},{"location":"guides/genomics-platform/analyzing-data/warden/#limma","text":"mdsPlot.png , maPlot.png , volcanoPlot.png are described in Initial analysis of results results.txt , GSEA.input.txt and GSEA.tStat.txt are describe in Differential expression results voomLimma.R , countFile.txt , Rparameters.txt , and sampleList.txt are described in Differential analysis input See LIMMA differential expression viewer for a description of the VIEWERS directory. Other files in the LIMMA directory include contrastFiles.txt contrastsFile.txt, and limmaSampleList.txt which are used internally. limmaMethods.txt is an intermediate file describing methods. Out.tar.gz is used for testing purposes. The sessionInfo.txt file describe the R session working parameters and modules loaded. meanVariance.png is a plot for assessing quality of count data ( https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29 ) METHODS The files here are described in Methods Files . SAMPLELIST These files are used internally by the pipeline. SIMPLE_DIFEX mdsPlot.normCPM.png and simpleDEPlot.png are described in Initial analysis of results simpleDE.txt are describe in Differential expression results simpleDE.R , countFile.txt , Rparameters_simple.txt , and sampleList.txt are described in Differential analysis input See Simple differential expression viewer for a description of the VIEWERS directory. Other files in the SIMPLE_DIFEX directory include contrastFiles.txt contrastsFile.txt, and limmaSampleList.txt which are used internally. simpleDifEx.methods.txt is an intermediate file describing methods. Out.tar.gz is used for testing purposes. The sessionInfo.txt file describe the R session working parameters and modules loaded.","title":"LIMMA"},{"location":"guides/genomics-platform/analyzing-data/warden/#frequently-asked-questions","text":"None yet! If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently Asked Questions"},{"location":"guides/genomics-platform/analyzing-data/warden/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/","text":"XenoCP Authors John Doe Publication N/A (not published) Technical Support Contact Us Overview \u00b6 abstract-type description of tool Inputs \u00b6 table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500 Input file configuration \u00b6 if needed Outputs \u00b6 table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format Workflow Steps \u00b6 description of algorithm(s) or workflow steps Additional Info \u00b6 description of any additional information that the user might need to know/do before running the workflow Creating a workspace \u00b6 Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here . Uploading Input files \u00b6 note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created. Running the Workflow \u00b6 Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow Analysis of Results \u00b6 Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files. Interpreting results \u00b6 detailed explanations with helpful screenshots or gifs Frequently asked questions \u00b6 faqs If you have any questions not covered here, feel free to reach out on our contact form . Similar Topics \u00b6 Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"XenoCP"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#overview","text":"abstract-type description of tool","title":"Overview"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#inputs","text":"table of inputs; example below Name Type Description Example FastQ files ( required if using FastQ inputs) Input file Gzipped FastQ files generated by experiment. Sample_R1.fastq.gz and Sample_R2.fastq.gz BAM files ( required if using BAM inputs) Input file BAM files aligned against HG19/Hg38 (WGS, WES or RNA-Seq). Sample.bam BAM indices ( required if using BAM inputs) Input file Corresponding BAM index of the BAM files above. Sample.bam.bai Mutation file ( required ) Input file File describing the mutations present in the sample (special format, see below). *.txt (tab-delimited) SNV or fusion Parameter Specify the mutation file contains SNV or gene fusion. SNV Peptide size Parameter Size of the peptide. 9 Affinity threshold Parameter Affinity cutoff for epitope prediction report. 500","title":"Inputs"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#input-file-configuration","text":"if needed","title":"Input file configuration"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#outputs","text":"table of outputs; example below Name Description Epitope affinity prediction (html) Epitope affinity. The peptide with affinity < cutoff will be highlighted. Epitope affinity prediction (xlsx) Excel tables for the information of all epitopes Affinity (raw output) Epitope affinity Peptide sequence (raw output) Peptide sequences in Fasta format","title":"Outputs"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#workflow-steps","text":"description of algorithm(s) or workflow steps","title":"Workflow Steps"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#additional-info","text":"description of any additional information that the user might need to know/do before running the workflow","title":"Additional Info"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#creating-a-workspace","text":"Before you can run one of our workflows, you must first create a workspace in DNAnexus for the run. Refer to the general workflow guide to learn how to create a DNAnexus workspace for each workflow run. You can navigate to the workflow name workflow page here .","title":"Creating a workspace"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#uploading-input-files","text":"note any additional information the user should know about what input files are required Refer to the general workflow guide to learn how to upload input files to the workspace you just created.","title":"Uploading Input files"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#running-the-workflow","text":"Refer to the general workflow guide to learn how to launch the workflow, hook up input files, adjust parameters, start a run, and monitor run progress. !!! caution any cautionary notes specific to running this workflow","title":"Running the Workflow"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#analysis-of-results","text":"Each tool in St. Jude Cloud produces a visualization that makes understanding results more accessible than working with excel spreadsheet or tab delimited files. This is the primary way we recommend you work with your results. Refer to the general workflow guide to learn how to access these visualizations. We also include the raw output files for you to dig into if the visualization is not sufficient to answer your research question. Refer to the general workflow guide to learn how to access raw results files.","title":"Analysis of Results"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#interpreting-results","text":"detailed explanations with helpful screenshots or gifs","title":"Interpreting results"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#frequently-asked-questions","text":"faqs If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/genomics-platform/analyzing-data/xeno-cp/#similar-topics","text":"Running our Workflows Working with our Data Overview Downloading/Uploading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/managing-data/data-transfer-app/","text":"Data Transfer App The Data Transfer App is a downloadable tool with an easy to use graphical user interface that allows you to upload/download files to/from your DNAnexus projects on the cloud. For users looking to upload/download files using the command line, please refer to the command line interaction guide . If you are interested in viewing the source code, you can do so here . If you would like to file an issue you are experiencing with the application, you can do so here or let us know your feedback through our contact us form . Getting Started \u00b6 Click here to download the latest release of the St. Jude Cloud data transfer application. Once you've completed installing the app, you will see a page that looks like this. Log in with your DNAnexus credentials (or click on I'm a St. Jude employee to log in with your St. Jude credentials). Each time you log in, the app will prompt you to grant it access to all files in your DNAnexus projects. Click Grant Access to proceed. Access is granted per session and will expire once you log out of the data transfer app. Once you've given the app access to your DNANexus projects, you will see the projects listed in a sidebar on the left and an upload/download panel on the right. Before moving on, we encourage you to take the TOUR by clicking on the green button in the upper right corner. As you will see in the tour, you have the option to Show All Files in your DNAnexus projects. It is a good idea to always have this option enabled. Warning You can increase the concurrency (# of files that will upload or download at the same time) but this will affect the performance of the app. For example changing the concurrency from 1 to 100 will move files at roughly 100 th of the speed. Uploading Files \u00b6 Select the DNAnexus project on the left that you would like to upload files to. Select Upload in the app's Upload/Download panel. To select files you may either (1) Click in the upload space to select files in your computer's file navigation application or (2) highlight all the files you want to upload, then drag and drop them into the app's upload space. Review the list of files to upload, and click Upload . To learn how to upload files using the command line, please refer to the command line interaction guide . Downloading Files \u00b6 Select the DNAnexus project on the left that you would like to download files from. Select Download in the app's Upload/Download panel. In may take a minute to display all the files in your project. Once all files are displayed, select the files you want to download and click Download . Note that the Data Transfer App does not recognize any directory structure you may have within your DNAnexus projects. It is simply a dump of all the files in each project. To learn how to download files using the command line, please refer to the command line interaction guide . Similar Topics \u00b6 About our Data Making a Data Request Managing Data Overview Command Line Interaction","title":"Uploading/Downloading Data"},{"location":"guides/genomics-platform/managing-data/data-transfer-app/#getting-started","text":"Click here to download the latest release of the St. Jude Cloud data transfer application. Once you've completed installing the app, you will see a page that looks like this. Log in with your DNAnexus credentials (or click on I'm a St. Jude employee to log in with your St. Jude credentials). Each time you log in, the app will prompt you to grant it access to all files in your DNAnexus projects. Click Grant Access to proceed. Access is granted per session and will expire once you log out of the data transfer app. Once you've given the app access to your DNANexus projects, you will see the projects listed in a sidebar on the left and an upload/download panel on the right. Before moving on, we encourage you to take the TOUR by clicking on the green button in the upper right corner. As you will see in the tour, you have the option to Show All Files in your DNAnexus projects. It is a good idea to always have this option enabled. Warning You can increase the concurrency (# of files that will upload or download at the same time) but this will affect the performance of the app. For example changing the concurrency from 1 to 100 will move files at roughly 100 th of the speed.","title":"Getting Started"},{"location":"guides/genomics-platform/managing-data/data-transfer-app/#uploading-files","text":"Select the DNAnexus project on the left that you would like to upload files to. Select Upload in the app's Upload/Download panel. To select files you may either (1) Click in the upload space to select files in your computer's file navigation application or (2) highlight all the files you want to upload, then drag and drop them into the app's upload space. Review the list of files to upload, and click Upload . To learn how to upload files using the command line, please refer to the command line interaction guide .","title":"Uploading Files"},{"location":"guides/genomics-platform/managing-data/data-transfer-app/#downloading-files","text":"Select the DNAnexus project on the left that you would like to download files from. Select Download in the app's Upload/Download panel. In may take a minute to display all the files in your project. Once all files are displayed, select the files you want to download and click Download . Note that the Data Transfer App does not recognize any directory structure you may have within your DNAnexus projects. It is simply a dump of all the files in each project. To learn how to download files using the command line, please refer to the command line interaction guide .","title":"Downloading Files"},{"location":"guides/genomics-platform/managing-data/data-transfer-app/#similar-topics","text":"About our Data Making a Data Request Managing Data Overview Command Line Interaction","title":"Similar Topics"},{"location":"guides/genomics-platform/managing-data/how-to-fill-out-Extension/","text":"Filling Out The Extension Addendum The St. Jude Cloud Data Access Agreement (DAA) is only valid for one year after the date it was approved. When your DAA is about to expire, you will get an automated email from notifications@stjude.cloud with the name of the DAA that is expiring and a link to the St. Jude Cloud Extension Addendum. In order to extend your DAA, you must fill out an Extension Addendum. The Extension Addendum will extend the previous agreement for an additional year. Please note that if you do not fill out an Extension Addendum, you will be expected to delete all copies of the data subject to the expiring agreement. Follow the steps below to ensure that you have accurately filled out all sections of the Extension Addendum. Page 1 In the top section, enter the current date on which that the agreement is being filled out, your current institution, and the date on which you signed the expiring DAA or extension. In the bottom section, enter a date that is one year after the date on which you signed the expiring DAA or extension. This can be found on the page linked from the email notifying you of agreement expiration. Page 2 Check exactly the datasets that you were granted access to by the terms of the original DAA. The datasets checked on the original DAA and any extensions must match. If you would like to apply for access to additional datasets, please make a new data request for the additional dataset(s). The Principal Investigator (PI) or faculty level supervisor on the project must sign and date the extension. The PI who signed the original DAA must match the PI signing any extensions. All additional applicants (excluding the administrative authority and information security officer) that were included in the original DAA must sign and date the extension. Page 3 Enter the name of your current institution. This must match the institution entered on page 1. The Administrative Authority must sign and date the extension. This is usually the same administrative authority as the one on the original DAA. The Information Security Officer must sign and date the extension. The bottom section of page 3 is for St. Jude to sign and date. Do not fill out this section. Page 4 If your research question(s) or contemplated use has changed since the original DAA, use this space to provide your updated project description. You may also use this space to explain why you need to extend your agreement. Finally, if the Administrative Authority or Information Security Officer has changed from the original DAA to this extension, please use this space to explain why. Once you have finished filling out the Extension Addendum, you may upload the completed form on the extension addendum page linked in the notification of expiration email from notifications@stjude.cloud . Similar Topics \u00b6 About our Decision Process & Terminology Filling Out a Data Access Agreement","title":"Renewing your Data Access"},{"location":"guides/genomics-platform/managing-data/how-to-fill-out-Extension/#similar-topics","text":"About our Decision Process & Terminology Filling Out a Data Access Agreement","title":"Similar Topics"},{"location":"guides/genomics-platform/managing-data/working-with-our-data/","text":"In this overview, we will explain how to manage your data request(s) from St. Jude Cloud's genomics platform My Dashboard page and how to access and manage your data (once it has been vended to you) from within a DNAnexus project. The DNAnexus genomic ecosystem is the backbone for the computation and storage in St. Jude Cloud. This means that each data request in St. Jude Cloud corresponds to a project in DNAnexus. If you'd like, you can read an introduction to the DNAnexus ecosystem here . If you haven't already, follow this guide to request access to St. Jude data in this secure cloud ecosystem. Managing Your Data Requests \u00b6 Below is a snapshot of the My Dashboard on our Genomcis Platform. From this page you can check the status of your data request, complete an EDAA draft, upload a revised DAA or an Extension Addendum , or link to your DNAnexus project folder for a specific data request. Pending Request Types Request 1 is an Open Draft, meaning the requestor has not yet finished the setup wizard and the DocuSign envelope has not yet been sent to any of the signatories. Request 2, listed in the Projects section, has been sent to the signatories, but has not been completed by all of them. This status will look like the Request 3 when all of the signatories sign the document and it is ready to be sent to the Data Access Committee(s). Request 3 is pending approval from the Data Access Committee(s), and the status will change from Pending to either Approved or Rejected, based on their decision. All submitted manual-process Data Access Agreements will show up on your My Dashboard page like Request 3. If you have a question about the status of your data request which is not answered on the \"My Dashboard\" page, you can email us at support@stjude.cloud . Accessing Your Data \u00b6 Once your data access request is approved, the data you requested from St. Jude Cloud will automatically be distributed to a DNAnexus project with the same name that you entered through the data request setup wizard. From your My Dashboard page, click on a specific data request name to navigate directly to your corresponding project in DNAnexus. (You can also follow the link in the approval email from notifications@stjude.cloud .) When the data is vended, the directory structure of your DNAnexus project will look something like this: project_space/ \u251c\u2500\u2500 restricted/ \u2502 \u251c\u2500\u2500 bam/ \u2502 \u251c\u2500\u2500 gVCF/ \u2502 \u251c\u2500\u2500 Somatic_VCF/ \u2502 \u2514\u2500\u2500 CNV/ \u2514\u2500\u2500 SAMPLE_INFO.txt The SAMPLE_INFO.txt file provides all the metadata associated with the request, and the restricted folder contains all the data for which you were approved separated by file type. Using Your Data \u00b6 There are two primary ways you can interact with data vended to you in St. Jude Cloud: Cloud access . You can choose to work with the data in DNAnexus' genomics cloud ecosystem. This is our suggested method of interaction, as you can avoid downloading the data to your local servers (which both takes time and is error prone). If you choose to leverage this approach, you can either wrap your own analysis pipeline as a cloud app (see our guide ) or leverage any of DNAnexus' publicly available apps (see DNAnexus' guide . Download the data ( not suggested ). The second way to interact with data vended to you in St. Jude Cloud is by downloading the data to your local servers. If you wish to do this, you can either leverage the St. Jude Cloud Data Transfer Application (see our guide ) or you can download the data on the command line (see our guide ). Note that you must have indicated you wish to download the data in your data access agreement . Similar Topics \u00b6 About our Data Making a Data Request Uploading/Downloading Data","title":"Managing Data Overview"},{"location":"guides/genomics-platform/managing-data/working-with-our-data/#managing-your-data-requests","text":"Below is a snapshot of the My Dashboard on our Genomcis Platform. From this page you can check the status of your data request, complete an EDAA draft, upload a revised DAA or an Extension Addendum , or link to your DNAnexus project folder for a specific data request. Pending Request Types Request 1 is an Open Draft, meaning the requestor has not yet finished the setup wizard and the DocuSign envelope has not yet been sent to any of the signatories. Request 2, listed in the Projects section, has been sent to the signatories, but has not been completed by all of them. This status will look like the Request 3 when all of the signatories sign the document and it is ready to be sent to the Data Access Committee(s). Request 3 is pending approval from the Data Access Committee(s), and the status will change from Pending to either Approved or Rejected, based on their decision. All submitted manual-process Data Access Agreements will show up on your My Dashboard page like Request 3. If you have a question about the status of your data request which is not answered on the \"My Dashboard\" page, you can email us at support@stjude.cloud .","title":"Managing Your Data Requests"},{"location":"guides/genomics-platform/managing-data/working-with-our-data/#accessing-your-data","text":"Once your data access request is approved, the data you requested from St. Jude Cloud will automatically be distributed to a DNAnexus project with the same name that you entered through the data request setup wizard. From your My Dashboard page, click on a specific data request name to navigate directly to your corresponding project in DNAnexus. (You can also follow the link in the approval email from notifications@stjude.cloud .) When the data is vended, the directory structure of your DNAnexus project will look something like this: project_space/ \u251c\u2500\u2500 restricted/ \u2502 \u251c\u2500\u2500 bam/ \u2502 \u251c\u2500\u2500 gVCF/ \u2502 \u251c\u2500\u2500 Somatic_VCF/ \u2502 \u2514\u2500\u2500 CNV/ \u2514\u2500\u2500 SAMPLE_INFO.txt The SAMPLE_INFO.txt file provides all the metadata associated with the request, and the restricted folder contains all the data for which you were approved separated by file type.","title":"Accessing Your Data"},{"location":"guides/genomics-platform/managing-data/working-with-our-data/#using-your-data","text":"There are two primary ways you can interact with data vended to you in St. Jude Cloud: Cloud access . You can choose to work with the data in DNAnexus' genomics cloud ecosystem. This is our suggested method of interaction, as you can avoid downloading the data to your local servers (which both takes time and is error prone). If you choose to leverage this approach, you can either wrap your own analysis pipeline as a cloud app (see our guide ) or leverage any of DNAnexus' publicly available apps (see DNAnexus' guide . Download the data ( not suggested ). The second way to interact with data vended to you in St. Jude Cloud is by downloading the data to your local servers. If you wish to do this, you can either leverage the St. Jude Cloud Data Transfer Application (see our guide ) or you can download the data on the command line (see our guide ). Note that you must have indicated you wish to download the data in your data access agreement .","title":"Using Your Data"},{"location":"guides/genomics-platform/managing-data/working-with-our-data/#similar-topics","text":"About our Data Making a Data Request Uploading/Downloading Data","title":"Similar Topics"},{"location":"guides/genomics-platform/requesting-data/about-our-data/","text":"About Our Data File Formats \u00b6 St. Jude Cloud hosts both raw genomic data files and processed results files: File Type Short Description Details BAM HG38 aligned BAM files produced by Microsoft Genomics Service (DNA-Seq) or STAR 2-pass mapping (RNA-Seq). Click here gVCF Genomic VCF files produced by Microsoft Genomics Service . Click here Somatic VCF Curated list of somatic variants produced by the St. Jude somatic variant analysis pipeline. Click here CNV List of somatic copy number alterations produced by St. Jude CONSERTING pipeline. Click here BAM files \u00b6 In St. Jude Cloud, we store aligned sequence reads in BAM file format for whole genome sequencing, whole exome sequencing, and RNA-seq. For more information on SAM/BAM files, please refer to the SAM/BAM specification . For research samples, we require the standard 30X coverage for whole genome and 100X for whole exome sequencing. For clinical samples, we require higher coverage, 45X, for whole genome sequencing due to tumor purity issues found in clinical tumor specimens. For RNA-Seq, since only a subset of genes are expressed in a specific tissue, we require 30% of the exons to have 20X coverage in order to ensure that at least 30% of the expressed genes have sufficient coverage. gVCF files \u00b6 We provide gVCF files produced by the Microsoft Genomics Service . gVCF files are derived from the BAM files produced above as called by GATK's haplotype caller . Today, we defer to the official specification document from the Broad Institute, as well as this discussion on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to the official Microsoft Genomics whitepaper . Somatic VCF files \u00b6 Somatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking: Reads were aligned to HG19 using bwa backtrack ( bwa aln + bwa sampe ) using default parameters. Post processing of aligned reads was performed using Picard CleanSam and MarkDuplicates . Variants were called using the Bambino variant caller (you can download Bambino here or by navigating to the Zhang Lab page where the \"Bambino package\" is listed as a dependency under the CONSERTING section). Variants were post-processed using an in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available. Variants were manually reviewed by analysts and published with the relevant Pediatric Cancer Genome Project (PCGP) paper . Post-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the HG19 INFO field.). Note Our Somatic VCF files were designed specifically for St. Jude Cloud visualization purposes. Variants in these files were manually curated from analyses across multiple sequencing types including WGS and WES. For more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed here . CNV files \u00b6 CNV files contain copy number alteration (CNA) analysis results for paired tumor-normal WGS samples. Files are produced by running paired tumor-normal BAM files through the CONSERTING pipeline which identifies CNA through iterative analysis of (i) local segmentation by read depth within boundaries identified by structural variation (SV) breakpoints followed by (ii) segment merging and local SV analysis. CREST was used to identify local SV breakpoints. CNV files contain the following information: Field Description chrom chromosome loc.start start of segment loc.end end of segment num.mark number of windows retained in the segment (gaps and windows with low mappability are excluded) length.ratio The ratio between the length of the used windows to the genomic length seg.mean The estimated GC corrected difference signal (2 copy gain will have a seg.mean of 1) GMean The mean coverage in the germline sample (a value of 1 represents diploid) DMean The mean coverage in the tumor sample LogRatio Log2 ratio between tumor and normal coverage Quality score A empirical score used in merging SV_Matching Whether the boundary of the segments were supported by SVs (3: both ends supported, 2: right end supported, 1: left end supported, 0: neither end supported) Sequencing Information \u00b6 Whole Genome and Whole Exome \u00b6 Whole Genome Sequence (WGS) and Whole Exome Sequence (WES) BAM files were produced by the Microsoft Genomics Service aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to the official Microsoft Genomics whitepaper . RNA-Seq \u00b6 RNA-Seq BAM files are mapped to HG38 + ERCC Spike In Sequences (commonly used for normalization of expression analyses). For alignment, STAR v2.5.3a 2-pass mapping followed by Picard MarkDuplicates . Below is the STAR command used during alignment. For more information about any of the parameters used, please refer to the STAR manual for v2.5.3a. STAR \\ --runThreadN $NUM_THREADS \\ # $NUM_THREADS is the number of threads to parallelize the alignment across (generally we use 4). --genomeDir $GENOME_DIR \\ # $GENOME_DIR is a STAR reference directory containing HG38 and ERCC Spike In sequences. --readFilesIn $READ_FILES \\ # $READ_FILES are the input FastQ files to align. --limitBAMsortRAM $MEMORY_LIMIT \\ # $MEMORY_LIMIT is a upper limit on the amount of RAM to use in the alignment. --outFileNamePrefix $OUT_FILE_PREFIX \\ --outSAMtype BAM SortedByCoordinate \\ --outSAMstrandField intronMotif \\ --outSAMattributes NH HI AS nM NM MD XS \\ --outSAMunmapped Within \\ --outSAMattrRGline $RGs \\ # $RGs is the read group information for each FastQ passed in $READ_FILES. --outFilterMultimapNmax 20 \\ --outFilterMultimapScoreRange 1 \\ --outFilterScoreMinOverLread 0 .66 \\ --outFilterMatchNminOverLread 0 .66 \\ --outFilterMismatchNmax 10 \\ --alignIntronMax 500000 \\ --alignMatesGapMax 1000000 \\ --alignSJDBoverhangMin 1 \\ --sjdbScore 2 \\ --twopassMode Basic Data Access Units \u00b6 We currently have the five Data Access Units (DAU) listed below. Basic clinical data is available for relevant subjects in each DAU. Click on the DAU's abbreviation below to navigate directly to that DAU's Study page for more detailed information. Pediatric Cancer Genome Project (PCGP) \u00b6 PCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer. The Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients. St. Jude Lifetime (SJLIFE) \u00b6 SJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy. St. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples. Clinical Genomics (Clinical Pilot, G4K, and RTCG) \u00b6 Clinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors. Clinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of three studies: Clinical Pilot, Genomes4Kids, and Real-time Clinical Genomics. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. The RTCG study aims to release Clinical Genomics data in real time to the research community. The goal of these studies is to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors. Sickle Cell Genome Project (SGP) \u00b6 SGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood. The Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing. Childhood Cancer Survivor Study (CCSS) \u00b6 CCSS is a germline-only dataset consisting of whole genome sequencing of childhood cancer survivors. CCSS is a multi-institutional, multi-disciplinary, NCI-funded collaborative resource established to evaluate long-term outcomes among survivors of childhood cancer. It is a retrospective cohort consisting of >24,000 five-year survivors of childhood cancer who were diagnosed between 1970-1999 at one of 31 participating centers in the U.S. and Canada. The primary purpose of this sequencing of CCSS participants is to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. CCSS: Potential Bacterial Contamination Samples for the Childhood Cancer Survivorship Study were collected by sending out Buccal swab kits to enrolled participants and having them complete the kits at home. This mechanism of collecting saliva and buccal cells for sequencing is highly desirable because of its non-invasive nature and ease of execution. However, collection of samples in this manner also has higher probability of contamination from external sources (as compared to, say, samples collected using blood). We have observed some samples in this cohort which suffer from bacterial contamination. To address this issue, we have taken the following steps: We have estimated the bacterial contamination rate and annotated each of the samples in the CCSS cohort. For each sample, you will find the estimated contamination rate in the Description field of the SAMPLE_INFO.txt file that is vended with your data (and as a property on the DNAnexus file). For information on this field, see the Metadata specification . Using this estimated contamination rate, we have removed 82 samples which exhibited large rates of bacterial contamination. For the remaining samples, we have provided the BAM file as aligned with bwa mem with default parameters. We have observed that there are instances of reads originating from bacterial contamination that are erroneously mapped to the human genome and display a very low mapping quality. Please be advised that we have kept these reads as they were aligned and have not yet made any attempt to unmap these reads. Any analysis you perform on these samples will need to take this into account! Last, we will be working over the coming months to unmap the reads originating from bacterial contamination and release updated BAM files along with the associated gVCF files from Microsoft Genomics Service. With any questions on the nature or implications of this warning, please contact us at support@stjude.cloud . Metadata \u00b6 Each data request includes a text file called SAMPLE_INFO.txt that provides a number of file level properties (sample identifiers, clinical attributes, etc). Standard Metadata \u00b6 Below are the set of tags which may exist for any given file in St. Jude Cloud. All optional metadata will have sj_ prepended to their tag name. Property Description file_path The path to the file in your St. Jude Cloud project. subject_name A unique subject identifier assigned internally at St. Jude. sample_name A unique sample identifier assigned internally at St. Jude. sample_type One of Autopsy, Cell line, Diagnosis, Germline, Metastasis, Relapse, or Xenograft. sequencing_type Whether the file was generated from Whole Genome (WGS), Whole Exome (WES), or RNA-Seq. file_type One of the file types available in St. Jude Cloud. description Optional field that may contain additional file information. sj_diseases Short disease identifier assigned at the time of genomic sequencing. Note that this diagnosis may be refined after undergoing genomic testing. When including diagnosis in your analysis, we recommend you use attr_diagnosis , which is the most up to date value for diagnosis. sj_datasets If present, the datasets in the data browser which this file is associated with. sj_pmid_accessions If the file was associated with a paper, the related Pubmed accession number. sj_ega_accessions If the file was associated with a paper, the related EGA accession number. sj_dataset_accession If present, the permanent accession number assigned in St. Jude Cloud. sj_embargo_date The embargo date , which specifies the first date which the files can be used in a publication. Clinical and Phenotypic Information \u00b6 Also included is a set of phenotypic information queried from the physician or research team's records at the time of sample submission to St. Jude Cloud. These are all considered to be optional , as the level of information gathered for each sample varies. If empty, the physician or research team did not indicate a value for the field. All basic clinical or phenotypic information will have attr_ prepended to their tag name. Property Description attr_age_at_diagnosis Age at first diagnosis. This field is normalized as a decimal value. If empty, the physician or research team did not indicate a value for this field. attr_diagnosis Primary diagnosis reported by the clinic. attr_ethnicity Self-reported ethnicity. Values are normalized according to the US Census Bureau classifications . attr_race Self-reported race. Values are normalized according to the US Census Bureau classifications . attr_sex Self-reported sex. attr_oncotree_disease_code The disease code (assigned at the time of genomic sequencing) as specified by Oncotree Version 2019-03-01 . Short Disease Code Mapping \u00b6 Embedded in both the filename and the SAMPLE_INFO.txt file that comes with your data request will be a list of short diagnosis codes ( sj_diseases ). These short codes were assigned at the time that the sample was sent for sequencing, and they are not necessarily the final diagnosis ( attr_diagnosis ). For instance, the diagnosis is often refined as the sample undergoes genomic testing. Below, we include a table of short disease code to long disease name mappings so you can interpret what these abbreviations mean. Short Disease Code Long Diagnosis Description ACT Adrenocortical Carcinoma AEL Acute erythroid leukemia (AML M6) ALCL Anaplastic Large Cell Lymphoma ALL Acute Lymphoblastic Leukemia ALS Amyotrophic Lateral Sclerosis (\"Lou Gehrig's Disease\"\")\" ALZ Alzheimer's Disease AML Acute Myeloid Leukemia AMLM Acute Megakaryoblastic Leukemia ANEM Anemia ASPS Alveolar Soft Part Sarcoma AUL Acute Undifferentiated Leukemia BALL B-cell Acute Lymphoblastic Leukemia BCC Basal Cell Carcinoma BLACA Bladder Cancer BT Brain Tumor CA Carcinoma CBF Acute Myeloid Leukemia - Core Binding Factor subtype CLL Chronic Lymphocytic Leukemia CML Chronic Myelogenous Leukemia CMML Chronic Myelomonocytic Leukemia CMV Cytomegalovirus CNS Central Nervous System CPC Choroid Plexus Carcinoma CRC Colorectal Cancer CS Chondrosarcoma CTP Congenital Thrombocytopenia DIPG Diffuse Intrinsic Pontine Glioma DLBCL Diffuse Large B-cell Lymphoma DOWN Down Syndrome DSRCT Desmoplastic Small Round Cell Tumor E2A B-Lineage Acute Lymphoblastic Leukemia - E2A-PBX1 subtype ECD Erdheim-Chester Disease EPD Ependymoma ERG Acute Lymphoblastic Leukemia - ERG alteration subtype ETV Acute Lymphoblastic Leukemia - ETV6-RUNX1 fusion subtype EWS Ewing's Sarcoma GCT Germ Cell Tumor GENBN General Bone GENKY General Kidney GENLK General Leukemia GICT Giant Cell Tumor GIST Gastrointestinal Stromal Tumor HB Hepatoblastoma HGG High Grade Glioma HGS High Grade Sarcoma HIST Histiocytosis HL Hodgkin's Lymphoma HM Hematopoietic Malignancies HS Hidradenitis Suppurativa HYPER Acute Lymphoblastic Leukemia - Hyperdiploid subtype HYPO Acute Lymphoblastic Leukemia - Hypodiploid subtype IFS Infantile Fibromyosarcoma INF Acute Lymphoblastic Leukemia (Infant) ITP Idiopathic Thrombocytopenia JMML Juvenile Myelomonocytic Leukemia LCH Langerhans Cell Histiocytocis LGG Low Grade Glioma LM Liver Malignancies MB Medulloblastoma MDS Myelodysplastic Syndrome MEL Melanoma MLL Mixed Lineage Leukemia MM Multiple Myeloma MPAL Acute Lymphoblastic Leukemia - Multi-phenotypic MPNST Malignant Peripheral Nerve Sheath Tumor MRT Malignant Rhabdoid Tumour MYF Myelofibrosis NBL Neuroblastoma NEUTP Neutropenia NHL Non-Hodgkin's Lymphoma NM Non-malignancy NORM Control Sample NPC Nasopharyngeal Carcinoma NPCA Nasopharyngeal Carcinoma OS Osteosarcoma PF Posterior Fossa PGL Paraganglioma PHALL Acute Lymphoblastic Leukemia - BCR-ABL1 fusion subtype PHCML Ph+ Chronic Myeloid Leukemia PML Promyelocitic Leukemia PRAD Prostate Adenocarcoma PSO Psoriasis RB Retinoblastoma RCC Renal cell carcinoma RECA Renal Cancer RHB Rhabdomyosarcoma SBO Spina Bifida Occulta SCD Sickle Cell Disease SCZ Schizophrenia SS Synovial Sarcoma ST Solid Tumor STS Soft Tissue Sarcoma TALL T-cell Acute Lymphoblastic Leukemia TCP Thrombocytopenia TESCA Testicular Cancer THCA Thyroid Carcinoma WLM Wilms' tumor Similar Topics \u00b6 About our Decision Process & Terminology Making a Data Request Managing Data Overview","title":"About our Data"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#file-formats","text":"St. Jude Cloud hosts both raw genomic data files and processed results files: File Type Short Description Details BAM HG38 aligned BAM files produced by Microsoft Genomics Service (DNA-Seq) or STAR 2-pass mapping (RNA-Seq). Click here gVCF Genomic VCF files produced by Microsoft Genomics Service . Click here Somatic VCF Curated list of somatic variants produced by the St. Jude somatic variant analysis pipeline. Click here CNV List of somatic copy number alterations produced by St. Jude CONSERTING pipeline. Click here","title":"File Formats"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#bam-files","text":"In St. Jude Cloud, we store aligned sequence reads in BAM file format for whole genome sequencing, whole exome sequencing, and RNA-seq. For more information on SAM/BAM files, please refer to the SAM/BAM specification . For research samples, we require the standard 30X coverage for whole genome and 100X for whole exome sequencing. For clinical samples, we require higher coverage, 45X, for whole genome sequencing due to tumor purity issues found in clinical tumor specimens. For RNA-Seq, since only a subset of genes are expressed in a specific tissue, we require 30% of the exons to have 20X coverage in order to ensure that at least 30% of the expressed genes have sufficient coverage.","title":"BAM files"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#gvcf-files","text":"We provide gVCF files produced by the Microsoft Genomics Service . gVCF files are derived from the BAM files produced above as called by GATK's haplotype caller . Today, we defer to the official specification document from the Broad Institute, as well as this discussion on the difference between VCF and gVCF files. For more information about how Microsoft Genomics produces gVCF files or any other questions regarding data generation, please refer to the official Microsoft Genomics whitepaper .","title":"gVCF files"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#somatic-vcf-files","text":"Somatic VCF files contain HG38 based SNV/Indel variant calls from the St. Jude somatic variant analysis pipeline as follows. Broadly speaking: Reads were aligned to HG19 using bwa backtrack ( bwa aln + bwa sampe ) using default parameters. Post processing of aligned reads was performed using Picard CleanSam and MarkDuplicates . Variants were called using the Bambino variant caller (you can download Bambino here or by navigating to the Zhang Lab page where the \"Bambino package\" is listed as a dependency under the CONSERTING section). Variants were post-processed using an in-house post-processing pipeline that cleans and annotates variants. This pipeline is not currently publicly available. Variants were manually reviewed by analysts and published with the relevant Pediatric Cancer Genome Project (PCGP) paper . Post-publication, variants were lifted over to HG38 (the original HG19 coordinates are stored in the HG19 INFO field.). Note Our Somatic VCF files were designed specifically for St. Jude Cloud visualization purposes. Variants in these files were manually curated from analyses across multiple sequencing types including WGS and WES. For more information on variants for each of the individuals, please refer to the relevant PCGP paper. For more information on the variant calling format (VCF), please see the latest specification for VCF document listed here .","title":"Somatic VCF files"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#cnv-files","text":"CNV files contain copy number alteration (CNA) analysis results for paired tumor-normal WGS samples. Files are produced by running paired tumor-normal BAM files through the CONSERTING pipeline which identifies CNA through iterative analysis of (i) local segmentation by read depth within boundaries identified by structural variation (SV) breakpoints followed by (ii) segment merging and local SV analysis. CREST was used to identify local SV breakpoints. CNV files contain the following information: Field Description chrom chromosome loc.start start of segment loc.end end of segment num.mark number of windows retained in the segment (gaps and windows with low mappability are excluded) length.ratio The ratio between the length of the used windows to the genomic length seg.mean The estimated GC corrected difference signal (2 copy gain will have a seg.mean of 1) GMean The mean coverage in the germline sample (a value of 1 represents diploid) DMean The mean coverage in the tumor sample LogRatio Log2 ratio between tumor and normal coverage Quality score A empirical score used in merging SV_Matching Whether the boundary of the segments were supported by SVs (3: both ends supported, 2: right end supported, 1: left end supported, 0: neither end supported)","title":"CNV files"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#sequencing-information","text":"","title":"Sequencing Information"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#whole-genome-and-whole-exome","text":"Whole Genome Sequence (WGS) and Whole Exome Sequence (WES) BAM files were produced by the Microsoft Genomics Service aligned to HG38 (GRCh38 no alt analysis set). For more information about how Microsoft Genomics produces BAM files or any other questions regarding data generation, please refer to the official Microsoft Genomics whitepaper .","title":"Whole Genome and Whole Exome"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#rna-seq","text":"RNA-Seq BAM files are mapped to HG38 + ERCC Spike In Sequences (commonly used for normalization of expression analyses). For alignment, STAR v2.5.3a 2-pass mapping followed by Picard MarkDuplicates . Below is the STAR command used during alignment. For more information about any of the parameters used, please refer to the STAR manual for v2.5.3a. STAR \\ --runThreadN $NUM_THREADS \\ # $NUM_THREADS is the number of threads to parallelize the alignment across (generally we use 4). --genomeDir $GENOME_DIR \\ # $GENOME_DIR is a STAR reference directory containing HG38 and ERCC Spike In sequences. --readFilesIn $READ_FILES \\ # $READ_FILES are the input FastQ files to align. --limitBAMsortRAM $MEMORY_LIMIT \\ # $MEMORY_LIMIT is a upper limit on the amount of RAM to use in the alignment. --outFileNamePrefix $OUT_FILE_PREFIX \\ --outSAMtype BAM SortedByCoordinate \\ --outSAMstrandField intronMotif \\ --outSAMattributes NH HI AS nM NM MD XS \\ --outSAMunmapped Within \\ --outSAMattrRGline $RGs \\ # $RGs is the read group information for each FastQ passed in $READ_FILES. --outFilterMultimapNmax 20 \\ --outFilterMultimapScoreRange 1 \\ --outFilterScoreMinOverLread 0 .66 \\ --outFilterMatchNminOverLread 0 .66 \\ --outFilterMismatchNmax 10 \\ --alignIntronMax 500000 \\ --alignMatesGapMax 1000000 \\ --alignSJDBoverhangMin 1 \\ --sjdbScore 2 \\ --twopassMode Basic","title":"RNA-Seq"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#data-access-units","text":"We currently have the five Data Access Units (DAU) listed below. Basic clinical data is available for relevant subjects in each DAU. Click on the DAU's abbreviation below to navigate directly to that DAU's Study page for more detailed information.","title":"Data Access Units"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#pediatric-cancer-genome-project-pcgp","text":"PCGP is a paired-tumor normal dataset focused on discovering the genetic origins of pediatric cancer. The Pediatric Cancer Genome Project is a collaboration between St. Jude Children's Research Hospital and the McDonnell Genome Institute at Washington University School of Medicine that sequenced the genomes of over 600 pediatric cancer patients.","title":"Pediatric Cancer Genome Project (PCGP)"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#st-jude-lifetime-sjlife","text":"SJLIFE is a germline-only dataset focused on studying the long-term adverse outcomes associated with cancer and cancer-related therapy. St. Jude Lifetime (SJLIFE) is a longevity study from St. Jude Children's Research Hospital that aims to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. This cohort contains unpaired germline samples and does not contain tumor samples.","title":"St. Jude Lifetime (SJLIFE)"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#clinical-genomics-clinical-pilot-g4k-and-rtcg","text":"Clinical Genomics is a paired tumor-normal dataset focused on identifying variants that influence the development and behavior of childhood tumors. Clinical Genomics is a cohort from St. Jude Children's Research Hospital, comprised of three studies: Clinical Pilot, Genomes4Kids, and Real-time Clinical Genomics. Clinical Pilot is a smaller, pilot study generated to asses the validity and accuracy of moving forward with the G4K study. The RTCG study aims to release Clinical Genomics data in real time to the research community. The goal of these studies is to identify all inherited and tumor-acquired (somatic) genome sequence and structural variants influencing the development and behavior of childhood tumors.","title":"Clinical Genomics (Clinical Pilot, G4K, and RTCG)"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#sickle-cell-genome-project-sgp","text":"SGP is a germline-only dataset of Sickle Cell Disease (SCD) patients from birth to young adulthood. The Sickle Cell Genome Project (SGP) is a collaboration between St. Jude Children\u2019s Research Hospital and Baylor College of Medicine focused on identifying genetic modifiers that contribute to various health complications in SCD patients. Additional objectives include, but are not limited to, developing accurate methods to characterize germline structural variants in highly homologous globin locus and blood typing.","title":"Sickle Cell Genome Project (SGP)"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#childhood-cancer-survivor-study-ccss","text":"CCSS is a germline-only dataset consisting of whole genome sequencing of childhood cancer survivors. CCSS is a multi-institutional, multi-disciplinary, NCI-funded collaborative resource established to evaluate long-term outcomes among survivors of childhood cancer. It is a retrospective cohort consisting of >24,000 five-year survivors of childhood cancer who were diagnosed between 1970-1999 at one of 31 participating centers in the U.S. and Canada. The primary purpose of this sequencing of CCSS participants is to identify all inherited genome sequence and structural variants influencing the development of childhood cancer and occurrence of long-term adverse outcomes associated with cancer and cancer-related therapy. CCSS: Potential Bacterial Contamination Samples for the Childhood Cancer Survivorship Study were collected by sending out Buccal swab kits to enrolled participants and having them complete the kits at home. This mechanism of collecting saliva and buccal cells for sequencing is highly desirable because of its non-invasive nature and ease of execution. However, collection of samples in this manner also has higher probability of contamination from external sources (as compared to, say, samples collected using blood). We have observed some samples in this cohort which suffer from bacterial contamination. To address this issue, we have taken the following steps: We have estimated the bacterial contamination rate and annotated each of the samples in the CCSS cohort. For each sample, you will find the estimated contamination rate in the Description field of the SAMPLE_INFO.txt file that is vended with your data (and as a property on the DNAnexus file). For information on this field, see the Metadata specification . Using this estimated contamination rate, we have removed 82 samples which exhibited large rates of bacterial contamination. For the remaining samples, we have provided the BAM file as aligned with bwa mem with default parameters. We have observed that there are instances of reads originating from bacterial contamination that are erroneously mapped to the human genome and display a very low mapping quality. Please be advised that we have kept these reads as they were aligned and have not yet made any attempt to unmap these reads. Any analysis you perform on these samples will need to take this into account! Last, we will be working over the coming months to unmap the reads originating from bacterial contamination and release updated BAM files along with the associated gVCF files from Microsoft Genomics Service. With any questions on the nature or implications of this warning, please contact us at support@stjude.cloud .","title":"Childhood Cancer Survivor Study (CCSS)"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#metadata","text":"Each data request includes a text file called SAMPLE_INFO.txt that provides a number of file level properties (sample identifiers, clinical attributes, etc).","title":"Metadata"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#standard-metadata","text":"Below are the set of tags which may exist for any given file in St. Jude Cloud. All optional metadata will have sj_ prepended to their tag name. Property Description file_path The path to the file in your St. Jude Cloud project. subject_name A unique subject identifier assigned internally at St. Jude. sample_name A unique sample identifier assigned internally at St. Jude. sample_type One of Autopsy, Cell line, Diagnosis, Germline, Metastasis, Relapse, or Xenograft. sequencing_type Whether the file was generated from Whole Genome (WGS), Whole Exome (WES), or RNA-Seq. file_type One of the file types available in St. Jude Cloud. description Optional field that may contain additional file information. sj_diseases Short disease identifier assigned at the time of genomic sequencing. Note that this diagnosis may be refined after undergoing genomic testing. When including diagnosis in your analysis, we recommend you use attr_diagnosis , which is the most up to date value for diagnosis. sj_datasets If present, the datasets in the data browser which this file is associated with. sj_pmid_accessions If the file was associated with a paper, the related Pubmed accession number. sj_ega_accessions If the file was associated with a paper, the related EGA accession number. sj_dataset_accession If present, the permanent accession number assigned in St. Jude Cloud. sj_embargo_date The embargo date , which specifies the first date which the files can be used in a publication.","title":"Standard Metadata"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#clinical-and-phenotypic-information","text":"Also included is a set of phenotypic information queried from the physician or research team's records at the time of sample submission to St. Jude Cloud. These are all considered to be optional , as the level of information gathered for each sample varies. If empty, the physician or research team did not indicate a value for the field. All basic clinical or phenotypic information will have attr_ prepended to their tag name. Property Description attr_age_at_diagnosis Age at first diagnosis. This field is normalized as a decimal value. If empty, the physician or research team did not indicate a value for this field. attr_diagnosis Primary diagnosis reported by the clinic. attr_ethnicity Self-reported ethnicity. Values are normalized according to the US Census Bureau classifications . attr_race Self-reported race. Values are normalized according to the US Census Bureau classifications . attr_sex Self-reported sex. attr_oncotree_disease_code The disease code (assigned at the time of genomic sequencing) as specified by Oncotree Version 2019-03-01 .","title":"Clinical and Phenotypic Information"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#short-disease-code-mapping","text":"Embedded in both the filename and the SAMPLE_INFO.txt file that comes with your data request will be a list of short diagnosis codes ( sj_diseases ). These short codes were assigned at the time that the sample was sent for sequencing, and they are not necessarily the final diagnosis ( attr_diagnosis ). For instance, the diagnosis is often refined as the sample undergoes genomic testing. Below, we include a table of short disease code to long disease name mappings so you can interpret what these abbreviations mean. Short Disease Code Long Diagnosis Description ACT Adrenocortical Carcinoma AEL Acute erythroid leukemia (AML M6) ALCL Anaplastic Large Cell Lymphoma ALL Acute Lymphoblastic Leukemia ALS Amyotrophic Lateral Sclerosis (\"Lou Gehrig's Disease\"\")\" ALZ Alzheimer's Disease AML Acute Myeloid Leukemia AMLM Acute Megakaryoblastic Leukemia ANEM Anemia ASPS Alveolar Soft Part Sarcoma AUL Acute Undifferentiated Leukemia BALL B-cell Acute Lymphoblastic Leukemia BCC Basal Cell Carcinoma BLACA Bladder Cancer BT Brain Tumor CA Carcinoma CBF Acute Myeloid Leukemia - Core Binding Factor subtype CLL Chronic Lymphocytic Leukemia CML Chronic Myelogenous Leukemia CMML Chronic Myelomonocytic Leukemia CMV Cytomegalovirus CNS Central Nervous System CPC Choroid Plexus Carcinoma CRC Colorectal Cancer CS Chondrosarcoma CTP Congenital Thrombocytopenia DIPG Diffuse Intrinsic Pontine Glioma DLBCL Diffuse Large B-cell Lymphoma DOWN Down Syndrome DSRCT Desmoplastic Small Round Cell Tumor E2A B-Lineage Acute Lymphoblastic Leukemia - E2A-PBX1 subtype ECD Erdheim-Chester Disease EPD Ependymoma ERG Acute Lymphoblastic Leukemia - ERG alteration subtype ETV Acute Lymphoblastic Leukemia - ETV6-RUNX1 fusion subtype EWS Ewing's Sarcoma GCT Germ Cell Tumor GENBN General Bone GENKY General Kidney GENLK General Leukemia GICT Giant Cell Tumor GIST Gastrointestinal Stromal Tumor HB Hepatoblastoma HGG High Grade Glioma HGS High Grade Sarcoma HIST Histiocytosis HL Hodgkin's Lymphoma HM Hematopoietic Malignancies HS Hidradenitis Suppurativa HYPER Acute Lymphoblastic Leukemia - Hyperdiploid subtype HYPO Acute Lymphoblastic Leukemia - Hypodiploid subtype IFS Infantile Fibromyosarcoma INF Acute Lymphoblastic Leukemia (Infant) ITP Idiopathic Thrombocytopenia JMML Juvenile Myelomonocytic Leukemia LCH Langerhans Cell Histiocytocis LGG Low Grade Glioma LM Liver Malignancies MB Medulloblastoma MDS Myelodysplastic Syndrome MEL Melanoma MLL Mixed Lineage Leukemia MM Multiple Myeloma MPAL Acute Lymphoblastic Leukemia - Multi-phenotypic MPNST Malignant Peripheral Nerve Sheath Tumor MRT Malignant Rhabdoid Tumour MYF Myelofibrosis NBL Neuroblastoma NEUTP Neutropenia NHL Non-Hodgkin's Lymphoma NM Non-malignancy NORM Control Sample NPC Nasopharyngeal Carcinoma NPCA Nasopharyngeal Carcinoma OS Osteosarcoma PF Posterior Fossa PGL Paraganglioma PHALL Acute Lymphoblastic Leukemia - BCR-ABL1 fusion subtype PHCML Ph+ Chronic Myeloid Leukemia PML Promyelocitic Leukemia PRAD Prostate Adenocarcoma PSO Psoriasis RB Retinoblastoma RCC Renal cell carcinoma RECA Renal Cancer RHB Rhabdomyosarcoma SBO Spina Bifida Occulta SCD Sickle Cell Disease SCZ Schizophrenia SS Synovial Sarcoma ST Solid Tumor STS Soft Tissue Sarcoma TALL T-cell Acute Lymphoblastic Leukemia TCP Thrombocytopenia TESCA Testicular Cancer THCA Thyroid Carcinoma WLM Wilms' tumor","title":"Short Disease Code Mapping"},{"location":"guides/genomics-platform/requesting-data/about-our-data/#similar-topics","text":"About our Decision Process & Terminology Making a Data Request Managing Data Overview","title":"Similar Topics"},{"location":"guides/genomics-platform/requesting-data/data-request/","text":"Request Process Overview \u00b6 Creating a data request is the premier way to access raw St. Jude next generation sequencing data in the cloud. You can get a free copy of the data in a secure cloud environment powered by Microsoft Azure and DNAnexus , or you can elect to download the data to your local computing environment. Things to Remember Data in St. Jude Cloud is grouped into Data Access Units (DAUs) , which usually correspond to large-scale sequencing initiatives at St. Jude. Individuals can apply for access to DAUs on a case-by-case basis for a specific amount of time (usually 1 year). Access to data in a given DAU is assessed by the corresponding Data Access Committee who reviews a variety of factors to grant access. There are a number of terms of use and restrictions outlined in the Data Access Agreement . Everyone who will be working with the data must understand and agree to these terms. Selecting Data \u00b6 The primary way to make your data selection is through our Genomics Platform Data Browser . You can search our raw genomic data by diagnosis, publication, or study by selecting a tab along the top. You may further refine your search by applying filters from four categories: Sequencing Type, Sample Type, File Type, and Tissue Type. Please note that applying multiple filters within the same category filters using 'OR' logic while applying multiple filters across different categories filters using 'AND' logic. For example in the overview image above, we have filtered the browser to only show data that is (either WGS OR WES) AND (Diagnosis OR Relapse) AND BAM AND Paired Tumor-Normal. As you filter and make selections, the data summary panel in the upper left hand corner will update dynamically to give you important descriptive information about the set of data you have selected. Click on an empty box to make a selection; when selected, the box will turn blue with a white check mark. Once you have completed your data selection, click on Request Data to submit your request and proceed. Note You must have created an account and be logged in to submit a data request. If you have not yet created an account or you are not logged in, the submit button will say Log In rather than Request Data . Alternatively, you may be directed to the Genomics Platform Data Browser through another App to request specific samples. The PeCan homepage is one such app that allows you to select data through an interactive visualization. After clicking on Request Data , we ask that you review your selection and make sure that the DAUs corresponding to the set of data you have selected is indeed the data you want to request. Making the Request \u00b6 Now that you have selected your data, you will need to fill in some information to complete the request. From here, necessary information will be collected through a setup wizard. All of your progress will be automatically saved, and you can follow along with your progress on the left sidebar. This information will be collected whether you are requesting open-access or controlled-access data. It helps us structure your project folder correctly when we vend the data to you. Signing the Data Access Agreement \u00b6 Info If you already have access to the data or are requesting open-access data, you will not be prompted to go through this section. Every person who requests access to our controlled-access data must sign the Data Access Agreememnt (DAA) . If you are located in the United States of America, you can opt in to completing the DAA through an electronic setup wizard. If you are not located in the USA, or would like to complete the form manually, you can follow our instructions on Filling Out The Data Access Agreement . If you opt to do the process through the setup wizard, the necessary information will be collected and added automatically to your agreement. Once you have completed the setup wizard, the form will be sent to you and necessary signatories through email via DocuSign . You can learn more about our electronic data access agreement process here . Request approval typically takes a week or two if your data access agreement is correctly and completely filled out. You will receive automated emails from notifications@stjude.cloud at the time that your request is approved. Tip If you receive an email from us that your DAA is incomplete, you may edit your DAA and upload the revised copy using the 'Add a Form' button the on My Dashboard page. Managing your Data Request \u00b6 Go to our Managing Data Overview documentation page to learn how to check the status of your data request, complete an EDAA draft, upload a revised DAA, and ultimately access your data from your My Dashboard page. Info If you would like to download the data to local storage, there are extra steps you'll need to follow such as getting additional signatures on your data access agreement. We recommend that you work with the data in the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow this guide to get started. Similar Topics \u00b6 About our Data About our Decision Process & Terminology Creating an Account Managing Data Overview Renewing your Data Access","title":"Making a Data Request"},{"location":"guides/genomics-platform/requesting-data/data-request/#request-process-overview","text":"Creating a data request is the premier way to access raw St. Jude next generation sequencing data in the cloud. You can get a free copy of the data in a secure cloud environment powered by Microsoft Azure and DNAnexus , or you can elect to download the data to your local computing environment. Things to Remember Data in St. Jude Cloud is grouped into Data Access Units (DAUs) , which usually correspond to large-scale sequencing initiatives at St. Jude. Individuals can apply for access to DAUs on a case-by-case basis for a specific amount of time (usually 1 year). Access to data in a given DAU is assessed by the corresponding Data Access Committee who reviews a variety of factors to grant access. There are a number of terms of use and restrictions outlined in the Data Access Agreement . Everyone who will be working with the data must understand and agree to these terms.","title":"Request Process Overview"},{"location":"guides/genomics-platform/requesting-data/data-request/#selecting-data","text":"The primary way to make your data selection is through our Genomics Platform Data Browser . You can search our raw genomic data by diagnosis, publication, or study by selecting a tab along the top. You may further refine your search by applying filters from four categories: Sequencing Type, Sample Type, File Type, and Tissue Type. Please note that applying multiple filters within the same category filters using 'OR' logic while applying multiple filters across different categories filters using 'AND' logic. For example in the overview image above, we have filtered the browser to only show data that is (either WGS OR WES) AND (Diagnosis OR Relapse) AND BAM AND Paired Tumor-Normal. As you filter and make selections, the data summary panel in the upper left hand corner will update dynamically to give you important descriptive information about the set of data you have selected. Click on an empty box to make a selection; when selected, the box will turn blue with a white check mark. Once you have completed your data selection, click on Request Data to submit your request and proceed. Note You must have created an account and be logged in to submit a data request. If you have not yet created an account or you are not logged in, the submit button will say Log In rather than Request Data . Alternatively, you may be directed to the Genomics Platform Data Browser through another App to request specific samples. The PeCan homepage is one such app that allows you to select data through an interactive visualization. After clicking on Request Data , we ask that you review your selection and make sure that the DAUs corresponding to the set of data you have selected is indeed the data you want to request.","title":"Selecting Data"},{"location":"guides/genomics-platform/requesting-data/data-request/#making-the-request","text":"Now that you have selected your data, you will need to fill in some information to complete the request. From here, necessary information will be collected through a setup wizard. All of your progress will be automatically saved, and you can follow along with your progress on the left sidebar. This information will be collected whether you are requesting open-access or controlled-access data. It helps us structure your project folder correctly when we vend the data to you.","title":"Making the Request"},{"location":"guides/genomics-platform/requesting-data/data-request/#signing-the-data-access-agreement","text":"Info If you already have access to the data or are requesting open-access data, you will not be prompted to go through this section. Every person who requests access to our controlled-access data must sign the Data Access Agreememnt (DAA) . If you are located in the United States of America, you can opt in to completing the DAA through an electronic setup wizard. If you are not located in the USA, or would like to complete the form manually, you can follow our instructions on Filling Out The Data Access Agreement . If you opt to do the process through the setup wizard, the necessary information will be collected and added automatically to your agreement. Once you have completed the setup wizard, the form will be sent to you and necessary signatories through email via DocuSign . You can learn more about our electronic data access agreement process here . Request approval typically takes a week or two if your data access agreement is correctly and completely filled out. You will receive automated emails from notifications@stjude.cloud at the time that your request is approved. Tip If you receive an email from us that your DAA is incomplete, you may edit your DAA and upload the revised copy using the 'Add a Form' button the on My Dashboard page.","title":"Signing the Data Access Agreement"},{"location":"guides/genomics-platform/requesting-data/data-request/#managing-your-data-request","text":"Go to our Managing Data Overview documentation page to learn how to check the status of your data request, complete an EDAA draft, upload a revised DAA, and ultimately access your data from your My Dashboard page. Info If you would like to download the data to local storage, there are extra steps you'll need to follow such as getting additional signatures on your data access agreement. We recommend that you work with the data in the cloud if it's feasible; the data provided by St. Jude is free, the compute charges are reasonable, and working in the cloud helps to eliminate the long, error-prone downloading process. Porting your tools to be run in the cloud is easy, as well. We recommend you follow this guide to get started.","title":"Managing your Data Request"},{"location":"guides/genomics-platform/requesting-data/data-request/#similar-topics","text":"About our Data About our Decision Process & Terminology Creating an Account Managing Data Overview Renewing your Data Access","title":"Similar Topics"},{"location":"guides/genomics-platform/requesting-data/glossary/","text":"Data Access Unit \u00b6 A St. Jude Cloud Data Access Unit (DAU) is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. Each DAU has its own governing body of researchers, the Data Access Committee , who preside over the data and who may grant or deny access. Each Data Access Committee has its own protocols for approving access to their DAU. Please contact us if you have questions about committee approval protocols. We currently have 5 DAUs: Pediatric Cancer Genome Project (PCGP), St. Jude Lifetime Cohort Study (SJLIFE), Genomes for Kids (G4K) and Clinical Genomics, Sickle Cell Genome Project (SGP), and Childhood Cancer Survivor Study (CCSS). For a brief description of each DAU see the Studies page . For a more detailed description please see the respective Schedule 1(s) . Data Access Agreement \u00b6 A St. Jude Cloud Data Access Agreement (DAA) is a legally binding document outlining a number of terms and conditions to which anyone working with St. Jude Cloud data must agree. We do not negotiate the terms of this document unless terms are found to be in conflict with the institution's state law. Filling out the Data Access Agreement carefully and completely is crucial to having your request approved promptly. Click Here to download a copy of the DAA. Click Here for a step by step guide on how to fill out the DAA. If you have incompletely or incorrectly filled out your DAA and would like to upload a revised form, Click Here for instructions. Once you have submitted a correctly filled out DAA and have been granted access to one or more Data Access Units (DAUs) , you can continue checking out files from those DAUs until your access expires. Access is generally granted for 1 year, at which point you must submit an Extension Addendum to continue using the data. Click Here for a step-by-step guide on how to fill out the Extension Addendum. Data Access Committee \u00b6 A St. Jude Cloud Data Access Committee (DAC) is group of St. Jude researchers who oversee access to a particular Data Access Unit (DAU) and evaluate incoming data requests. The first time you request access to files in a DAU, it is required that you fill out a Data Access Agreement (DAA) . Access is granted at the DAU level based on the decision of each DAC upon reviewing the DAA. Example For example, if you make a request asking for all of St. Jude's Acute Lymphoblastic Leukemia sequencing data, you might be asking for data from multiple different projects (DAUs) here at St. Jude. For the sake of the example, let's say the data you want is spread across three different DAUs. Once you place a request, your application will be routed to the corresponding three data access committees for approval. Since each DAC is made up of different individuals using different criteria for evaluation, you may or may not be approved for access to all of the files. Embargo Date \u00b6 The Embargo Date specifies the date that a publishing embargo on the file in question has been lifted. Publishing using any of the files before the embargo date has passed is strictly prohibited as outlined in the Data Access Agreement (DAA) . Typically, samples from the same Data Access Unit (DAU) all have the same embargo date, as they would have been released on St. Jude Cloud at the same time. Current Embargo Dates Data Access Unit Embargo Date Pediatric Cancer Genome Project July 23, 2018 St. Jude LIFE January 15, 2019 Clinical Genomics Rolling based on release date Sickle Cell Genome Project September 1, 2019 Childhood Cancer Survivor Study November 1, 2019 Similar Topics \u00b6 About our Data","title":"About our Decision Process & Terminology"},{"location":"guides/genomics-platform/requesting-data/glossary/#data-access-unit","text":"A St. Jude Cloud Data Access Unit (DAU) is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. Each DAU has its own governing body of researchers, the Data Access Committee , who preside over the data and who may grant or deny access. Each Data Access Committee has its own protocols for approving access to their DAU. Please contact us if you have questions about committee approval protocols. We currently have 5 DAUs: Pediatric Cancer Genome Project (PCGP), St. Jude Lifetime Cohort Study (SJLIFE), Genomes for Kids (G4K) and Clinical Genomics, Sickle Cell Genome Project (SGP), and Childhood Cancer Survivor Study (CCSS). For a brief description of each DAU see the Studies page . For a more detailed description please see the respective Schedule 1(s) .","title":"Data Access Unit"},{"location":"guides/genomics-platform/requesting-data/glossary/#data-access-agreement","text":"A St. Jude Cloud Data Access Agreement (DAA) is a legally binding document outlining a number of terms and conditions to which anyone working with St. Jude Cloud data must agree. We do not negotiate the terms of this document unless terms are found to be in conflict with the institution's state law. Filling out the Data Access Agreement carefully and completely is crucial to having your request approved promptly. Click Here to download a copy of the DAA. Click Here for a step by step guide on how to fill out the DAA. If you have incompletely or incorrectly filled out your DAA and would like to upload a revised form, Click Here for instructions. Once you have submitted a correctly filled out DAA and have been granted access to one or more Data Access Units (DAUs) , you can continue checking out files from those DAUs until your access expires. Access is generally granted for 1 year, at which point you must submit an Extension Addendum to continue using the data. Click Here for a step-by-step guide on how to fill out the Extension Addendum.","title":"Data Access Agreement"},{"location":"guides/genomics-platform/requesting-data/glossary/#data-access-committee","text":"A St. Jude Cloud Data Access Committee (DAC) is group of St. Jude researchers who oversee access to a particular Data Access Unit (DAU) and evaluate incoming data requests. The first time you request access to files in a DAU, it is required that you fill out a Data Access Agreement (DAA) . Access is granted at the DAU level based on the decision of each DAC upon reviewing the DAA. Example For example, if you make a request asking for all of St. Jude's Acute Lymphoblastic Leukemia sequencing data, you might be asking for data from multiple different projects (DAUs) here at St. Jude. For the sake of the example, let's say the data you want is spread across three different DAUs. Once you place a request, your application will be routed to the corresponding three data access committees for approval. Since each DAC is made up of different individuals using different criteria for evaluation, you may or may not be approved for access to all of the files.","title":"Data Access Committee"},{"location":"guides/genomics-platform/requesting-data/glossary/#embargo-date","text":"The Embargo Date specifies the date that a publishing embargo on the file in question has been lifted. Publishing using any of the files before the embargo date has passed is strictly prohibited as outlined in the Data Access Agreement (DAA) . Typically, samples from the same Data Access Unit (DAU) all have the same embargo date, as they would have been released on St. Jude Cloud at the same time. Current Embargo Dates Data Access Unit Embargo Date Pediatric Cancer Genome Project July 23, 2018 St. Jude LIFE January 15, 2019 Clinical Genomics Rolling based on release date Sickle Cell Genome Project September 1, 2019 Childhood Cancer Survivor Study November 1, 2019","title":"Embargo Date"},{"location":"guides/genomics-platform/requesting-data/glossary/#similar-topics","text":"About our Data","title":"Similar Topics"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/","text":"Filling Out The Data Access Agreement The Data Access Agreement (DAA) is a legal document used by St. Jude Cloud to verify the identity and intent of those requesting to access St. Jude Children\u2019s Research Hospital\u2019s genomics data. The document binds you and your institution in agreement to protect, use and share the data appropriately. Upon selection of your desired data, you will be prompted to complete a Data Access Agreement if you have not already been approved for access to the selected datasets. In order to simplify the data access request process, an electronic data access agreement is available for US residents only. If you reside outside of the US, you must fill out the Data Access Agreement manually. You may click here to download the latest version of the DAA. Please read the first 6 pages carefully, which consist of terms and conditions that you and your institution must agree to in order to access any data on St. Jude Cloud. Then, follow the directions starting at Data Access Units to ensure that you have correctly filled out the DAA. Please note that there are two additional required sections if you intend to download data to your local infrastructure. The Data Access Agreement \u00b6 Downloading Data \u00b6 If and only if you wish to download the genomic data locally, you must have the Principal Investigator initial on page 7 and have the Information Security Officer sign on page 13. If filling out the Data Access Agreement electronically, you must select the option to download within the setup wizard and input the contact information of your institution\u2019s Information Security Officer. Data Access Units \u00b6 A St. Jude Cloud Data Access Unit (DAU) is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. To learn more, see our section on Data Access Units . On page 7 of the DAA, you must mark all Data Access Units for which you are applying. The DAU(s) associated to the data you requested are listed in the Controlled Access Data section, directly above the Download Data Access Agreement button. This can be found on the Request Data webpage which immediately follows your selection of data from the data browser. If you mark the incorrect datasets, you will be required to resubmit your agreement with the correct datasets marked. When completing the DAA electronically using the setup wizard, the DAU options will be automatically preselected for you. Contemplated Use \u00b6 On page 8 of the DAA, you must submit a description of your research project. Please specifically describe the intended role of St. Jude\u2019s data in your research project. Your contemplated use can be anywhere from a paragraph to a few pages long, although a typical contemplated use is 1-2 paragraphs. Each Data Access Committee will evaluate your contemplated use case and decide whether to approve or deny your application for data access based on their own set of protocols. Please contact us if you have any questions regarding the protocols of the approval process. Contemplated Use Example \u00b6 The below is a simulated example of a contemplated use. \"We propose to apply our own variant calling method to detect structural variants in both pediatric tumor and germline samples. The detected variants will then be compared to our own genomic data and to publicly available data sources of normal and disease samples to assess the novelty of those variants and their association with disease. Variants will be categorized by their expected effect on genes known to be relevant to cancer, DNA repair, or epigenetics. One goal of the research is to compare our mutation detection software to existing methods on samples with known mutations and structural variants such as those in the PCGP data set. Another goal is the identification of novel variants with potential clinical relevance. Promising tractable variants will be introduced into cell lines to observe their effect on cell growth and tumor progression. Lastly, we propose to use the results of our analyses to further develop and refine our variant detection methods.\" Principal Investigator \u00b6 On page 9 of the DAA, the Principal Investigator of the research project must input their information and sign the agreement. Typically the PI signee is the faculty-level supervisor on the project, but it is not a requirement. Who may qualify as a Principal Investigator (PI)? The PI is designated by the grantee organization to direct the project or activity being supported by the grant. The PI is responsible and accountable to the grantee for the proper conduct of the project or activity. The role of the PI within the eRA Commons is to complete the grant process, either by completing the required forms via the eRA Commons or by delegating this responsibility to another individual. A PI can access information for any grant for which they are designated the PI. See eRA Commons Roles & Privileges Matrix. Additional Applicants \u00b6 Pages 10 and 11 of the DAA must be signed by any additional person(s) who will have access to the data. Additional applicants may include those working on your project, those working in your lab, or those who have access to where the data will be stored. These individuals will be legally bound to protecting and handling the data properly. Pages 10 and 11 may be duplicated and added to the agreement to accommodate for more than 8 additional applicants. When filling out the DAA electronically, you may only include up to 8 additional applicants on your agreement. Institutional Authority \u00b6 Page 12 of the agreement must be filled out and signed by your Institutional or Administrative Authority. The institutional authority is an individual who has the authority to sign for a grant application. This individual cannot be the same as the Principal Investigator that signed on page 9, as this additional signature provides a second-party authority of the institution to ensure that the institution will uphold the terms of this agreement. Information Security Officer \u00b6 On page 13 of the DAA, your institution\u2019s Information Security Officer's signature is required if and only if you intend to download a local copy of the data (note: you must also initial in the line below the DAU selection). This individual may go by varying job titles, such as Information Director or Chief Information Security Officer, but is the individual responsible for information security at your institution. This signature verifies that the data, once downloaded, will remain protected by your institution's data security protocols. Uploading A Revised DAA \u00b6 If your DAA is incomplete (for example you missed a required signature or neglected to check the box next to a dataset for which you requested data), you will be contacted by a member of the St. Jude Cloud team. Once you have made the required edits, you can reupload a revised DAA through the Manage Data page by clicking on the Add a Form button. The Electronic Data Access Agreement Process \u00b6 Users who live in the United States will be given the option to complete the Data Access Agreement electronically or manually. If you elect to complete it electronically, the setup wizard will request information about you, your institution and your associates. You will also be asked to provide a detailed description of the research project in which the data you have requested will be used. After submitting the required information, the Data Access Agreement will be sent via email to each individual entered through the setup wizard. Each individual must follow the link in their email to sign the appropriate page of the agreement via DocuSign. If any individual rejects or declines to sign the agreement, you will need to start a new data request from the data browser. Once you start the Electronic Data Access Agreement process, you will have a draft autosaved for you on your My Dashboard page, accessible at any time. Learn how to check your Request Status Once all signatures have been collected, your data access request will be submitted to St. Jude Cloud. A St. Jude Cloud administrator will forward your Data Access Agreement and intended use to the appropriate Data Access Committee(s) for approval. Upon approval from the DAC(s), you will receive an email with a link to the approved data, which will be hosted through DNAnexus. Similar Topics \u00b6 Studies Making a Data Request Renewing your Data Access Managing Data Overview","title":"Filling Out a Data Access Agreement"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#the-data-access-agreement","text":"","title":"The Data Access Agreement"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#downloading-data","text":"If and only if you wish to download the genomic data locally, you must have the Principal Investigator initial on page 7 and have the Information Security Officer sign on page 13. If filling out the Data Access Agreement electronically, you must select the option to download within the setup wizard and input the contact information of your institution\u2019s Information Security Officer.","title":"Downloading Data"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#data-access-units","text":"A St. Jude Cloud Data Access Unit (DAU) is a grouping of data that typically corresponds to a project, study, or dataset generated at the same time at the same institution. To learn more, see our section on Data Access Units . On page 7 of the DAA, you must mark all Data Access Units for which you are applying. The DAU(s) associated to the data you requested are listed in the Controlled Access Data section, directly above the Download Data Access Agreement button. This can be found on the Request Data webpage which immediately follows your selection of data from the data browser. If you mark the incorrect datasets, you will be required to resubmit your agreement with the correct datasets marked. When completing the DAA electronically using the setup wizard, the DAU options will be automatically preselected for you.","title":"Data Access Units"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#contemplated-use","text":"On page 8 of the DAA, you must submit a description of your research project. Please specifically describe the intended role of St. Jude\u2019s data in your research project. Your contemplated use can be anywhere from a paragraph to a few pages long, although a typical contemplated use is 1-2 paragraphs. Each Data Access Committee will evaluate your contemplated use case and decide whether to approve or deny your application for data access based on their own set of protocols. Please contact us if you have any questions regarding the protocols of the approval process.","title":"Contemplated Use"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#contemplated-use-example","text":"The below is a simulated example of a contemplated use. \"We propose to apply our own variant calling method to detect structural variants in both pediatric tumor and germline samples. The detected variants will then be compared to our own genomic data and to publicly available data sources of normal and disease samples to assess the novelty of those variants and their association with disease. Variants will be categorized by their expected effect on genes known to be relevant to cancer, DNA repair, or epigenetics. One goal of the research is to compare our mutation detection software to existing methods on samples with known mutations and structural variants such as those in the PCGP data set. Another goal is the identification of novel variants with potential clinical relevance. Promising tractable variants will be introduced into cell lines to observe their effect on cell growth and tumor progression. Lastly, we propose to use the results of our analyses to further develop and refine our variant detection methods.\"","title":"Contemplated Use Example"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#principal-investigator","text":"On page 9 of the DAA, the Principal Investigator of the research project must input their information and sign the agreement. Typically the PI signee is the faculty-level supervisor on the project, but it is not a requirement. Who may qualify as a Principal Investigator (PI)? The PI is designated by the grantee organization to direct the project or activity being supported by the grant. The PI is responsible and accountable to the grantee for the proper conduct of the project or activity. The role of the PI within the eRA Commons is to complete the grant process, either by completing the required forms via the eRA Commons or by delegating this responsibility to another individual. A PI can access information for any grant for which they are designated the PI. See eRA Commons Roles & Privileges Matrix.","title":"Principal Investigator"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#additional-applicants","text":"Pages 10 and 11 of the DAA must be signed by any additional person(s) who will have access to the data. Additional applicants may include those working on your project, those working in your lab, or those who have access to where the data will be stored. These individuals will be legally bound to protecting and handling the data properly. Pages 10 and 11 may be duplicated and added to the agreement to accommodate for more than 8 additional applicants. When filling out the DAA electronically, you may only include up to 8 additional applicants on your agreement.","title":"Additional Applicants"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#institutional-authority","text":"Page 12 of the agreement must be filled out and signed by your Institutional or Administrative Authority. The institutional authority is an individual who has the authority to sign for a grant application. This individual cannot be the same as the Principal Investigator that signed on page 9, as this additional signature provides a second-party authority of the institution to ensure that the institution will uphold the terms of this agreement.","title":"Institutional Authority"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#information-security-officer","text":"On page 13 of the DAA, your institution\u2019s Information Security Officer's signature is required if and only if you intend to download a local copy of the data (note: you must also initial in the line below the DAU selection). This individual may go by varying job titles, such as Information Director or Chief Information Security Officer, but is the individual responsible for information security at your institution. This signature verifies that the data, once downloaded, will remain protected by your institution's data security protocols.","title":"Information Security Officer"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#uploading-a-revised-daa","text":"If your DAA is incomplete (for example you missed a required signature or neglected to check the box next to a dataset for which you requested data), you will be contacted by a member of the St. Jude Cloud team. Once you have made the required edits, you can reupload a revised DAA through the Manage Data page by clicking on the Add a Form button.","title":"Uploading A Revised DAA"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#the-electronic-data-access-agreement-process","text":"Users who live in the United States will be given the option to complete the Data Access Agreement electronically or manually. If you elect to complete it electronically, the setup wizard will request information about you, your institution and your associates. You will also be asked to provide a detailed description of the research project in which the data you have requested will be used. After submitting the required information, the Data Access Agreement will be sent via email to each individual entered through the setup wizard. Each individual must follow the link in their email to sign the appropriate page of the agreement via DocuSign. If any individual rejects or declines to sign the agreement, you will need to start a new data request from the data browser. Once you start the Electronic Data Access Agreement process, you will have a draft autosaved for you on your My Dashboard page, accessible at any time. Learn how to check your Request Status Once all signatures have been collected, your data access request will be submitted to St. Jude Cloud. A St. Jude Cloud administrator will forward your Data Access Agreement and intended use to the appropriate Data Access Committee(s) for approval. Upon approval from the DAC(s), you will receive an email with a link to the approved data, which will be hosted through DNAnexus.","title":"The Electronic Data Access Agreement Process"},{"location":"guides/genomics-platform/requesting-data/how-to-fill-out-DAA/#similar-topics","text":"Studies Making a Data Request Renewing your Data Access Managing Data Overview","title":"Similar Topics"},{"location":"guides/model-systems/model-systems/cstn/","text":"CSTN","title":"CSTN"},{"location":"guides/model-systems/model-systems/pbtp/","text":"PBTP","title":"PBTP"},{"location":"guides/pecan/","text":"PeCan PeCan provides interactive visualizations of pediatric cancer mutations across various projects at St. Jude Children's Research Hospital and its collaborating institutions. Homepage \u00b6 The PeCan homepage contains two main visualizations that work with each other to give a high level overview of the data being presented (SJ Cloud's PCGP dataset along with curated datasets from other institutions such as TARGET , dkfz , and others). Donut Chart \u00b6 The donut chart (shown below) gives an at-a-glance disease distribution and disease hierarchy. You can hover over the various donut slices to glance at the number (and %) of samples being represented by that disease. The diseases are categorized in two three main root categories: 1) HM -Hematopoietic Malignancies, 2) BT -Brain Tumor, 3) ST -Solid Tumor. Click here for a full mapping of disease codes. Bubble Chart \u00b6 Any slice (at any level) of the donut chart can be clicked on to select it, and reveal a bubble chart of related genes. Note that the dataset bar (shown below) on top of the bubble chart visualizes the distribution of selected data across the datasets used in this visualization. It will update dynamically as you interact with the donut chart and make different selections. An example of the bubble chart is shown below. You can see the selected disease shown at the top (1). The bubbles represent the most prevalent genes in the selected disease sample set. The size of the bubble corresponds to the number of mutations in the set with that gene. For some disease sets (like the one shown above), we have identified the most important disease pathway for the gene and have categorized them as such. This information is represented here via the use of colors. The legend at the bottom allows you to view the pathway information being shown (including the number of genes that are attached to each pathway). Hovering over a pathway in the legend will highlight all matching genes. Clicking a gene will open it's ProteinPaint view. To learn more about the software behind ProteinPaint, visit the ProteinPaint documentation . Requesting Raw Genomics through PeCan \u00b6 Add samples to your cart by diagnosis. Add samples to your cart by gene mutation. Add samples to your cart by gene expression. Clicking Submit to SJCloud from the PeCan checkout window will land you back in the Data Browser with your checked out data selected.","title":"Getting Started"},{"location":"guides/pecan/#homepage","text":"The PeCan homepage contains two main visualizations that work with each other to give a high level overview of the data being presented (SJ Cloud's PCGP dataset along with curated datasets from other institutions such as TARGET , dkfz , and others).","title":"Homepage"},{"location":"guides/pecan/#donut-chart","text":"The donut chart (shown below) gives an at-a-glance disease distribution and disease hierarchy. You can hover over the various donut slices to glance at the number (and %) of samples being represented by that disease. The diseases are categorized in two three main root categories: 1) HM -Hematopoietic Malignancies, 2) BT -Brain Tumor, 3) ST -Solid Tumor. Click here for a full mapping of disease codes.","title":"Donut Chart"},{"location":"guides/pecan/#bubble-chart","text":"Any slice (at any level) of the donut chart can be clicked on to select it, and reveal a bubble chart of related genes. Note that the dataset bar (shown below) on top of the bubble chart visualizes the distribution of selected data across the datasets used in this visualization. It will update dynamically as you interact with the donut chart and make different selections. An example of the bubble chart is shown below. You can see the selected disease shown at the top (1). The bubbles represent the most prevalent genes in the selected disease sample set. The size of the bubble corresponds to the number of mutations in the set with that gene. For some disease sets (like the one shown above), we have identified the most important disease pathway for the gene and have categorized them as such. This information is represented here via the use of colors. The legend at the bottom allows you to view the pathway information being shown (including the number of genes that are attached to each pathway). Hovering over a pathway in the legend will highlight all matching genes. Clicking a gene will open it's ProteinPaint view. To learn more about the software behind ProteinPaint, visit the ProteinPaint documentation .","title":"Bubble Chart"},{"location":"guides/pecan/#requesting-raw-genomics-through-pecan","text":"Add samples to your cart by diagnosis. Add samples to your cart by gene mutation. Add samples to your cart by gene expression. Clicking Submit to SJCloud from the PeCan checkout window will land you back in the Data Browser with your checked out data selected.","title":"Requesting Raw Genomics through PeCan"},{"location":"guides/pecan/faq/","text":"Frequently Asked Questions Will I be charged for using St. Jude Cloud PeCan? Will St. Jude Cloud host my institution's data in the data browser or on PeCan? Will I be charged for using St. Jude Cloud PeCan? \u00b6 You will not incur any costs for using PeCan. Will St. Jude Cloud host my institution's data in the data browser or on PeCan? \u00b6 If you are interested in submitting data to St. Jude Cloud, please contact us at support@stjude.cloud","title":"Frequently Asked Questions"},{"location":"guides/pecan/faq/#will-i-be-charged-for-using-st-jude-cloud-pecan","text":"You will not incur any costs for using PeCan.","title":"Will I be charged for using St. Jude Cloud PeCan?"},{"location":"guides/pecan/faq/#will-st-jude-cloud-host-my-institutions-data-in-the-data-browser-or-on-pecan","text":"If you are interested in submitting data to St. Jude Cloud, please contact us at support@stjude.cloud","title":"Will St. Jude Cloud host my institution's data in the data browser or on PeCan?"},{"location":"guides/pecan/pecan-pie/","text":"Pecan PIE Authors Michael Edmonson, Aman Patel Publication Edmonson et al., Genome Research 2019 Technical Support Contact Us Pecan PIE (the **Pe**diatric **Can**cer Variant **P**athogenicity **I**nformation **E**xchange) is a cloud-based variant classification and interpretation service. It annotates and ranks variants by putative pathogenicity, then displays them in an interactive web interface for formal review and classification following ACMG guidelines . The portal also contains a repository of expert-reviewed germline mutations that may predispose individuals to cancer. It is free for non-commercial use. Pecan PIE utilizes St. Jude Medal Ceremony, the same pipeline that powers our clinical and research genomics projects. Medal Ceremony provides a 3-level ranking of putative pathogenicity - Gold, Silver or Bronze - for mutations within disease-related genes. Medal assignment is based on matches to 22 mutation databases, mutation type, population frequency, tumor suppressor status and predicted functional impact. The evidence used for medal assignment is imported into an interactive variant review page where an analyst can enter additional curated information such as primary diagnosis, presence of subsequent neoplasm, family history and related literature. Classification tags can be assigned to curated data enabling automated calculation of pathogenicity rating based on ACMG/AMP 2015 guidelines. See file_download PowerPoint slides presented at the ASHG 2017 annual meeting (note that some of this information is out of date, various improvements have been made since then). Go to https://pecan.stjude.cloud/pie to get started! Overview \u00b6 An overview of the Pecan PIE workflow: Log in and upload a VCF of SNVs and indels. The portal will process your variants, notifying you upon completion. Variants are annotated with VEP+ (VEP with postprocessing for enhanced splice variant calling) then classified with Medal Ceremony. Browse results, which include a detailed page for each variation. Variants may be formally classified with an interface based on ACMG guidelines. Getting started \u00b6 Start by logging into the portal with a DNAnexus account, creating an account if you need one. PIE uses DNAnexus as a secure cloud backend. Logging in is required for private storage of your data and so that we can send you e-mail notifications when your analysis jobs are complete. PIE is free for non-commercial use. St. Jude pays the (small) cloud computing costs, your DNAnexus account will not be billed. Uploading data \u00b6 Pecan PIE takes standard VCF files as input, which may be either uncompressed or compressed with bgzip . Click the \"Securely upload a VCF file\" button. Choose the genome your variants were mapped to, which may be either GRCh37-lite or GRCh38. Advanced options \u00b6 The \"Advanced option\" panel lets you customize the behavior of the pipeline: Gene list: Pick a gene list from the pulldown. This filters your variants to genes in the specified list. This option is required and turned on automatically if your uploaded file is 2 megabytes or larger. See the frequently asked questions for more information. This option reduces the variant processing burden on PIE by removing variants that will not be assigned a medal in any case because they are not on the cancer predisposition gene list. You can review the genes by clicking on the link that will appear just below the pull down titled \"See gene list\". Custom gene list: Choosing \"custom\" as your gene list will open a window that will let you paste in a list of genes. Any invalid genes will be dropped from your list automatically. You can separate your genes by spaces or new lines. Max Population frequency: PIE by default will not call medals for variants present in the ExAC (ex-TCGA) database at an allele frequency greater than 0.001. This option lets you override the filtering threshold to whatever frequency you prefer. To disable filtering altogether, specify a value of 1. Progress page \u00b6 After uploading is complete you will be taken to a status screen showing the progress of your job through the system. Analysis typically takes 10-15 minutes depending on file size and system availability. It isn't necessary to keep your browser open on this page until your results are ready: the system will e-mail you with a link to return to your results. Optional browser notifications are also available. Analysis of Results \u00b6 Results browser \u00b6 When your job is complete you will be taken to an overview page where you can browse your results and examine a detailed results page for each variant. The variants in the results can be filtered by: Filter Meaning Class Predicted effect of variant on protein coding, e.g. missense, nonsense, etc. Somatic medal Medal assigned to the variant by the somatic classifier. Germline medal Medal assigned to the variant by the germline classifier. Committee Classification If the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank. The \"search\" box lets you filter the results by gene and/or amino acid change. The view is dynamically filtered to matching variants as you type. Medal meaning \u00b6 Medals are only assigned for coding and splice-related variants in disease predisposition genes. Germline medals are only assigned for novel variants or those present in the ExAC (ex-TCGA) database with a MAF no greater than 0.1% (0.001 expressed fractionally). Gold medals are assigned to truncations in tumor suppressor genes, hotspots derived from the COSMIC database, as well as perfect matches to variants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases. Silver medals are assigned to in-frame indels, truncations in non-tumor suppressor genes, variants predicted deleterious by damage-prediction algorithms, variants receiving a gold medal from the somatic classifier, and perfect matches to variants in the following databases: ClinVar (predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD. Bronze medals are assigned to variants predicted tolerated by damage-prediction algorithms. Variants having an imperfect match to a database (i.e. different variants at the same genomic position or codon) typically receive a lesser medal. A summary graphic can be found in slide 4 of the ASHG 2017 presentation ( download here ). For additional details see Zhang et al., NEJM 2015 (supplementary appendix pp. 7-10). Variant page \u00b6 Each variant links to a detailed variant page, which integrates data from a variety of sources. If either you or the St. Jude germline variant review committee have annotated a variant, that information will be pre-populated. Summary information The top of the page shows a summary of the variant, including its genomic and HGVS annotations, predicted effect on the protein, and somatic and germline medals. A description of the gene from Entrez follows, and a custom description or selection rationale may also be entered. Medal call information Clicking on one of the medal icons (gold, silver, bronze, unknown) or on the top of the page will show a summary of information related to the medal call. ProteinPaint An embedded version of ProteinPaint ( Zhou et al., Nat. Genet. 2016 ) appears next, showing the variant in the context of a number of pediatric datasets including PCGP. A link is provided to the main ProteinPaint application which provides visualizations for additional datasets, including COSMIC and ClinVar. ASHG pathogenicity classification Formal variant pathogenicity classification is supported by an interface implementing ACMG guidelines ( Richards et al., Genet Med. 2015 ). The analyst reviews a series of curated category tags, assigning applicable tags to the variant and optionally supplying additional information for each such as PubMed IDs and supporting evidence. The system will then compute an appropriate pathogenicity score based on the user-flagged categories. Additional free-form custom evidence can also be entered. This structured approach both helps eliminate arbitrary decision-making from the pathogenicity classification process and also constructs a concise summary of the logic and evidence supporting the final call. ClinVar and allele frequency Matches of the variant in ClinVar are also provided, along with predicted clinical significance and review status. Allele frequencies for the variant in the PCGP (somatic and germline), NHLBI ESP 6500, and ExAC databases are presented both as fractional values and on a log10 plot. Detailed allele population breakdowns are provided for ExAC. Damage prediction algorithms Precomputed damage-prediction algorithm calls for nonsynonymous coding SNVs are presented from the dbNSFP database. Available algorithms are PolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT. The calls are presented in a circular diagram with entries color-coded based on the predicted severity of the result. Medal ceremony and linkouts Additional output from medal ceremony classification can also be reviewed. This is only loosely structured, additional fields here may eventually be integrated into Pecan PIE. Links are provided to relevant dbSNP entries and other information sources. Final classification The final 5-tier ACMG classification can be selected after which the decision will be marked as reviewed. A checkbox is also available to indicate this variant is a potential candidate for functional review. Standalone usage \u00b6 This section is intended only for users who want to invoke Pecan PIE's underlying analysis pipelines independently on the DNAnexus platform. If you just want to use the Pecan PIE website you can safely ignore this section of the documentation. We assume familiarity with the DNAnexus platform. If you aren't familiar with this, DNAnexus' quickstart guide is a great place to start. Warning This section of the guide is only relevant to power users! Two DNAnexus cloud application pipelines were created during the development of Pecan PIE: Name Corresponding DNAnexus App Description VEP+ app-stjude_vep_plus A cloud installation of VEP with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format. Medal Ceremony app-stjude_medal_ceremony Additional annotation and automated variant classification. Requires a special input format which is produced by VEP+. Permissions \u00b6 In order to run the cloud pipelines independently, your DNAnexus account needs to be granted permissions to access them. After your initial login to St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single login is required even if you just want to use the standalone pipelines rather than the Pecan PIE portal ( contact us if you encounter problems accessing the pipelines). There are two methods of running pipelines on DNAnexus: DNAnexus GUI. DNAnexus provides a standardized graphical user interface for configurating, launching, and monitoring jobs on the cloud. Our pipelines can be run like any other DNAnexus pipeline. Command line. Jobs may also be invoked via the dx command line client. Command-line use allows submitting cloud jobs without interacting with a GUI, and so supports scripting and easier integration with local workflows. See this section for information on how to get set up with the dx-toolkit . Note The following examples demonstrate command-line usage. Uploading files \u00b6 All input files must be uploaded onto the DNAnexus platform. When specifying files for input you can use either the DNAnexus fie IDs (e.g. file-FBgvp680gz1bGQ5p8yZKz69g ), or the filenames if they are unique. For an idea of how to upload files to DNAnexus, see this guide . Step 1: Running VEP+ \u00b6 To run the VEP+ DNAnexus app, you can use the following dx command with your own inputs in place of the example's: dx run app-stjude_vep_plus -iinput_file = my_vcf.vcf -igenome_string = GRCh37-lite -igermline_reviewable_only = true Tip genome_string must be either GRCh37-lite or GRCh38 . If GRCh38 is specified, variants will be lifted over to GRCh37-lite in output, i.e. the output will always be GRCh37-lite (Medal Ceremony currently only supports GRCh37-lite ). The input VCF specified by input_file may be either uncompressed, or compressed with bgzip only (htslib/tabix packages). The germline_reviewable_only parameter is optional, but strongly recommended. If specified, only variants in disease-gene related intervals will be annotated, which is appropriate for Medal Ceremony. If this option is not specified all variants will be annotated, which depending on the size of your VCF might take a lot longer, and many of the resulting variants won't be usable by Medal Ceremony. If you want to do this anyway and have a large number of variants, consider submitting your job to an instance with more CPU cores (e.g. mem1_ssd1_x16 or mem1_ssd1_x32 ) as the code will take advantage of the additional cores. If you are using a custom gene list (below) that takes precedence and this parameter is not needed. The optional parameter custom_genes_file specifies a plain text file of HUGO gene symbols to analyze (whitespace separated, or one per line). If specified, analysis will be restricted to these genes only. This pipeline produces two output files, output_file contains annotations for all variants, while medal_prep_output_file is the specially-filtered and formatted file required as input to Medal Ceremony below. Step 2: Running Medal Ceremony \u00b6 To run the medal ceremony DNAnexus app, you can use the following dx command with your own inputs in place of the example's: dx run app-stjude_medal_ceremony -iinfile = medal_prep_output_file Tip The optional parameter custom_genes_file operates in the same way as in the VEP+ pipeline above. For custom gene lists to work properly this parameter must be specified when running both the VEP+ and Medal Ceremony pipelines. The optional parameter max_population_frequency may be specified, a fractional value representing the maximum population frequency allowed for a variant in the ExAC (ex-TCGA) database to receive a medal. The default is 0.001, a.k.a. \".1%\". Frequently asked questions \u00b6 If you have any questions not covered here, feel free to reach out on our contact form . Q: Which files are supported? PIE works with variants in VCF format: Uploaded files must be compliant with the VCF specification . VCF files may be either uncompressed, or compressed with bgzip only . bgzip is part of the htslib/tabix packages (see below). Improperly formatted VCF files will not work with PIE. Some common problems include: Missing header line Missing required columns Files were compressed by gzip, zip, or any method other than the required bgzip To verify compatibility of your VCF you can try one of these methods: Compressing your VCF with bgzip and indexing it with tabix , both programs from the HTSlib package (some systems also use the earlier, pre-HTSlib \"tabix\" package). This process will only succeed for compliant VCF files, and can help diagnose failures. Running \"vcf-validator\" program from the vcftools package. While the VCF specification also requires that variants be sorted by chromosome name and position, PIE is now often able to automatically correct sorting issues in uploaded files. PIE requires sorted data in order to query data for targeted genes. Q: Are there limits on the size of VCF files? Uploaded files must not exceed 4 gigabytes. If an uploaded file is larger than 2 megabytes, the cancer predisposition gene list filter will be automatically enabled unless you are using a custom gene list. This reduces the processing burden on the system by removing variants outside of targeted genes. Q: Is there an example/demo VCF I can try with PIE? A. You can use this VCF from the Genome in a Bottle project. This ~133 megabyte bgzip-compressed VCF was used during testing of Pecan PIE and is known to work. These variants are mapped to GRCh37. Q. What genome versions are supported? A. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38. GRCh38 variants are automatically lifted over to 37, as the system uses 37 internally. The liftover process is able to compensate for strand and reference/variant allele swaps which can occur. A native hg38 version is in development, but is not yet available. Pecan PIE only works for human data. Q. What genes are on the curated gene list? A. The list consists of disease-related genes, both cancer and non-cancer, see the file_download Excel spreadsheet for details. Filtering the source variants to a target list of genes reduces the processing burden on the system. When browsing the results the view may be filtered to disease sub-categories of interest. You can also specify your own custom list of genes to process when submitting your VCF file (see the advanced options panel). Q. Why is the classification column blank in my results? Q. This column displays the classification assigned by the St. Jude Germline Committee reviewers. If a variant was not classified by this committee before, this field will be blank. Pecan PIE provides classifications from the Medal Ceremony pipeline, which may assign variants gold, silver, or bronze medals. An \"Unknown\" medal may be assigned for non-disease-predisposition genes, variants present in the ExAC (ex-TCGA) database at an allele frequency > 0.1%, or variants without functional annotations (which includes most silent variants). Q. What do the medals mean? A. The medal column is a rough indicator of the likelihood of the variant being clinically significant as predicted by the medal ceremony software. Variants with gold medals are most likely to be significant, and those with no medal are least likely. More details can be found in the Analysis of Results <results> section. Q. Why are some of my variants missing? A. Currently only coding and splice-related variants in disease-related genes make it to the medaling process. Intergenic, intronic, and UTR variants are excluded, as are those in non-coding transcripts. Q. Why does the ExAC allele frequency shown differ from the ExAC portal? A. The reported ExAC frequency may differ for several reasons: PIE uses the TCGA-subtracted distribution of ExAC rather than the main distribution. PIE reports the primary allele frequencies in the ExAC database, specifically the AC, AN, and AF fields from the VCF distribution. The ExAC portal appears to use the \"adjusted\" frequencies which may be different. Q. Is Pecan PIE free? A. Pecan PIE is free for non-commercial use. St. Jude covers the cost of running the pipeline and hosting. DNANexus accounts are required to keep track of your jobs in the cloud so that you can retrieve and manage from multiple locations. Accounts also make it possible to alert you of job completion via email.","title":"Pecan PIE"},{"location":"guides/pecan/pecan-pie/#overview","text":"An overview of the Pecan PIE workflow: Log in and upload a VCF of SNVs and indels. The portal will process your variants, notifying you upon completion. Variants are annotated with VEP+ (VEP with postprocessing for enhanced splice variant calling) then classified with Medal Ceremony. Browse results, which include a detailed page for each variation. Variants may be formally classified with an interface based on ACMG guidelines.","title":"Overview"},{"location":"guides/pecan/pecan-pie/#getting-started","text":"Start by logging into the portal with a DNAnexus account, creating an account if you need one. PIE uses DNAnexus as a secure cloud backend. Logging in is required for private storage of your data and so that we can send you e-mail notifications when your analysis jobs are complete. PIE is free for non-commercial use. St. Jude pays the (small) cloud computing costs, your DNAnexus account will not be billed.","title":"Getting started"},{"location":"guides/pecan/pecan-pie/#uploading-data","text":"Pecan PIE takes standard VCF files as input, which may be either uncompressed or compressed with bgzip . Click the \"Securely upload a VCF file\" button. Choose the genome your variants were mapped to, which may be either GRCh37-lite or GRCh38.","title":"Uploading data"},{"location":"guides/pecan/pecan-pie/#advanced-options","text":"The \"Advanced option\" panel lets you customize the behavior of the pipeline: Gene list: Pick a gene list from the pulldown. This filters your variants to genes in the specified list. This option is required and turned on automatically if your uploaded file is 2 megabytes or larger. See the frequently asked questions for more information. This option reduces the variant processing burden on PIE by removing variants that will not be assigned a medal in any case because they are not on the cancer predisposition gene list. You can review the genes by clicking on the link that will appear just below the pull down titled \"See gene list\". Custom gene list: Choosing \"custom\" as your gene list will open a window that will let you paste in a list of genes. Any invalid genes will be dropped from your list automatically. You can separate your genes by spaces or new lines. Max Population frequency: PIE by default will not call medals for variants present in the ExAC (ex-TCGA) database at an allele frequency greater than 0.001. This option lets you override the filtering threshold to whatever frequency you prefer. To disable filtering altogether, specify a value of 1.","title":"Advanced options"},{"location":"guides/pecan/pecan-pie/#progress-page","text":"After uploading is complete you will be taken to a status screen showing the progress of your job through the system. Analysis typically takes 10-15 minutes depending on file size and system availability. It isn't necessary to keep your browser open on this page until your results are ready: the system will e-mail you with a link to return to your results. Optional browser notifications are also available.","title":"Progress page"},{"location":"guides/pecan/pecan-pie/#analysis-of-results","text":"","title":"Analysis of Results"},{"location":"guides/pecan/pecan-pie/#results-browser","text":"When your job is complete you will be taken to an overview page where you can browse your results and examine a detailed results page for each variant. The variants in the results can be filtered by: Filter Meaning Class Predicted effect of variant on protein coding, e.g. missense, nonsense, etc. Somatic medal Medal assigned to the variant by the somatic classifier. Germline medal Medal assigned to the variant by the germline classifier. Committee Classification If the variant has been reviewed by the St. Jude germline variant review committee, the result will appear in this column, otherwise it will be blank. The \"search\" box lets you filter the results by gene and/or amino acid change. The view is dynamically filtered to matching variants as you type.","title":"Results browser"},{"location":"guides/pecan/pecan-pie/#medal-meaning","text":"Medals are only assigned for coding and splice-related variants in disease predisposition genes. Germline medals are only assigned for novel variants or those present in the ExAC (ex-TCGA) database with a MAF no greater than 0.1% (0.001 expressed fractionally). Gold medals are assigned to truncations in tumor suppressor genes, hotspots derived from the COSMIC database, as well as perfect matches to variants in the IARC TP53, PCGP, ASU TERT, ARUP RET, and BIC databases. Silver medals are assigned to in-frame indels, truncations in non-tumor suppressor genes, variants predicted deleterious by damage-prediction algorithms, variants receiving a gold medal from the somatic classifier, and perfect matches to variants in the following databases: ClinVar (predicted pathogenic or likely pathogenic), RB1, LOVD, and UMD. Bronze medals are assigned to variants predicted tolerated by damage-prediction algorithms. Variants having an imperfect match to a database (i.e. different variants at the same genomic position or codon) typically receive a lesser medal. A summary graphic can be found in slide 4 of the ASHG 2017 presentation ( download here ). For additional details see Zhang et al., NEJM 2015 (supplementary appendix pp. 7-10).","title":"Medal meaning"},{"location":"guides/pecan/pecan-pie/#variant-page","text":"Each variant links to a detailed variant page, which integrates data from a variety of sources. If either you or the St. Jude germline variant review committee have annotated a variant, that information will be pre-populated. Summary information The top of the page shows a summary of the variant, including its genomic and HGVS annotations, predicted effect on the protein, and somatic and germline medals. A description of the gene from Entrez follows, and a custom description or selection rationale may also be entered. Medal call information Clicking on one of the medal icons (gold, silver, bronze, unknown) or on the top of the page will show a summary of information related to the medal call. ProteinPaint An embedded version of ProteinPaint ( Zhou et al., Nat. Genet. 2016 ) appears next, showing the variant in the context of a number of pediatric datasets including PCGP. A link is provided to the main ProteinPaint application which provides visualizations for additional datasets, including COSMIC and ClinVar. ASHG pathogenicity classification Formal variant pathogenicity classification is supported by an interface implementing ACMG guidelines ( Richards et al., Genet Med. 2015 ). The analyst reviews a series of curated category tags, assigning applicable tags to the variant and optionally supplying additional information for each such as PubMed IDs and supporting evidence. The system will then compute an appropriate pathogenicity score based on the user-flagged categories. Additional free-form custom evidence can also be entered. This structured approach both helps eliminate arbitrary decision-making from the pathogenicity classification process and also constructs a concise summary of the logic and evidence supporting the final call. ClinVar and allele frequency Matches of the variant in ClinVar are also provided, along with predicted clinical significance and review status. Allele frequencies for the variant in the PCGP (somatic and germline), NHLBI ESP 6500, and ExAC databases are presented both as fractional values and on a log10 plot. Detailed allele population breakdowns are provided for ExAC. Damage prediction algorithms Precomputed damage-prediction algorithm calls for nonsynonymous coding SNVs are presented from the dbNSFP database. Available algorithms are PolyPhen2 (HVAR), SIFT, CADD, REVEL, FATHMM, MutationAssessor, and LRT. The calls are presented in a circular diagram with entries color-coded based on the predicted severity of the result. Medal ceremony and linkouts Additional output from medal ceremony classification can also be reviewed. This is only loosely structured, additional fields here may eventually be integrated into Pecan PIE. Links are provided to relevant dbSNP entries and other information sources. Final classification The final 5-tier ACMG classification can be selected after which the decision will be marked as reviewed. A checkbox is also available to indicate this variant is a potential candidate for functional review.","title":"Variant page"},{"location":"guides/pecan/pecan-pie/#standalone-usage","text":"This section is intended only for users who want to invoke Pecan PIE's underlying analysis pipelines independently on the DNAnexus platform. If you just want to use the Pecan PIE website you can safely ignore this section of the documentation. We assume familiarity with the DNAnexus platform. If you aren't familiar with this, DNAnexus' quickstart guide is a great place to start. Warning This section of the guide is only relevant to power users! Two DNAnexus cloud application pipelines were created during the development of Pecan PIE: Name Corresponding DNAnexus App Description VEP+ app-stjude_vep_plus A cloud installation of VEP with improved logic for splice variant calls. Converts an input VCF of variants to annotated, tab-delimited format. Medal Ceremony app-stjude_medal_ceremony Additional annotation and automated variant classification. Requires a special input format which is produced by VEP+.","title":"Standalone usage"},{"location":"guides/pecan/pecan-pie/#permissions","text":"In order to run the cloud pipelines independently, your DNAnexus account needs to be granted permissions to access them. After your initial login to St. Jude Cloud and/or Pecan PIE, these permissions will be granted automatically. A single login is required even if you just want to use the standalone pipelines rather than the Pecan PIE portal ( contact us if you encounter problems accessing the pipelines). There are two methods of running pipelines on DNAnexus: DNAnexus GUI. DNAnexus provides a standardized graphical user interface for configurating, launching, and monitoring jobs on the cloud. Our pipelines can be run like any other DNAnexus pipeline. Command line. Jobs may also be invoked via the dx command line client. Command-line use allows submitting cloud jobs without interacting with a GUI, and so supports scripting and easier integration with local workflows. See this section for information on how to get set up with the dx-toolkit . Note The following examples demonstrate command-line usage.","title":"Permissions"},{"location":"guides/pecan/pecan-pie/#uploading-files","text":"All input files must be uploaded onto the DNAnexus platform. When specifying files for input you can use either the DNAnexus fie IDs (e.g. file-FBgvp680gz1bGQ5p8yZKz69g ), or the filenames if they are unique. For an idea of how to upload files to DNAnexus, see this guide .","title":"Uploading files"},{"location":"guides/pecan/pecan-pie/#step-1-running-vep","text":"To run the VEP+ DNAnexus app, you can use the following dx command with your own inputs in place of the example's: dx run app-stjude_vep_plus -iinput_file = my_vcf.vcf -igenome_string = GRCh37-lite -igermline_reviewable_only = true Tip genome_string must be either GRCh37-lite or GRCh38 . If GRCh38 is specified, variants will be lifted over to GRCh37-lite in output, i.e. the output will always be GRCh37-lite (Medal Ceremony currently only supports GRCh37-lite ). The input VCF specified by input_file may be either uncompressed, or compressed with bgzip only (htslib/tabix packages). The germline_reviewable_only parameter is optional, but strongly recommended. If specified, only variants in disease-gene related intervals will be annotated, which is appropriate for Medal Ceremony. If this option is not specified all variants will be annotated, which depending on the size of your VCF might take a lot longer, and many of the resulting variants won't be usable by Medal Ceremony. If you want to do this anyway and have a large number of variants, consider submitting your job to an instance with more CPU cores (e.g. mem1_ssd1_x16 or mem1_ssd1_x32 ) as the code will take advantage of the additional cores. If you are using a custom gene list (below) that takes precedence and this parameter is not needed. The optional parameter custom_genes_file specifies a plain text file of HUGO gene symbols to analyze (whitespace separated, or one per line). If specified, analysis will be restricted to these genes only. This pipeline produces two output files, output_file contains annotations for all variants, while medal_prep_output_file is the specially-filtered and formatted file required as input to Medal Ceremony below.","title":"Step 1: Running VEP+"},{"location":"guides/pecan/pecan-pie/#step-2-running-medal-ceremony","text":"To run the medal ceremony DNAnexus app, you can use the following dx command with your own inputs in place of the example's: dx run app-stjude_medal_ceremony -iinfile = medal_prep_output_file Tip The optional parameter custom_genes_file operates in the same way as in the VEP+ pipeline above. For custom gene lists to work properly this parameter must be specified when running both the VEP+ and Medal Ceremony pipelines. The optional parameter max_population_frequency may be specified, a fractional value representing the maximum population frequency allowed for a variant in the ExAC (ex-TCGA) database to receive a medal. The default is 0.001, a.k.a. \".1%\".","title":"Step 2: Running Medal Ceremony"},{"location":"guides/pecan/pecan-pie/#frequently-asked-questions","text":"If you have any questions not covered here, feel free to reach out on our contact form . Q: Which files are supported? PIE works with variants in VCF format: Uploaded files must be compliant with the VCF specification . VCF files may be either uncompressed, or compressed with bgzip only . bgzip is part of the htslib/tabix packages (see below). Improperly formatted VCF files will not work with PIE. Some common problems include: Missing header line Missing required columns Files were compressed by gzip, zip, or any method other than the required bgzip To verify compatibility of your VCF you can try one of these methods: Compressing your VCF with bgzip and indexing it with tabix , both programs from the HTSlib package (some systems also use the earlier, pre-HTSlib \"tabix\" package). This process will only succeed for compliant VCF files, and can help diagnose failures. Running \"vcf-validator\" program from the vcftools package. While the VCF specification also requires that variants be sorted by chromosome name and position, PIE is now often able to automatically correct sorting issues in uploaded files. PIE requires sorted data in order to query data for targeted genes. Q: Are there limits on the size of VCF files? Uploaded files must not exceed 4 gigabytes. If an uploaded file is larger than 2 megabytes, the cancer predisposition gene list filter will be automatically enabled unless you are using a custom gene list. This reduces the processing burden on the system by removing variants outside of targeted genes. Q: Is there an example/demo VCF I can try with PIE? A. You can use this VCF from the Genome in a Bottle project. This ~133 megabyte bgzip-compressed VCF was used during testing of Pecan PIE and is known to work. These variants are mapped to GRCh37. Q. What genome versions are supported? A. Pecan PIE will accept variants mapped to either GRCh37-lite/hg19 or GRCh38. GRCh38 variants are automatically lifted over to 37, as the system uses 37 internally. The liftover process is able to compensate for strand and reference/variant allele swaps which can occur. A native hg38 version is in development, but is not yet available. Pecan PIE only works for human data. Q. What genes are on the curated gene list? A. The list consists of disease-related genes, both cancer and non-cancer, see the file_download Excel spreadsheet for details. Filtering the source variants to a target list of genes reduces the processing burden on the system. When browsing the results the view may be filtered to disease sub-categories of interest. You can also specify your own custom list of genes to process when submitting your VCF file (see the advanced options panel). Q. Why is the classification column blank in my results? Q. This column displays the classification assigned by the St. Jude Germline Committee reviewers. If a variant was not classified by this committee before, this field will be blank. Pecan PIE provides classifications from the Medal Ceremony pipeline, which may assign variants gold, silver, or bronze medals. An \"Unknown\" medal may be assigned for non-disease-predisposition genes, variants present in the ExAC (ex-TCGA) database at an allele frequency > 0.1%, or variants without functional annotations (which includes most silent variants). Q. What do the medals mean? A. The medal column is a rough indicator of the likelihood of the variant being clinically significant as predicted by the medal ceremony software. Variants with gold medals are most likely to be significant, and those with no medal are least likely. More details can be found in the Analysis of Results <results> section. Q. Why are some of my variants missing? A. Currently only coding and splice-related variants in disease-related genes make it to the medaling process. Intergenic, intronic, and UTR variants are excluded, as are those in non-coding transcripts. Q. Why does the ExAC allele frequency shown differ from the ExAC portal? A. The reported ExAC frequency may differ for several reasons: PIE uses the TCGA-subtracted distribution of ExAC rather than the main distribution. PIE reports the primary allele frequencies in the ExAC database, specifically the AC, AN, and AF fields from the VCF distribution. The ExAC portal appears to use the \"adjusted\" frequencies which may be different. Q. Is Pecan PIE free? A. Pecan PIE is free for non-commercial use. St. Jude covers the cost of running the pipeline and hosting. DNANexus accounts are required to keep track of your jobs in the cloud so that you can retrieve and manage from multiple locations. Accounts also make it possible to alert you of job completion via email.","title":"Frequently asked questions"},{"location":"guides/studies/","text":"Studies Study pages discuss how St. Jude has generated and used a particular dataset. St. Jude Cloud currently hosts datasets from the following studies: Pediatric Cancer Genome Project (PCGP) St. Jude Lifetime (SJLIFE) Clinical Genomics (Clinical Pilot and G4K) Sickle Cell Genome Project (SGP) Childhood Cancer Survivor Study (CCSS) Click on a link above to navigate to the corresponding Study page. On each Study page you will find general information about the project such as the goal of the study, demographics and other information about study participants, study design and sequencing technology used, as well as how to cite the study. For complete citation guidelines across datasets, please review the citing St. Jude Cloud page .","title":"About"},{"location":"guides/studies/sickle-cell/","text":"The Sickle Cell Genomics Portal contains two viewers for the exploration of data from the Sickle Cell Genome Project (SGP) dataset . Genome Browser \u00b6 Overview \u00b6 Upon launching the browser, you will see an image similar to the one shown here. A description of the elements of the browser are as follows: # Description 1 Navigation tools and track selector. ( See Navigation Buttons section ) 2 DNase hypersensitivity tracks. By default, four epigenetic tracks are shown. These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells. Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below) 3 RefSeq genes. Gene models from the RefSeq database are displayed in this tracks. 4 -log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease. The analysis has only been performed around the KIAA1109/Tenr/IL2/IL21 region. Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)). The Y-axis for the track represents the -log10 of the p-value. The higher the value, the more statistically significant the association between the variant and pain rate is. Clicking on a variant will open op a window that gives further details about the variant. (See Figure 3). 5 -log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease. See (4) above for more information on this type of track. 6 Filters: Filters allow variants within tracks to be filtered by numerous citeria. See Filter description Navigation buttons \u00b6 # Description a Location/Locus entry field. One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID. b Browser zoom in and out c Tracks: Add or hide tracks (See section below on adding/hiding tracks) d More: Save svg image of browser, get DNA sequence or highlight regions of the browser. Filters \u00b6 # Description a Filters for pain rate p-value track b Filters for age of first vaso-occlusive crisis (VOC) p-value c The highlighted filter shows which value is used for the Y-axis on the browser track. The value can be changed. d A highlighted value within a filter shows which filter value is set. The number next to the filter represents the number of individuals that meet the filter criteria. Getting Started \u00b6 Finding a variant of interest \u00b6 A user can navigate to a gene or to a variant ID. Enter in the variant ID rs13140464 into the search text field at the top of the browser. (See below) Pressing enter will center the browser of the selected variant. (see below) Zooming in and out \u00b6 One can use the buttons next to the search field to zoom in and out along the genome. Press the x50 button to zoom out 50 fold This will show a larger region of the chromosome. One can now see three DNase peaks (1) around the rs13140464 variant(2). In addition there is another variant (3) seen near one of the DNase peaks. Obtaining additional variant information \u00b6 Left clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant. Adding and removing tracks \u00b6 Select the tracks button from the top of the genome browser. A window displaying selected tracks and tracks available for selection will pop up. One can scroll down to see additional tracks. Try selecting and unselecting various tracks and observe the updated tracks on the browser. Getting DNA sequence \u00b6 Select the 'More' button at the top of the browser. Several options will be available. Select the DNA sequence button. You will be shown the DNA sequence for the region. Variants and Phenotype Viewer \u00b6 Overview \u00b6 When the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization. The different elements of the view are as follows. # Description 1 Settings and sort buttons. In addition a link to this help document. 2 Legend for different tracks in the viewer 3 Phenotypic data displayed with an individual represented in each column 4 Genotypic data displayed with an individual represented in each column Labels \u00b6 See glossary for further details # Description Hb Hemoglobin HbF Fetal Hemoglobin HbA2 Variant of hemoglobin that contains two alpha subunits and two delta subunits MCV Mean corpuscular volume. This is the average size of red blood cells PainRate Number of hospitalizations per year over a two year period. Sickle cell genotype (SS_Genotype in legend. Whether patient is SS or SB0 Alpha deletion Whether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles) rs###### Several variants that we have found to be associated with pain in Sickle Cell Disease Getting Started \u00b6 Sorting \u00b6 Hover your mouse over the MCV label in the graph. A box will pop up with several icons. Select the triangle that is pointed to the left to sort individuals by MCV. The following graph shows individuals sorted by MCV. Blank columns represent no data available. Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values. View values for all patients \u00b6 Hovering over one column will enable the viewing of all phenotypic values for that patient. Undo \u00b6 While exploring the data, one may inadvertently sort or remove data. One can undo the changes by selecting the undo button at the top of the viewer. The redo button will revert the undo. Glossary \u00b6 Fetal hemoglobin (HbF) Fetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subunits of beta-globin and two units of alpha-globin. Heriditary persistence of fetal hemoglobin (HPFH) Individuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease. Principal Component Analysis (PCA) A method for reducing high dimensional data into low-dimensional representations. SC An individual with one copy of the sickle cell allele rs334 and one copy of hemoglobin C . S\u03b2 + An individual with beta-thalassemia who has one copy of the sickle cell allele rs334 and one copy of a beta-globin gene that has reduced expression. S\u03b2 0 An individual with beta-thalassemia who has one copy of the sickle cell allele rs334 and one copy of a beta-globin gene that is not expressed or is deleted. SS An individual with sickle cell disease who is homozygous for the sickle cell allele rs334 . SCCRIP The Sickle Cell Research and Intervention Program .","title":"Sickle Cell"},{"location":"guides/studies/sickle-cell/#genome-browser","text":"","title":"Genome Browser"},{"location":"guides/studies/sickle-cell/#overview","text":"Upon launching the browser, you will see an image similar to the one shown here. A description of the elements of the browser are as follows: # Description 1 Navigation tools and track selector. ( See Navigation Buttons section ) 2 DNase hypersensitivity tracks. By default, four epigenetic tracks are shown. These are DNAse hypersensitivity tracks for Hematopoeitic stem cells (HSC), T Cells, Monocytes, and B Cells. Additional tracks can be viewed by selecting the \u2018Tracks\u2019 button (See Adding/Removing Tracks section below) 3 RefSeq genes. Gene models from the RefSeq database are displayed in this tracks. 4 -log10 of the p-value of the association of each variants with pain rate in individuals with Sickle Cell Disease. The analysis has only been performed around the KIAA1109/Tenr/IL2/IL21 region. Each dot on the track represents a genomic variant (Single Nucleotide Variant (SNV) or small insertion/deletion (INDEL)). The Y-axis for the track represents the -log10 of the p-value. The higher the value, the more statistically significant the association between the variant and pain rate is. Clicking on a variant will open op a window that gives further details about the variant. (See Figure 3). 5 -log10 of the p-value of the association of each variants with age of first vaso-occlusive crisis in individuals with Sickle Cell Disease. See (4) above for more information on this type of track. 6 Filters: Filters allow variants within tracks to be filtered by numerous citeria. See Filter description","title":"Overview"},{"location":"guides/studies/sickle-cell/#navigation-buttons","text":"# Description a Location/Locus entry field. One can enter genomic coordinates in the form of chromosome:start-end (for example chr1:12345-9876), or a gene name or a SNP rs ID. b Browser zoom in and out c Tracks: Add or hide tracks (See section below on adding/hiding tracks) d More: Save svg image of browser, get DNA sequence or highlight regions of the browser.","title":"Navigation buttons"},{"location":"guides/studies/sickle-cell/#filters","text":"# Description a Filters for pain rate p-value track b Filters for age of first vaso-occlusive crisis (VOC) p-value c The highlighted filter shows which value is used for the Y-axis on the browser track. The value can be changed. d A highlighted value within a filter shows which filter value is set. The number next to the filter represents the number of individuals that meet the filter criteria.","title":"Filters"},{"location":"guides/studies/sickle-cell/#getting-started","text":"","title":"Getting Started"},{"location":"guides/studies/sickle-cell/#finding-a-variant-of-interest","text":"A user can navigate to a gene or to a variant ID. Enter in the variant ID rs13140464 into the search text field at the top of the browser. (See below) Pressing enter will center the browser of the selected variant. (see below)","title":"Finding a variant of interest"},{"location":"guides/studies/sickle-cell/#zooming-in-and-out","text":"One can use the buttons next to the search field to zoom in and out along the genome. Press the x50 button to zoom out 50 fold This will show a larger region of the chromosome. One can now see three DNase peaks (1) around the rs13140464 variant(2). In addition there is another variant (3) seen near one of the DNase peaks.","title":"Zooming in and out"},{"location":"guides/studies/sickle-cell/#obtaining-additional-variant-information","text":"Left clicking a variant (see red circle below), will cause a new window to pop up in the browser that will contain additional information about the variant.","title":"Obtaining additional variant information"},{"location":"guides/studies/sickle-cell/#adding-and-removing-tracks","text":"Select the tracks button from the top of the genome browser. A window displaying selected tracks and tracks available for selection will pop up. One can scroll down to see additional tracks. Try selecting and unselecting various tracks and observe the updated tracks on the browser.","title":"Adding and removing tracks"},{"location":"guides/studies/sickle-cell/#getting-dna-sequence","text":"Select the 'More' button at the top of the browser. Several options will be available. Select the DNA sequence button. You will be shown the DNA sequence for the region.","title":"Getting DNA sequence"},{"location":"guides/studies/sickle-cell/#variants-and-phenotype-viewer","text":"","title":"Variants and Phenotype Viewer"},{"location":"guides/studies/sickle-cell/#overview_1","text":"When the Variants and Phenotype Viewer is launched, the user will be presented with the following visualization. The different elements of the view are as follows. # Description 1 Settings and sort buttons. In addition a link to this help document. 2 Legend for different tracks in the viewer 3 Phenotypic data displayed with an individual represented in each column 4 Genotypic data displayed with an individual represented in each column","title":"Overview"},{"location":"guides/studies/sickle-cell/#labels","text":"See glossary for further details # Description Hb Hemoglobin HbF Fetal Hemoglobin HbA2 Variant of hemoglobin that contains two alpha subunits and two delta subunits MCV Mean corpuscular volume. This is the average size of red blood cells PainRate Number of hospitalizations per year over a two year period. Sickle cell genotype (SS_Genotype in legend. Whether patient is SS or SB0 Alpha deletion Whether the individual has an alpha globin deletion (het=1 deleted allele, homo=2 deleted alleles) rs###### Several variants that we have found to be associated with pain in Sickle Cell Disease","title":"Labels"},{"location":"guides/studies/sickle-cell/#getting-started_1","text":"","title":"Getting Started"},{"location":"guides/studies/sickle-cell/#sorting","text":"Hover your mouse over the MCV label in the graph. A box will pop up with several icons. Select the triangle that is pointed to the left to sort individuals by MCV. The following graph shows individuals sorted by MCV. Blank columns represent no data available. Note that PainRate, Sickle cell genotype and alpha deletion status appear to correlate with MCV values.","title":"Sorting"},{"location":"guides/studies/sickle-cell/#view-values-for-all-patients","text":"Hovering over one column will enable the viewing of all phenotypic values for that patient.","title":"View values for all patients"},{"location":"guides/studies/sickle-cell/#undo","text":"While exploring the data, one may inadvertently sort or remove data. One can undo the changes by selecting the undo button at the top of the viewer. The redo button will revert the undo.","title":"Undo"},{"location":"guides/studies/sickle-cell/#glossary","text":"Fetal hemoglobin (HbF) Fetal hemoglobin contains two subunits of gamma-globin and two units of alpha-globin, while adult hemoglobin contains two subunits of beta-globin and two units of alpha-globin. Heriditary persistence of fetal hemoglobin (HPFH) Individuals with HPFH have elevated levels of fetal hemoglobin. These elevated levels reduce or eliminate many of the symptoms of Sickle Cell Disease. Principal Component Analysis (PCA) A method for reducing high dimensional data into low-dimensional representations. SC An individual with one copy of the sickle cell allele rs334 and one copy of hemoglobin C . S\u03b2 + An individual with beta-thalassemia who has one copy of the sickle cell allele rs334 and one copy of a beta-globin gene that has reduced expression. S\u03b2 0 An individual with beta-thalassemia who has one copy of the sickle cell allele rs334 and one copy of a beta-globin gene that is not expressed or is deleted. SS An individual with sickle cell disease who is homozygous for the sickle cell allele rs334 . SCCRIP The Sickle Cell Research and Intervention Program .","title":"Glossary"},{"location":"guides/visualization-community/","text":"Visualization Community In St. Jude Cloud's Visualization Community app , we provide genomic and phenomic data in cloud-based, dynamic visualizations generated by combining world class visualization software with high quality next-generation sequencing datasets, both generated and developed at St. Jude Children's Research Hospital. We aim to share these visualizations to encourage discovery amongst researchers; accessing raw data and having to start at the beginning isn't always most efficient. You can search interactive visualizations by Name, Tool , Research Domain , or chronological order. Visualization Tools \u00b6 ProteinPaint is the flagship protein-based visualization tool created at St. Jude. You can use it to examine the domains of genes, known isoforms of a given gene, hotspot mutations for single nucleotide variations (SNVs), insertions and deletions (Indels), and structual variations (SVs) in both pediatric and adult cancers, and RNA-seq expression of a given protein in different tumour types. See the documentation here. GenomePaint visualizes somatic coding and noncoding alterations from ~3,800 pediatric trumors, along with multi-omics information to reveal oncogene activation by noncoding alterations, enhancer hijacking, aberrant splicing, mutual-exclusivity, mutation signature, and perform Kaplan-Meier analysis. It can be used to visualize your own data with easy customization and embedding on your own page. See the documentation here. SJCharts is a collection of plotting libraries useful in creating individual or summary level views of somatic and germline variation. It includes facilities for generating dynamic heatmaps variation in a cohort, Circos-like plot to summarize individual or cohort-level data (such as the one shown here), ribbon plots to visualize gene-pathway-diagnosis relationships, and much more. (Documentation coming soon.)","title":"Getting Started"},{"location":"guides/visualization-community/#visualization-tools","text":"ProteinPaint is the flagship protein-based visualization tool created at St. Jude. You can use it to examine the domains of genes, known isoforms of a given gene, hotspot mutations for single nucleotide variations (SNVs), insertions and deletions (Indels), and structual variations (SVs) in both pediatric and adult cancers, and RNA-seq expression of a given protein in different tumour types. See the documentation here. GenomePaint visualizes somatic coding and noncoding alterations from ~3,800 pediatric trumors, along with multi-omics information to reveal oncogene activation by noncoding alterations, enhancer hijacking, aberrant splicing, mutual-exclusivity, mutation signature, and perform Kaplan-Meier analysis. It can be used to visualize your own data with easy customization and embedding on your own page. See the documentation here. SJCharts is a collection of plotting libraries useful in creating individual or summary level views of somatic and germline variation. It includes facilities for generating dynamic heatmaps variation in a cohort, Circos-like plot to summarize individual or cohort-level data (such as the one shown here), ribbon plots to visualize gene-pathway-diagnosis relationships, and much more. (Documentation coming soon.)","title":"Visualization Tools"},{"location":"guides/visualization-community/faq/","text":"Frequently Asked Questions Will I be charged for using St. Jude Cloud Visualization Community? Will I be charged for using St. Jude Cloud Visualization Community? \u00b6 You will not incur any costs except in the following situations: When data driving a visualization is stored in St. Jude Cloud Genomics Platform or DNAnexus, a monthly fee will be incurred for storage costs. See your DNAnexus billing information for the cost per GB.","title":"Frequently Asked Questions"},{"location":"guides/visualization-community/faq/#will-i-be-charged-for-using-st-jude-cloud-visualization-community","text":"You will not incur any costs except in the following situations: When data driving a visualization is stored in St. Jude Cloud Genomics Platform or DNAnexus, a monthly fee will be incurred for storage costs. See your DNAnexus billing information for the cost per GB.","title":"Will I be charged for using St. Jude Cloud Visualization Community?"},{"location":"guides/visualization-community/genomepaint/","text":"GenomePaint GenomePaint is a visualization browser for simultaneously viewing genomic, transcriptomic, and epigenomic pediatric cancer mutation datasets across a multitude of disease cohorts. GenomePaint datasets include WGS, WES, RNA-Seq, SNP-chip, ChIP-Seq, and Hi-C data visualized over the hg19 reference genome. You can use GenomePaint to interpret the impact of somatic coding and noncoding alterations from ~3,800 pediatric tumors, make novel discoveries through visual exploration, and create publication ready figures! Getting Started \u00b6 The GenomePaint browser homepage lands on a dense cohort view of the TAL1 gene region of chromosome 1. Each section of the display can be interacted with by clicking, dragging, or hovering. Filter the information displayed on the tracks by clicking buttons in the legend. Customize the legend display by hiding/showing classes. Click CONFIG to the right of tracks for additional display customization. To navigate tracks, Pan left or right by clicking on the middle part of the track and dragging Zoom in by dragging the genomic coordinate ruler on top or zoom in 1 fold by clicking on the IN button Zoom out by x fold by clicking on an OUT button. You can even zoom in to display mutations at bp resolution. GenomePaint offers three different views: cohort view, sample view, and matrix view. The figure below summarizes how each view is connected. Cohort View \u00b6 The cohort view shows mutations from all samples over a genomic region, along with the gene expression ranks for each of the samples. By default the mutation track displays the cohort view in dense mode, a compact display showing density plots for SVs and SNV/indels. You can toggle the view to expanded mode by clicking the CONFIG button to the right of the mutation track and then clicking Expanded. In Expanded mode (see below) all types of mutations are shown for each sample, one row per sample. Circles represent SV/fusion breakpoints, and x marks represent SNV/indels, each of which are displayed together with CNV/LOH. SNV/indels and breakpoints are always shown on top of CNV and LOH. Text labels can be shown for SV/fusion/SNV/indel, if available. Sample View \u00b6 The sample view shows mutations for one sample alone, along with any available genomic assay tracks. You can open a sample view from the expanded cohort view by clicking on any type of single mutation within the sample, and then selecting Focus . This brings up a new browser view showing data tracks from this sample in the region surrounding the mutation. On the sample view you can explore expression rank, tumor mutations, structural variants, splice junctions, WES coverage, and RNA-Seq coverage. Customize the display by zooming in/out, hiding and/or rearranging tracks, or editing CONFIG options. Matrix View \u00b6 The matrix view combines the mutation profiles of multiple genomic regions in one view, in the form of a sample-by-region matrix. Such a matrix can be generated for samples from one cancer type. To open a matrix view, select a disease cohort from the cohort view and then select Matrix view. This organizes the selected cohort tumors with mutations in the genomic region you are viewing into a single-column matrix. Next, go back to the cohort view and type another gene or region of interest into the genome coordinate box and press ENTER. The cohort view will now show data at the new genomic location. Click on the same disease cohort and then select Matrix view. This will add the new genomic variant as a second column in the matrix. You can continue adding columns to this matrix in the same manner. Advanced Customizations \u00b6 There are several more advanced customizations you can leverage with GenomePaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our detailed tutorial . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.","title":"GenomePaint"},{"location":"guides/visualization-community/genomepaint/#getting-started","text":"The GenomePaint browser homepage lands on a dense cohort view of the TAL1 gene region of chromosome 1. Each section of the display can be interacted with by clicking, dragging, or hovering. Filter the information displayed on the tracks by clicking buttons in the legend. Customize the legend display by hiding/showing classes. Click CONFIG to the right of tracks for additional display customization. To navigate tracks, Pan left or right by clicking on the middle part of the track and dragging Zoom in by dragging the genomic coordinate ruler on top or zoom in 1 fold by clicking on the IN button Zoom out by x fold by clicking on an OUT button. You can even zoom in to display mutations at bp resolution. GenomePaint offers three different views: cohort view, sample view, and matrix view. The figure below summarizes how each view is connected.","title":"Getting Started"},{"location":"guides/visualization-community/genomepaint/#cohort-view","text":"The cohort view shows mutations from all samples over a genomic region, along with the gene expression ranks for each of the samples. By default the mutation track displays the cohort view in dense mode, a compact display showing density plots for SVs and SNV/indels. You can toggle the view to expanded mode by clicking the CONFIG button to the right of the mutation track and then clicking Expanded. In Expanded mode (see below) all types of mutations are shown for each sample, one row per sample. Circles represent SV/fusion breakpoints, and x marks represent SNV/indels, each of which are displayed together with CNV/LOH. SNV/indels and breakpoints are always shown on top of CNV and LOH. Text labels can be shown for SV/fusion/SNV/indel, if available.","title":"Cohort View"},{"location":"guides/visualization-community/genomepaint/#sample-view","text":"The sample view shows mutations for one sample alone, along with any available genomic assay tracks. You can open a sample view from the expanded cohort view by clicking on any type of single mutation within the sample, and then selecting Focus . This brings up a new browser view showing data tracks from this sample in the region surrounding the mutation. On the sample view you can explore expression rank, tumor mutations, structural variants, splice junctions, WES coverage, and RNA-Seq coverage. Customize the display by zooming in/out, hiding and/or rearranging tracks, or editing CONFIG options.","title":"Sample View"},{"location":"guides/visualization-community/genomepaint/#matrix-view","text":"The matrix view combines the mutation profiles of multiple genomic regions in one view, in the form of a sample-by-region matrix. Such a matrix can be generated for samples from one cancer type. To open a matrix view, select a disease cohort from the cohort view and then select Matrix view. This organizes the selected cohort tumors with mutations in the genomic region you are viewing into a single-column matrix. Next, go back to the cohort view and type another gene or region of interest into the genome coordinate box and press ENTER. The cohort view will now show data at the new genomic location. Click on the same disease cohort and then select Matrix view. This will add the new genomic variant as a second column in the matrix. You can continue adding columns to this matrix in the same manner.","title":"Matrix View"},{"location":"guides/visualization-community/genomepaint/#advanced-customizations","text":"There are several more advanced customizations you can leverage with GenomePaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our detailed tutorial . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.","title":"Advanced Customizations"},{"location":"guides/visualization-community/proteinpaint/","text":"ProteinPaint ProteinPaint is a web application for simultaneously visualizing genetic lesions (including sequence mutations and gene fusions) and RNA expression in pediatric cancers. You can find the ProteinPaint paper here . Overview \u00b6 The image below shows an example ProteinPaint of the gene TP53 annotated with descriptions of the many interactive elements of a ProteinPaint visualization. As you can see, there is a lot to explore. Glossary of Classes \u00b6 The list below summarizes all classes of mutations used by ProteinPaint. Mutation Class Description MISSENSE a substitution variant in the coding region resulting in altered protein coding FRAMESHIFT an insertion or deletion variant that alters the protein coding frame NONSENSE a variant altering protein coding to produce a premature stopgain or stoploss. PROTEINDEL a deletion resulting in a loss of one or more codons from the product, but not altering the protein coding frame PROTEININS an insertion introducing one or more codons into the product, but not altering the protein coding frame SPLICE a variant near an exon edge that may affect splicing functionality SILENT a substitution variant in the coding region that does not alter protein coding SPLICE_REGION a variant in an intron within 10 nt of an exon boundary UTR_5 a variant in the 5' untranslated region UTR_3 a variant in the 3' untranslated region EXON a variant in the exon of a non-coding RNA INTRON an intronic variant Glossary of Origins \u00b6 The list below summarizes all origins of mutations used by ProteinPaint. Mutation Origin Description Germline a variant found in a normal sample of a cancer patient. Somatic a variant found only in a tumor sample. Relapse a variant that arose in recurrence tumor. Advanced Customizations \u00b6 There are several more advanced customizations you can leverage with ProteinPaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our detailed tutorial . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.","title":"ProteinPaint"},{"location":"guides/visualization-community/proteinpaint/#overview","text":"The image below shows an example ProteinPaint of the gene TP53 annotated with descriptions of the many interactive elements of a ProteinPaint visualization. As you can see, there is a lot to explore.","title":"Overview"},{"location":"guides/visualization-community/proteinpaint/#glossary-of-classes","text":"The list below summarizes all classes of mutations used by ProteinPaint. Mutation Class Description MISSENSE a substitution variant in the coding region resulting in altered protein coding FRAMESHIFT an insertion or deletion variant that alters the protein coding frame NONSENSE a variant altering protein coding to produce a premature stopgain or stoploss. PROTEINDEL a deletion resulting in a loss of one or more codons from the product, but not altering the protein coding frame PROTEININS an insertion introducing one or more codons into the product, but not altering the protein coding frame SPLICE a variant near an exon edge that may affect splicing functionality SILENT a substitution variant in the coding region that does not alter protein coding SPLICE_REGION a variant in an intron within 10 nt of an exon boundary UTR_5 a variant in the 5' untranslated region UTR_3 a variant in the 3' untranslated region EXON a variant in the exon of a non-coding RNA INTRON an intronic variant","title":"Glossary of Classes"},{"location":"guides/visualization-community/proteinpaint/#glossary-of-origins","text":"The list below summarizes all origins of mutations used by ProteinPaint. Mutation Origin Description Germline a variant found in a normal sample of a cancer patient. Somatic a variant found only in a tumor sample. Relapse a variant that arose in recurrence tumor.","title":"Glossary of Origins"},{"location":"guides/visualization-community/proteinpaint/#advanced-customizations","text":"There are several more advanced customizations you can leverage with ProteinPaint such as creating custom tracks, importing your own data, and embedding interactive visualizations on your web page. For instructions on these topics, please see our detailed tutorial . Please excuse the different location and formatting as we work to incorporate this into our main documentation pages.","title":"Advanced Customizations"},{"location":"guides/visualization-community/sjcharts/","text":"SJ Charts","title":"SJ Charts"},{"location":"guides/visualization-community/t-sne-charts/","text":"t-SNE Charts St. Jude Cloud provides functionality for generating t-Distributed Stochastic Neighbor Embedding (t-SNE) plots. You can find the t-SNE paper here . This tool allows plotting of RNA-seq data by running through the St. Jude Cloud normalization pipeline . The generated count data is then compared to a reference set of data from a cohort of St. Jude samples using t-SNE and a plot is produced. Requirements \u00b6 The t-SNE pipeline reference data uses sequencing data from fresh, frozen tissue samples. It has not been evaluated for use with sequencing data generated from formalin-fixed paraffin-embedded (FFPE) specimens. If running the aligned BAM or count-based t-SNE pipelines, alignment must be done against the GRCh38_no_alt reference . It should use parameters as specifed in our RNA-seq workflow to minimize any discrepancies caused by differing alignment specification. If running the count-based t-SNE pipeline, feature counts should be generated with htseq-count as described in our RNA-seq workflow . This pipeline uses Gencode v31 annotations. Inputs \u00b6 The input can be either of the two entries below, based on whether you want to start with a counts file or a BAM file. Name Description Example BAM file Aligned reads file from human RNA-Seq Sample.bam Counts file htseq-count output feature counts file from human RNA-Seq Sample.counts.txt Caution If you provide a BAM file to the non-alignemnet pipeline or counts data to the counts-based pipeline, it must be aligned to GRCh38_no_alt . Running a BAM aligned to any other reference genome is not supported. Maybe more importantly, we do not check the genome build of the BAM, so errors in computation or the results can occur. If your BAM is not aligned to this genome build, we recommend submitting the BAM to the realignment-based workflow. Outputs \u00b6 The interactive t-SNE pipeline produces the following outputs: Name Description Pipeline Version Interactive t-SNE plot (.html) t-SNE visualization all Aligned BAM (.bam) BAM file produced by our RNA-Seq pipeline for the input samples. Realignment Feature read counts (.txt) Read counts for the Gencode features. BAM-based and Realignment Workflow Steps \u00b6 The aligned BAM is converted to FastQ and is aligned to GRCh38_no_alt using standard STAR mapping . Only for realignment workflow A feature count (.count.txt) file is produced for comparison to St. Jude Cloud reference data. For BAM-based and realignment workflows A t-SNE visualizations for genomic features is produced. Mapping We use the STAR aligner to rapidly map reads to the GRCh38 human reference genome. Feature Counts We use htseq-count to produce genomic feature counts. Visualization A t-SNE visualization is produced using Rtsne . Getting Started \u00b6 Caution If you provide a BAM file to the non-alignemnet pipeline or counts data to the counts-based pipeline, it must be aligned to GRCh38_no_alt . Running a BAM aligned to any other reference genome is not supported. Maybe more importantly, we do not check the genome build of the BAM, so errors in computation or the results can occur. If your BAM is not aligned to this genome build, we recommend submitting the BAM to the realignment-based workflow. We provide three versions of the t-SNE tool depending on desired input. The full workflow allows a user to upload a sample in BAM format. That sample will then be converted to FastQ format, aligned with STAR two-pass alignment, and feature counts generated with htseq-count To get started, you need to navigate to the Interactive t-SNE tool page . You'll need to click the \"Start\" button in the left hand pane. This creates a cloud workspace in DNAnexus with the same name as the tool. After this, you will be able to upload your input files to that workspace. Uploading data \u00b6 The Interactive t-SNE pipeline takes either a htseq-count count file or a GRCh38_no_alt aligned BAM from human RNA-Seq. You can upload your input file(s) using the data transfer application or by uploading them through the command line. Both of the guides linked here will contain more details on how to upload data using that method, so we defer to those guides here. Running the tool \u00b6 Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the tool's landing page. A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with a counts file or a BAM file. Hooking up inputs \u00b6 Next, you'll need to hook up either the counts file or the BAM file you uploaded in the upload data section. In this example, we are using the realignment version of the pipeline, so you can hook up the inputs by clicking on the in_bams slot and selecting the respective files. If you are using the BAM-based or counts-based workflow, the process is similar. Additionally, a parameter selecting the tissue type to compare against must be selected. The available options are \"blood\", \"brain\", and \"solid\". Based on the selection, a reference collection of tumors of that type will be selected from St. Jude Cloud data and the input samples will be compared against this reference collection. Starting the workflow \u00b6 Once your input files are hooked up, you should be able to start the workflow by clicking the \"Run as Analysis...\" button in the top right hand corner of the workflow dialog. Monitoring run progress \u00b6 Once you have started one or more Interactive t-SNE runs, you can safely close your browser and come back later to check the status of the jobs. To do this, navigate to the tool's landing page. Next, click \"View Results\" then select the \"View Running Jobs\" option. You will be redirected to the job monitoring page. Each job you kicked off gets one row in this table. You can click the \"+\" on any of the runs to check the status of individual steps of the Interactive t-SNE pipeline. Other information, such as time, cost of individual steps in the pipeline, and even viewing the job logs can accessed by clicking around the sub-items. Visualizing results \u00b6 Once the resulting analysis job completes, an HTML plot of the results should be available. The plot is generated with the Plotly R library . The plot can be zoomed arbitrarily and group labels can be turned on/off for manual inspection. User input samples will be displayed in black marks with a label on the graph as well as an entry in the legend. Batch effect corrections \u00b6 When comparing numerous samples such as those included in this analysis, it is important to consider the variation in data created by obtaining from various sources and across time. Therefore the t-SNE visualization incorporates some batch effect corrections for the reference data. Currently we correct for batch effect based on strandedness of the RNA-Seq sample, library type, and read length protocol. Known issues \u00b6 !!! caution \"Fresh Frozen Tissue Only The t-SNE pipeline reference data uses sequencing data from fresh, frozen tissue samples. It has not been evaluated for use with sequencing data generated from formalin-fixed paraffin-embedded (FFPE) specimens. GRCh38 required If running the aligned BAM or count-based t-SNE pipelines, alignment must be done against the GRCh38_no_alt reference . It should use parameters as specifed in our RNA-seq workflow to minimize any discrepancies caused by differing alignment specification. Standardized feature counts must be provided If running the count-based t-SNE pipeline, feature counts should be generated with htseq-count as described in our RNA-seq workflow . This pipeline uses Gencode v31 annotations. Frequently asked questions \u00b6 If you have any questions not covered here, feel free to reach out on our contact form . Submit batch jobs on the command line \u00b6 See How can I run an analysis workflow on multiple sample files at the same time?","title":"t-SNE Charts"},{"location":"guides/visualization-community/t-sne-charts/#requirements","text":"The t-SNE pipeline reference data uses sequencing data from fresh, frozen tissue samples. It has not been evaluated for use with sequencing data generated from formalin-fixed paraffin-embedded (FFPE) specimens. If running the aligned BAM or count-based t-SNE pipelines, alignment must be done against the GRCh38_no_alt reference . It should use parameters as specifed in our RNA-seq workflow to minimize any discrepancies caused by differing alignment specification. If running the count-based t-SNE pipeline, feature counts should be generated with htseq-count as described in our RNA-seq workflow . This pipeline uses Gencode v31 annotations.","title":"Requirements"},{"location":"guides/visualization-community/t-sne-charts/#inputs","text":"The input can be either of the two entries below, based on whether you want to start with a counts file or a BAM file. Name Description Example BAM file Aligned reads file from human RNA-Seq Sample.bam Counts file htseq-count output feature counts file from human RNA-Seq Sample.counts.txt Caution If you provide a BAM file to the non-alignemnet pipeline or counts data to the counts-based pipeline, it must be aligned to GRCh38_no_alt . Running a BAM aligned to any other reference genome is not supported. Maybe more importantly, we do not check the genome build of the BAM, so errors in computation or the results can occur. If your BAM is not aligned to this genome build, we recommend submitting the BAM to the realignment-based workflow.","title":"Inputs"},{"location":"guides/visualization-community/t-sne-charts/#outputs","text":"The interactive t-SNE pipeline produces the following outputs: Name Description Pipeline Version Interactive t-SNE plot (.html) t-SNE visualization all Aligned BAM (.bam) BAM file produced by our RNA-Seq pipeline for the input samples. Realignment Feature read counts (.txt) Read counts for the Gencode features. BAM-based and Realignment","title":"Outputs"},{"location":"guides/visualization-community/t-sne-charts/#workflow-steps","text":"The aligned BAM is converted to FastQ and is aligned to GRCh38_no_alt using standard STAR mapping . Only for realignment workflow A feature count (.count.txt) file is produced for comparison to St. Jude Cloud reference data. For BAM-based and realignment workflows A t-SNE visualizations for genomic features is produced. Mapping We use the STAR aligner to rapidly map reads to the GRCh38 human reference genome. Feature Counts We use htseq-count to produce genomic feature counts. Visualization A t-SNE visualization is produced using Rtsne .","title":"Workflow Steps"},{"location":"guides/visualization-community/t-sne-charts/#getting-started","text":"Caution If you provide a BAM file to the non-alignemnet pipeline or counts data to the counts-based pipeline, it must be aligned to GRCh38_no_alt . Running a BAM aligned to any other reference genome is not supported. Maybe more importantly, we do not check the genome build of the BAM, so errors in computation or the results can occur. If your BAM is not aligned to this genome build, we recommend submitting the BAM to the realignment-based workflow. We provide three versions of the t-SNE tool depending on desired input. The full workflow allows a user to upload a sample in BAM format. That sample will then be converted to FastQ format, aligned with STAR two-pass alignment, and feature counts generated with htseq-count To get started, you need to navigate to the Interactive t-SNE tool page . You'll need to click the \"Start\" button in the left hand pane. This creates a cloud workspace in DNAnexus with the same name as the tool. After this, you will be able to upload your input files to that workspace.","title":"Getting Started"},{"location":"guides/visualization-community/t-sne-charts/#uploading-data","text":"The Interactive t-SNE pipeline takes either a htseq-count count file or a GRCh38_no_alt aligned BAM from human RNA-Seq. You can upload your input file(s) using the data transfer application or by uploading them through the command line. Both of the guides linked here will contain more details on how to upload data using that method, so we defer to those guides here.","title":"Uploading data"},{"location":"guides/visualization-community/t-sne-charts/#running-the-tool","text":"Once you've uploaded data to your cloud workspace, click \"Launch Tool\" on the tool's landing page. A dropdown will present the different presets for running the workflow. Here, you can select whether you wish to start with a counts file or a BAM file.","title":"Running the tool"},{"location":"guides/visualization-community/t-sne-charts/#hooking-up-inputs","text":"Next, you'll need to hook up either the counts file or the BAM file you uploaded in the upload data section. In this example, we are using the realignment version of the pipeline, so you can hook up the inputs by clicking on the in_bams slot and selecting the respective files. If you are using the BAM-based or counts-based workflow, the process is similar. Additionally, a parameter selecting the tissue type to compare against must be selected. The available options are \"blood\", \"brain\", and \"solid\". Based on the selection, a reference collection of tumors of that type will be selected from St. Jude Cloud data and the input samples will be compared against this reference collection.","title":"Hooking up inputs"},{"location":"guides/visualization-community/t-sne-charts/#starting-the-workflow","text":"Once your input files are hooked up, you should be able to start the workflow by clicking the \"Run as Analysis...\" button in the top right hand corner of the workflow dialog.","title":"Starting the workflow"},{"location":"guides/visualization-community/t-sne-charts/#monitoring-run-progress","text":"Once you have started one or more Interactive t-SNE runs, you can safely close your browser and come back later to check the status of the jobs. To do this, navigate to the tool's landing page. Next, click \"View Results\" then select the \"View Running Jobs\" option. You will be redirected to the job monitoring page. Each job you kicked off gets one row in this table. You can click the \"+\" on any of the runs to check the status of individual steps of the Interactive t-SNE pipeline. Other information, such as time, cost of individual steps in the pipeline, and even viewing the job logs can accessed by clicking around the sub-items.","title":"Monitoring run progress"},{"location":"guides/visualization-community/t-sne-charts/#visualizing-results","text":"Once the resulting analysis job completes, an HTML plot of the results should be available. The plot is generated with the Plotly R library . The plot can be zoomed arbitrarily and group labels can be turned on/off for manual inspection. User input samples will be displayed in black marks with a label on the graph as well as an entry in the legend.","title":"Visualizing results"},{"location":"guides/visualization-community/t-sne-charts/#batch-effect-corrections","text":"When comparing numerous samples such as those included in this analysis, it is important to consider the variation in data created by obtaining from various sources and across time. Therefore the t-SNE visualization incorporates some batch effect corrections for the reference data. Currently we correct for batch effect based on strandedness of the RNA-Seq sample, library type, and read length protocol.","title":"Batch effect corrections"},{"location":"guides/visualization-community/t-sne-charts/#known-issues","text":"!!! caution \"Fresh Frozen Tissue Only The t-SNE pipeline reference data uses sequencing data from fresh, frozen tissue samples. It has not been evaluated for use with sequencing data generated from formalin-fixed paraffin-embedded (FFPE) specimens. GRCh38 required If running the aligned BAM or count-based t-SNE pipelines, alignment must be done against the GRCh38_no_alt reference . It should use parameters as specifed in our RNA-seq workflow to minimize any discrepancies caused by differing alignment specification. Standardized feature counts must be provided If running the count-based t-SNE pipeline, feature counts should be generated with htseq-count as described in our RNA-seq workflow . This pipeline uses Gencode v31 annotations.","title":"Known issues"},{"location":"guides/visualization-community/t-sne-charts/#frequently-asked-questions","text":"If you have any questions not covered here, feel free to reach out on our contact form .","title":"Frequently asked questions"},{"location":"guides/visualization-community/t-sne-charts/#submit-batch-jobs-on-the-command-line","text":"See How can I run an analysis workflow on multiple sample files at the same time?","title":"Submit batch jobs on the command line"}]}